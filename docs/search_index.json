[["index.html", "Create a cryptocurrency trading bot in Elixir Welcome üëã Limit of Liability/Disclaimer of Warranty Contributing / Errata Preface Who this book is for What this book covers Source code", " Create a cryptocurrency trading bot in Elixir Kamil Skowron 2021-04-20 Welcome üëã This website is the home of Create a cryptocurrency trading bot in Elixir book. This book is 75% complete - chapters 1-15 are finished but I‚Äôm planning to add more content in the near future. Are you looking for a real-world project to gain hands-on experience/solidify your understanding of Elixir/OTP? You are in the right place! This book will take you on a journey to create a cryptocurrency trading bot in Elixir. You will be able to see first-hand, how complex systems are designed and developed as we will build them together! Note: This book was initially published via Leanpub, but to keep it up to date as well as publicly available for people that can‚Äôt afford to pay for it, it‚Äôs now available here free of charge. Knowledge is power and should be shared equally üôè This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC BY-NC-SA 4.0. Please consider supporting this book and my other work via the GitHub Sponsors program(it supports both one-off payments as well as ‚Äúmonthly‚Äù plans). Limit of Liability/Disclaimer of Warranty THIS BOOK IS NOT A FINANCIAL ADVICE THE SOFTWARE/BOOK IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE/BOOK OR THE USE OR OTHER DEALINGS IN THE SOFTWARE/BOOK. Contributing / Errata The book is written using R Markdown(it‚Äôs a very similar syntax to the GitHub markdown but supports many more features including code execution, etc.) and converted to final form (for example PDF) using the bookdown app. This means that editing a chapter is as simple as editing the markdown for that chapter - there is even an edit button at the top of the page to do exactly that. I would love to follow the standard process of forking, making changes, opening PR, merging, and releasing a new version of the book. I invite people to open issues as well with suggestions. Preface In recent years Elixir programming language gained a lot of interest in the industry. Its unmatched parallelization capabilities are unique and powerful, making it a great candidate for highly concurrent systems like the ones trading assets on exchanges. In this book, we will go through the development process of a cryptocurrency trading bot in Elixir. We will start grounds up and chapter by chapter progress with the implementation ending up with a fully fledged naive trading strategy. We will be designing process supervision trees, describing why specific decisions were taken and how will they impact the system going forward. By any stretch of the imagination, I don‚Äôt believe that ‚Äúthis is the only way‚Äù(nor even the best way) to create a cryptocurrency trading bot in Elixir. This book focuses more on building a real-life project and iterating over it, taking decisions on the way as it would happen in a real work environment. There are parts that will be ‚Äúperfect‚Äù the first time around, but there are also others, where we will take compromises to ‚Äúget it to working‚Äù and then when the time is right, we will refactor them as we will gain a better understanding of the domain problem. Who this book is for This book will be a great resource for everyone that already knows the basics of Elixir and wants to get a feel of how developing a non-trivial system looks like using it. Readers do not need deep knowledge about cryptocurrencies to follow along as I will shy away from crypto / trading jargon as well as will explain it whenever it‚Äôs unavoidable. This is not a book focused on trading strategies, neither it‚Äôs financial advice to trade at all. The main focus of this book is to showcase how the implementation of even complex problems can be achieved in Elixir by simple processes working together in a orchestrated manner. The strategy described in this book is naive and most probably will end up losing money, but that‚Äôs not the point of this book. As we will build up the strategy we will face a spectrum of problems that developers face at work. It‚Äôs a great primer if you want to get your first ‚Äúproject‚Äù behind your belt. So, if you‚Äôve already gone through the motions and learned Elixir and OTP but still feel like you need to get your hands dirty with a ‚Äúreal problem‚Äù to ‚Äúmake it stick‚Äù, this book is for you. What this book covers This book is a loosely written representation of ‚ÄúCreate a cryptocurrency trading bot in Elixir‚Äù video course released on YouTube. It‚Äôs still an ongoing production and will be continued in the upcoming months, at this moment it contains: Chapter 1 - Stream live crypto prices from Binance WSS Stream live cryptocurrency prices (trade events) from the Binance exchange. Starting grounds up, we will create a new umbrella project and a streamer application inside it. The streamer application will use a Websocket client called WebSockex to establish a connection with the Binance API and receive a live feed. After receiving the event as JSON string, we will decode it using the jason library and convert it to our own data struct. At the end of the chapter, we will see decoded trade events being logged to the terminal. Chapter 2 - Create a naive trading strategy - single trader without supervision In this chapter we will create our first naive trading strategy. We will create another application inside our umbrella called naive. We will put data streamed to our streamer application to good use by sending it over to the naive application. We will start with a very basic solution consisting of single process called trader that will utilize the GenServer behaviour. It will allow us to go through the full trading cycle and will give us something that ‚Äúworks‚Äù. Chapter 3 - Introduce PubSub as a communication method To allow our trading strategy to scale to multiple parallel traders, we need to find a way to distribute the latest prices (trade events) to those multiple traders. We will introduce PubSub to broadcast messages from the streamer(s) to the trader(s). PubSub will allow us to break hardcoded references between applications in our umbrella and that will become a pattern that we will utilize moving forward. Chapter 4 - Mock the Binance API Besides historical prices (trade events), to perform backtesting, we need to be able to mock placing orders and get trade events back as they are filled. In this chapter we will focus on developing the solution that will allow our trader to ‚Äútrade‚Äù without contacting the Binance exchange(for people without Binance accounts), it will also allow us to backtest our trading strategy. Chapter 5 - Enable parallel trading on multiple symbols Our basic strategy implementation from the last chapter is definitely too basic to be used in a ‚Äúproduction environment‚Äù - it can‚Äôt be neither scaled nor it is fault-tolerant. In this chapter, we will upgrade our naive strategy to be more resilient. This will require a supervision tree to be created and will allow us to see different supervision strategies in action and understand the motivation behind using and stacking them. Chapter 6 - Introduce a buy_down_interval to make a single trader more profitable At this moment our Naive.Trader implementation will blindly place a buy order at the price of the last trade event. Whenever the Naive.Trader process will finish trade, a new Naive.Trader process will be started and it will end up placing a buy order at the same price as the price of the previous sell order. This will cost us double the fee without gaining any advantage and would cause further complications down the line so we will introduce a buy_down_interval which will allow the Naive.Trader processes to place buy order below the current trade event‚Äôs price. Chapter 7 - Introduce a trader budget and calculating the quantity Since the second chapter our Naive.Trader processes are placing orders with a hardcoded quantity of 100. In this chapter, we will introduce a budget that will be evenly split between the Naive.Trader processes using chunks. We will utilize that budget to calculate quantity (to be able to do that we need to fetch further step_size information from the Binance API) Chapter 8 - Add support for multiple transactions per order Our Naive.Trader implementation assumes that our orders will be filled within a single transaction, but this isn‚Äôt always the case. In this chapter, we will discuss how could we implement the support for multiple transactions per order and race conditions that could occur between the bot and the Binance API. Chapter 9 - Run multiple traders in parallel With PubSub, supervision tree, buy down and budget in place we can progress with scaling the number of traders. This will require further improvements to our trading strategy like introducing a rebuy_interval. At the end of this chapter our trading strategy will be able to start and run multiple traders in parallel. Chapter 10 - Fine-tune trading strategy per symbol Currently naive strategy works based on settings hardcoded in the leader module. To allow for fine-tunning the naive trading strategy per symbol we will introduce a new database together with table that will store trading settings. Chapter 11 - Supervise and autostart streaming In the last chapter we introduced a new database inside the naive application to store default settings, in this chapter we will do the same for the streamer application. Inside the settings there will be a status flag that will allow us to implement the autostarting functionality on initialization using Task abstracting Chapter 12 - Start, stop, shutdown and autostart trading To follow up after autostarting streaming we will apply the same trick to the trading supervision tree using Task abstraction. We will need to introduce new supervision level to achieve correct supervision strategy. Chapter 13 - Abstract duplicated supervision code As both the naive and the streamer application contain almost the same copy-pasted code that allows us to start, stop and autostart workers. We will look into how could we abstract the common parts of that implementation to a single module. We will venture into utilizing the __using__ macro to get rid of the boilerplate. Chapter 14 - Store trade events and orders inside the database To be able to backtest the trading strategy, we need to have historical prices (trade events) and list of orders that were placed stored in the database, which will be the focus of this chapter. At this moment, the latest prices (trade events) are broadcasted to PubSub topic and traders are subscribing to it. We will create a new application called data_warehouse inside our umbrella that will be responsible for subscribing to the same PubSub topics and storing incoming prices (trade events) in the Postgres database. We will update the Naive.Trader module to broadcast orders as traders will place them. Then we will move on to adding supervision similar to the one from the naive and the streamer applications but this time we will show how we could avoid using both common module and macros by utilizing the Registry module. Chapter 15 - Backtest trading strategy In this chapter, we will be backtesting our trading strategy by developing a publisher inside the DataWarehouse application. It will stream trade events from the database to broadcast them to the TRADE_EVENTS:#{symbol} PubSub topic. It will use the same topic as data would be streamed directly from the Binance. From the trader‚Äôs perspective, it won‚Äôt any difference and will cause normal trading activity that will be stored inside the database to be analyzed later. Source code The source code for this book is hosted on Github: https://github.com/frathon/create-a-cryptocurrency-trading-bot-in-elixir-source-code Final code of each chapter has it‚Äôs own branch. "],["stream-live-crypto-prices-from-binance-wss.html", "Chapter 1 Stream live crypto prices from Binance WSS 1.1 Objectives 1.2 Create an umbrella app 1.3 Create a supervised application inside an umbrella 1.4 Connect to Binance‚Äôs WebSocket Stream using the WebSockex module 1.5 Decode incoming events using the Jason module", " Chapter 1 Stream live crypto prices from Binance WSS 1.1 Objectives create an umbrella app create a supervised application inside an umbrella connect to Binance‚Äôs WebSocket Stream using the WebSockex module define a TradeEvent struct that will hold incoming data decode incoming events using the Jason module 1.2 Create an umbrella app In the beginning, there was nothing‚Ä¶ so we need to create an umbrella project: mix new hedgehog --umbrella 1.3 Create a supervised application inside an umbrella We can now proceed with creating a new supervised application called streamer inside our umbrella: cd hedgehog/apps mix new streamer --sup 1.4 Connect to Binance‚Äôs WebSocket Stream using the WebSockex module To establish a connection to Binance API‚Äôs stream, we will need to use a WebSocket client. The module that we will use is called WebSockex. Scrolling down to the Installation section inside module‚Äôs readme on Github, we are instructed what dependency we need to add to our project. We will append :websockex to the deps function inside the mix.exs file of the streamer application: # /apps/streamer/mix.exs defp deps do [ {:websockex, &quot;~&gt; 0.4.2&quot;} ] end As we added a dependency to our project, we need to fetch it using mix deps.get. We can now progress with creating a module that will be responsible for streaming. We will create a new file called binance.ex inside the apps/streamer/lib/streamer directory. From the readme of WebSockex module, we can see that to use it we need to create a module that will implement the WebSockex behavior: # WebSockex&#39;s readme defmodule WebSocketExample do use WebSockex def start_link(url, state) do WebSockex.start_link(url, __MODULE__, state) end def handle_frame({type, msg}, state) do IO.puts &quot;Received Message - Type: #{inspect type} -- Message: #{inspect msg}&quot; {:ok, state} end def handle_cast({:send, {type, msg} = frame}, state) do IO.puts &quot;Sending #{type} frame with payload: #{msg}&quot; {:reply, frame, state} end end We will copy the whole code above across to our new binance.ex file. The first step will be to update the module name to match our file name: # /apps/streamer/lib/streamer/binance.ex defmodule Streamer.Binance do In the spirit of keeping things tidy - we will now remove the handle_cast/2 function (last function in our module) as we won‚Äôt be sending any messages back to Binance via WebSocket (to place orders etc - Binance provides a REST API which we will use in the next chapter). Next, let‚Äôs look up what URL should we use to connect to Binance‚Äôs API. Binance has a separate WSS (Web Socket Streams) documentation at Github Scrolling down we can see the General WSS information section where 3 important pieces of information are listed: The base endpoint is: wss://stream.binance.com:9443 Raw streams are accessed at /ws/ All symbols for streams are lowercase We can see that the full endpoint for raw streams(we will be using a ‚Äúraw‚Äù stream) will be wss://stream.binance.com:9443/ws/ with stream name at the end (together with lowercased symbol). Note: In context of Binance API, ‚Äúraw‚Äù means that no aggregation was performed before broadcasting the data on WebSocket. Let‚Äôs introduce a module attribute that will hold the full raw stream endpoint which will be used across the module: # /apps/streamer/lib/streamer/binance.ex @stream_endpoint &quot;wss://stream.binance.com:9443/ws/&quot; Now back in Binance‚Äôs WSS documentation we need to search for ‚ÄúTrade Streams‚Äù. ‚Äútrade‚Äù in context of this documentation means an exchange of assets(coins/tokens) by two sides (buyer and seller). Our future trading strategy will be interested in the ‚Äúlatest price‚Äù which is simply the last trade event‚Äôs price. We can see that docs are pointing to the following stream name: Stream Name: &lt;symbol&gt;@trade Together, our full URL looks like: ‚Äúwss://stream.binance.com:9443/ws/@trade‚Äù. To give a concrete example: raw trade events stream URL for symbol XRPUSDT is: ‚Äúwss://stream.binance.com:9443/ws/xrpusdt@trade‚Äù (remember that symbols need to be lowercased, otherwise no trade events will get streamed - there‚Äôs no error). Back to the IDE, we will now modify the start_link/2 function to use Binance API‚Äôs URL: # /apps/streamer/lib/streamer/binance.ex def start_link(symbol) do symbol = String.downcase(symbol) WebSockex.start_link( &quot;#{@stream_endpoint}#{symbol}@trade&quot;, __MODULE__, nil ) end Instead of passing an URL, we modified the function to accept a symbol, downcase it and use it together with the module‚Äôs @stream_endpoint attribute to build a full URL. At this moment streaming of trade events already works which we can test using iex: $ iex -S mix ... iex(1)&gt; Streamer.Binance.start_link(&quot;xrpusdt&quot;) {:ok, #PID&lt;0.335.0&gt;} Received Message - Type: :text -- Message: &quot;{\\&quot;e\\&quot;:\\&quot;trade\\&quot;,\\&quot;E\\&quot;:1603226394741,\\&quot;s\\&quot;:\\&quot;XRPUSDT\\&quot;,\\&quot;t\\&quot;:74608889,\\&quot;p\\&quot;:\\&quot;0.24373000\\&quot;,\\&quot;q\\&quot;:\\&quot;200.00000000\\&quot;,\\&quot;b\\&quot;:948244411,\\&quot;a\\&quot;:948244502,\\&quot;T\\&quot;:1603226394739,\\&quot;m\\&quot;:true,\\&quot;M\\&quot;:true}&quot; Received Message - Type: :text -- Message: &quot;{\\&quot;e\\&quot;:\\&quot;trade\\&quot;,\\&quot;E\\&quot;:1603226394741,\\&quot;s\\&quot;:\\&quot;XRPUSDT\\&quot;,\\&quot;t\\&quot;:74608890,\\&quot;p\\&quot;:\\&quot;0.24372000\\&quot;,\\&quot;q\\&quot;:\\&quot;143.20000000\\&quot;,\\&quot;b\\&quot;:948244412,\\&quot;a\\&quot;:948244502,\\&quot;T\\&quot;:1603226394739,\\&quot;m\\&quot;:true,\\&quot;M\\&quot;:true}&quot; We can see the messages logged above because we copied the sample implementation from WebSockex‚Äôs readme where handle_frame/2 function uses IO.puts/1 to print out all incoming data. The lesson here is that every incoming message from Binance will cause the handle_frame/2 callback to be called with the message and the process‚Äô state. Just for reference, our module should look currently as follows: # /apps/streamer/lib/streamer/binance.ex defmodule Streamer.Binance do use WebSockex @stream_endpoint &quot;wss://stream.binance.com:9443/ws/&quot; def start_link(symbol) do symbol = String.downcase(symbol) WebSockex.start_link( &quot;#{@stream_endpoint}#{symbol}@trade&quot;, __MODULE__, nil ) end def handle_frame({type, msg}, state) do IO.puts &quot;Received Message - Type: #{inspect type} -- Message: #{inspect msg}&quot; {:ok, state} end end 1.5 Decode incoming events using the Jason module Currently, all incoming data from WebSocket is encoded as a JSON. To decode JSON we will use the jason module. Scrolling down to the Installation section inside module‚Äôs readme, we can see that we need to add it to the dependencies and we can start to use it right away. Let‚Äôs open the mix.exs file of the streamer application and append the :jason dependency to the list inside deps function: # /apps/streamer/mix.exs defp deps do [ {:jason, &quot;~&gt; 1.2&quot;}, {:websockex, &quot;~&gt; 0.4.2&quot;} ] end As previously, don‚Äôt forget to run mix deps.get to fetch the new dependency. Looking through the documentation of the Jason module we can see encode!/2 and decode!/2 functions, both of them have exclamation marks which indicates that they will throw an error whenever they will be unable to successfully encode or decode the passed value. This is less than perfect for our use case as we would like to handle those errors in our own way(technically we could just use try/rescue but as we will find out both encode/2 and decode/2 are available). We will go a little bit off-topic but I would highly recommend those sorts of journeys around somebody‚Äôs code. Let‚Äôs look inside the Jason module. Scrolling down in search of decode/2 (without the exclamation mark) we can see it about line 54: # /lib/jason.ex def decode(input, opts \\\\ []) do input = IO.iodata_to_binary(input) Decoder.parse(input, format_decode_opts(opts)) end It looks like it uses the parse/2 function of a Decoder module, let‚Äôs scroll back up and check where it‚Äôs coming from. At line 6: # /lib/jason.ex alias Jason.{Encode, Decoder, DecodeError, EncodeError, Formatter} we can see that Decoder is an alias of the Jason.Decoder. Scrolling down to the Jason.Decoder module we will find a parse/2 function about line 43: # /lib/decoder.ex def parse(data, opts) when is_binary(data) do key_decode = key_decode_function(opts) string_decode = string_decode_function(opts) try do value(data, data, 0, [@terminate], key_decode, string_decode) catch {:position, position} -&gt; {:error, %DecodeError{position: position, data: data}} {:token, token, position} -&gt; {:error, %DecodeError{token: token, position: position, data: data}} else value -&gt; {:ok, value} end end Based on the result of decoding it will either return {:ok, value} or {:error, %Decode.Error{...}} we can confirm that by digging through documentation of the module on hex. Once again, the point of this lengthy investigation was to show that Elixir code is readable and easy to understand so don‚Äôt be thrown off when documentation is a little bit light, quite opposite, contribute to docs and code as you gain an better understanding of the codebase. We can now get back to our Streamer.Binance module and modify the handle_frame/2 function to decode the incoming JSON message. Based on the result of Jason.decode/2 we will either call the process_event/2 function or log an error. Here‚Äôs the new version of the handle_frame/2 function: # /apps/streamer/lib/streamer/binance.ex def handle_frame({_type, msg}, state) do case Jason.decode(msg) do {:ok, event} -&gt; process_event(event) {:error, _} -&gt; Logger.error(&quot;Unable to parse msg: #{msg}&quot;) end {:ok, state} end Please make note that type is now prefixed with an underscore as we aren‚Äôt using it at the moment. Second important thing to note is that we are using Logger so it needs to be required at the begining of the module: # /apps/streamer/lib/streamer/binance.ex require Logger Before implementing the process_event/2 function we need to create a structure that will hold the incoming trade event‚Äôs data. Let‚Äôs create a new directory called binance inside the apps/streamer/lib/streamer/ and new file called trade_event.ex inside it. Our new module will hold all the trade event‚Äôs information but we will also use readable field names(you will see the incoming data below). We can start by writing a skeleton module code: # /apps/streamer/lib/streamer/binance/trade_event.ex defmodule Streamer.Binance.TradeEvent do defstruct [] end We can refer to Binance‚Äôs docs to get a list of fields: { &quot;e&quot;: &quot;trade&quot;, // Event type &quot;E&quot;: 123456789, // Event time &quot;s&quot;: &quot;BNBUSDT&quot;, // Symbol &quot;t&quot;: 12345, // Trade ID &quot;p&quot;: &quot;0.001&quot;, // Price &quot;q&quot;: &quot;100&quot;, // Quantity &quot;b&quot;: 88, // Buyer order ID &quot;a&quot;: 50, // Seller order ID &quot;T&quot;: 123456785, // Trade time &quot;m&quot;: true, // Is the buyer the market maker? &quot;M&quot;: true // Ignore } Let‚Äôs copy them across and convert the comments to update the defstruct inside the Streamer.Binance.TradeEvent module‚Äôs struct to following: # /apps/streamer/lib/streamer/binance/trade_event.ex defstruct [ :event_type, :event_time, :symbol, :trade_id, :price, :quantity, :buyer_order_id, :seller_order_id, :trade_time, :buyer_market_maker ] That‚Äôs all for this struct, we can now get back to implementing the process_event/2 function inside the Streamer.Binance module. We will map every field of the response map to the %Streamer.Binance.TradeEvent struct. A useful trick here would be to copy the list of fields once again from the struct and assign the incoming fields one by one. Inside the header of the function, we will pattern match on event type(a field called ‚Äúe‚Äù in the message) to confirm that indeed we received a trade event). In the end, the process_event/2 function should look as follows: # /apps/streamer/lib/streamer/binance.ex defp process_event(%{&quot;e&quot; =&gt; &quot;trade&quot;} = event) do trade_event = %Streamer.Binance.TradeEvent{ :event_type =&gt; event[&quot;e&quot;], :event_time =&gt; event[&quot;E&quot;], :symbol =&gt; event[&quot;s&quot;], :trade_id =&gt; event[&quot;t&quot;], :price =&gt; event[&quot;p&quot;], :quantity =&gt; event[&quot;q&quot;], :buyer_order_id =&gt; event[&quot;b&quot;], :seller_order_id =&gt; event[&quot;a&quot;], :trade_time =&gt; event[&quot;T&quot;], :buyer_market_maker =&gt; event[&quot;m&quot;] } Logger.debug( &quot;Trade event received &quot; &lt;&gt; &quot;#{trade_event.symbol}@#{trade_event.price}&quot; ) end We added the Logger.debug/2 function to be able to see logs of incoming trade events. Lastly, before testing our implementation, let‚Äôs add a nice interface to our streamer application that allows to start streaming: # /apps/streamer/lib/streamer.ex defmodule Streamer do @moduledoc &quot;&quot;&quot; Documentation for `Streamer`. &quot;&quot;&quot; def start_streaming(symbol) do Streamer.Binance.start_link(symbol) end end The final version of the Streamer.Binance module should look like this. Last step will be to add the Logger configuration into main config/config.exs file. We will set the Logger level to :debug for a moment to be able to see incoming trade events: # /config/config.exs config :logger, level: :debug This finishes the implementation part of this chapter, we can now give our implementation a whirl using iex: $ iex -S mix ... iex(1)&gt; Streamer.start_streaming(&quot;xrpusdt&quot;) {:ok, #PID&lt;0.251.0&gt;} 23:14:32.217 [debug] Trade event received XRPUSDT@0.25604000 23:14:33.381 [debug] Trade event received XRPUSDT@0.25604000 23:14:35.380 [debug] Trade event received XRPUSDT@0.25605000 23:14:36.386 [debug] Trade event received XRPUSDT@0.25606000 As we can see, the streamer is establishing a WebSocket connection with the Binance‚Äôs API and it‚Äôs receiving trade events. It decodes them from JSON to %Streamer.Binance.TradeEvent struct and logs a compiled message. Also, our interface hides implementation details from the ‚Äúuser‚Äù of our application. We will now flip the Logger level back to info so the output won‚Äôt every incoming trade event: # /config/config.exs config :logger, level: :info [Note] Please remember to run mix format to keep things nice and tidy. Source code for this chapter can be found at Github "],["create-a-naive-trading-strategy-a-single-trader-process-without-supervision.html", "Chapter 2 Create a naive trading strategy - a single trader process without supervision 2.1 Objectives 2.2 Initializiation 2.3 How trading strategy will work?", " Chapter 2 Create a naive trading strategy - a single trader process without supervision 2.1 Objectives create another supervised application inside umbrella to store our trading strategy define callbacks for events dependent on state of the trader push events from the streamer app to the naive app 2.2 Initializiation To develop our naive strategy will need to create a new supervised application inside our umbrella project: cd apps mix new naive --sup We can now focus on creating a trader abstraction inside that newly created application. First we need to create a new file called trader.ex inside apps/naive/lib/naive/. Let‚Äôs start with a skeleton of a GenServer: # /apps/naive/lib/naive/trader.ex defmodule Naive.Trader do use GenServer require Logger def start_link(args) do GenServer.start_link(__MODULE__, args, name: :trader) end def init(args) do {:ok, args} end end Our module uses the GenServer behaviour and to fulfill it‚Äôs ‚Äúcontract‚Äù, we need to implement the init/1 function. The start_link/1 function is a convention and it allows us to register the process with a name(it‚Äôs a default function that supervisor will use when starting the Trader). We also add a require Logger as we will keep on logging across the module. Next, let‚Äôs model the state of our server: # /apps/naive/lib/naive/trader.ex defmodule State do @enforce_keys [:symbol, :profit_interval, :tick_size] defstruct [ :symbol, :buy_order, :sell_order, :profit_interval, :tick_size ] end Our trader needs to know: - what symbol does it need to trade (‚Äúsymbol‚Äù here is a pair of assets for example ‚ÄúXRPUSDT‚Äù, which is XRP to/from USDT) - placed buy order (if any) - placed sell order (if any) - profit interval (what net profit % we would like to achieve when buying and selling an asset - single trade cycle) - tick_size (yes, I know, jargon warning. We can‚Äôt ignore it here as it needs to be fetched from Binance and it‚Äôs used to calculate a valid price. Tick size differs between symbols and it is a smallest acceptable price movement up or down. For example in physical world tick size for USD is a single cent, you can‚Äôt sell something for $1.234, it‚Äôs either $1.23 or $1.24 (one cent difference between those is the tick size) - more info here Our strategy won‚Äôt be able to work without symbol, profit_interval nor tick_size so we added them to the @enforce_keys attribute. This will ensure that we won‚Äôt create an invalid %State{} without those values. As now we know that our GenServer will need to receive those details via args, we can update pattern matching in start_link/1 and init/1 functions to confirm that passed values are indeed maps: # /apps/naive/lib/naive/trader.ex def start_link(%{} = args) do ... end def init(%{symbol: symbol, profit_interval: profit_interval}) do ... end As we are already in the init/1 function we will need to modify it to fetch the tick_size for the passed symbol and initialize a fresh state: # /apps/naive/lib/naive/trader.ex def init(%{symbol: symbol, profit_interval: profit_interval}) do symbol = String.upcase(symbol) Logger.info(&quot;Initializing new trader for #{symbol}&quot;) tick_size = fetch_tick_size(symbol) {:ok, %State{ symbol: symbol, profit_interval: profit_interval, tick_size: tick_size }} end We are uppercasing the symbol above as Binance‚Äôs REST API accepts only uppercased symbols. It‚Äôs the time to connect to Binance‚Äôs REST API. The easiest way to do that will be to use the binance module. As previously, looking through the module‚Äôs docs on Github, we can see the Installation section. We will follow the steps mentioned there, starting from adding binance to the deps in /apps/naive/mix.exs: # /apps/naive/mix.ex defp deps do [ {:binance, &quot;~&gt; 0.7.1&quot;}, {:decimal, &quot;~&gt; 2.0&quot;}, {:streamer, in_umbrella: true} ] end Beside adding the :binance module, we also added :decimal and the :streamer. The decimal module will help us to calculate the buy and sell prices (without decimal module we would have problems with precision). Lastly, we need to include the :streamer application(created in the first chapter) as we will use the %Streamer.Binance.TradeEvent{} struct inside the naive application. We need to run mix deps.get to install our new deps. We can now get back to the trader module and focus on fetching the tick size from Binance: # /apps/naive/lib/naive/trader.ex defp fetch_tick_size(symbol) do Binance.get_exchange_info() |&gt; elem(1) |&gt; Map.get(:symbols) |&gt; Enum.find(&amp;(&amp;1[&quot;symbol&quot;] == symbol)) |&gt; Map.get(&quot;filters&quot;) |&gt; Enum.find(&amp;(&amp;1[&quot;filterType&quot;] == &quot;PRICE_FILTER&quot;)) |&gt; Map.get(&quot;tickSize&quot;) |&gt; Decimal.new() end We are using get_exchange_info/0 to fetch list of symbols, that we will filter to find the symbol that we are requested to trade. Tick size is defined as a PRICE_FILTER filter. Here‚Äôs the link to the documentation listing all keys in the result. In a nutshell, that‚Äôs how the important parts of the result looks like: {:ok, %{ ... &quot;symbols&quot;: [ %{ &quot;symbol&quot;: &quot;ETHUSDT&quot;, ... &quot;filters&quot;: [ ... %{&quot;filterType: &quot;PRICE_FILTER&quot;, &quot;tickSize&quot;: tickSize, ...} ], ... } ] }} 2.3 How trading strategy will work? Our trader process has an internal state that will serve as a indicator where in the trade cycle is it. The following diagram shows 3 possible trader states that trader will progress through from left to right: Our trader will be receiving trade events sequentially and take decisions based on it‚Äôs own state and trade event‚Äôs contents. We will focus on a trader in 3 different states: * a new trader without any orders * a trader with a buy order placed * a trader with a sell order placed. First state - A new trader Trader doesn‚Äôt have any open orders which we can confirm by pattern matching on the buy_order field from its state. From the incoming event, we can get the current price which we will use in the buy order that the trader will place. Second state - Buy order placed After placing a buy order, trader will be pattern matching to confirm that has incoming event filled his buy order otherwise ignoring it. When trade event matching the order id of trader‚Äôs buy order will arrive, it means that buy order got filled(simplification - our order could be filled in two or more transactions but implementation in this chapter won‚Äôt cater for this case, it will always assume that it got filled in a single transaction) and trader can now place the sell order based on the expected profit and the buy_price. Third state - Sell order placed Finally, in a very similar fashion to previous state, the trader will be pattern matching to confirm that incoming event has filled his sell order(matching order id), otherwise ignoring it. When trade event matching the order id of trader‚Äôs sell order will arrive, that means that sell order got filled(simplification as described above) and full trade cycle has ended and the trader can now exit. 2.3.1 Implementation of the first scenario Enough theory :) back to the editor, we will implement the first scenario. Before doing that let‚Äôs alias Streamer‚Äôs TradeEvent struct as we will rely on it heavily in pattern matching. # /apps/naive/lib/naive/trader.ex alias Streamer.Binance.TradeEvent We are also aliasing the %Streamer.Binance.TradeEvent{} struct as we will rely on it heavily in pattern matching. To confirm that we are dealing with a ‚Äúnew‚Äù trader, we will pattern match on buy_order: nil from it‚Äôs state: # /apps/naive/lib/naive/trader.ex def handle_cast( %TradeEvent{price: price}, %State{symbol: symbol, buy_order: nil} = state ) do quantity = 100 # &lt;= Hardcoded until chapter 7 Logger.info(&quot;Placing BUY order for #{symbol} @ #{price}, quantity: #{quantity}&quot;) {:ok, %Binance.OrderResponse{} = order} = Binance.order_limit_buy(symbol, quantity, price, &quot;GTC&quot;) {:noreply, %{state | buy_order: order}} end For the time being, we will keep the quantity hardcoded as this chapter will get really long otherwise - don‚Äôt worry, we will refactor this in one of the next chapters. After confirming that we deal with the ‚Äúnew‚Äù trader(by pattern matching on the buy_order field from state), we can safely progress to placing a new buy order. We just need to remember to return the updated state as otherwise, trader will go on a shopping spree, as every next incoming event will cause further buy orders(above pattern match will continue to be successful). 2.3.2 Implementation of the second scenario With that out of the way, we can now move on to monitoring for event that matches our buy order id and quantity to confirm that our buy order got filled: # /apps/naive/lib/naive/trader.ex def handle_cast( %TradeEvent{ buyer_order_id: order_id, quantity: quantity }, %State{ symbol: symbol, buy_order: %Binance.OrderResponse{ price: buy_price, order_id: order_id, orig_qty: quantity }, profit_interval: profit_interval, tick_size: tick_size } = state ) do sell_price = calculate_sell_price(buy_price, profit_interval, tick_size) Logger.info( &quot;Buy order filled, placing SELL order for &quot; &lt;&gt; &quot;#{symbol} @ #{sell_price}), quantity: #{quantity}&quot; ) {:ok, %Binance.OrderResponse{} = order} = Binance.order_limit_sell(symbol, quantity, sell_price, &quot;GTC&quot;) {:noreply, %{state | sell_order: order}} end We will implement calculating sell price in a sepearate function based on buy price, profit interval and tick_size. Our pattern match will confirm that indeed our buy order got filled(order_id and quantity matches) so we can now we proceed with placing a sell order using calculated sell price and quantity retrieved from buy order. Again, don‚Äôt forget to return the updated state as otherwise, trader will keep on placing sell orders for every incoming event. To calculate the sell price we will need to use precise math and that will require a custom module. We will use the Decimal module, so first, let‚Äôs alias it at the top of the file: # /apps/naive/lib/naive/trader.ex alias Decimal, as: D Now to calculate the correct sell price, we can use the following formula which gets me pretty close to expected value: # /apps/naive/lib/naive/trader.ex defp calculate_sell_price(buy_price, profit_interval, tick_size) do fee = D.new(&quot;1.001&quot;) original_price = D.mult(D.new(buy_price), fee) net_target_price = D.mult( original_price, D.add(&quot;1.0&quot;, profit_interval) ) gross_target_price = D.mult(net_target_price, fee) D.to_float( D.mult( D.div_int(gross_target_price, tick_size), tick_size ) ) end First, we would like to convert all the numbers to the Decimal structs so it will be easier to work with them. We will also hardcode the fee which we will refactor in one of the future chapters. We started by calculating the gross_buy_price which is a sum of buy price together with the fee that we paid on top of it. Next, we enlarge the originally paid price by profit interval to get net_target_price As we will be charged a fee for selling, we need to add the fee again on top of this target sell price(gross_target_price). Next, we will use the tick size as Binance won‚Äôt accept any prices that aren‚Äôt divisible by the symbols‚Äô tick sizes so we need to ‚Äúnormalize‚Äù them on our side. 2.3.3 Implementation of the third scenario Getting back to handling incoming events, we can now add a clause for a trader that wants to confirm that his sell order was filled: # /apps/naive/lib/naive/trader.ex def handle_cast( %TradeEvent{ seller_order_id: order_id, quantity: quantity }, %State{ sell_order: %Binance.OrderResponse{ order_id: order_id, orig_qty: quantity } } = state ) do Logger.info(&quot;Trade finished, trader will now exit&quot;) {:stop, :normal, state} end When the sell order was successfully filled(confirmed by pattern matching above), there‚Äôs nothing else to do for the trader, so it can retrun a tuple with :stop atom which will cause the trader process to terminate. 2.3.4 Implementation fallback scenario A final callback function that we will need to implement will just ignore all incoming events as they were not matched by any of the previous pattern matches: # /apps/naive/lib/naive/trader.ex def handle_cast(%TradeEvent{}, state) do {:noreply, state} end We need this callback for cases where our trader has an ‚Äúopen‚Äù order(not yet filled) and the incoming event has nothing to do with it, so it needs to be ignored. 2.3.5 Updating the Naive interface Now we will update an interface of our naive application by modifying the Naive module to allow to send an event to the trader: # /apps/naive/lib/naive.ex defmodule Naive do @moduledoc &quot;&quot;&quot; Documentation for `Naive`. &quot;&quot;&quot; alias Streamer.Binance.TradeEvent def send_event(%TradeEvent{} = event) do GenServer.cast(:trader, event) end end We will use the fact that we registered our trader process with a name to be able to cast a message to it. 2.3.6 Updating streamer app To glue our apps together for the time and keep things simple in this chapter we will modify the streamer process to simply call our new Naive interface directly by appending a following function call at the end of process_event/1 function inside the Streamer.Binance module: # /apps/streamer/lib/streamer/binance.ex defp process_event(%{&quot;e&quot; =&gt; &quot;trade&quot;} = event) do ... Naive.send_event(trade_event) end This creates a two way link between the streamer and the naive app. In the next chapter we will fix that as in the perfect world those apps shouldn‚Äôt even be aware of existance of each other. 2.3.7 Access details to Binance Inside the main config of our umbrella project we need to define access details for our Binance account: config :binance, api_key: &quot;YOUR-API-KEY-HERE&quot;, secret_key: &quot;YOUR-SECRET-KEY-HERE&quot; Important note: To be able to run below test and perform real trades, Binance account is required with balance of at least 20 USDT. In the 4th chapter we will focus on creating a BinanceMock that will allow us to run our bot without requirement for a real Binance account. You don‚Äôt need to test run it now if you don‚Äôt need/want to have an account. 2.3.8 Test run Now it‚Äôs the time to give our implementation a run for it‚Äôs money. Once again, to be able to do that you will need to have at least 20 USDT tokens in your Binance‚Äôs wallet and you will loose just under 0.5% of your USDTs(as ‚Äúexpected profit‚Äù is below 0 to quickly showcase the full trade cycle) in the following test: $ iex -S mix ... iex(1)&gt; Naive.Trader.start_link(%{symbol: &quot;XRPUSDT&quot;, profit_interval: Decimal.new(&quot;-0.01&quot;)}) 13:45:30.648 [info] Initializing new trader for XRPUSDT {:ok, #PID&lt;0.355.0&gt;} iex(2)&gt; Streamer.start_streaming(&quot;xrpusdt&quot;) {:ok, #PID&lt;0.372.0&gt;} iex(3)&gt; 13:45:32.561 [info] Placing BUY order for XRPUSDT @ 0.25979000, quantity: 100 13:45:32.831 [info] Buy order filled, placing SELL order for XRPUSDT @ 0.2577, quantity: 100 13:45:33.094 [info] Trade finished, trader will now exit After starting the IEx session, start the trader process with a map containing symbol and profit interval. To be able to quickly test full trade cycle we will pass sub-zero profit interval instead of waiting for the increase in price. Next, we will start streaming on the same symbol, please be aware that this will cause an immediate reaction in the trader process. We can see that our trader placed a buy order at 25.979c per XRP, it was filled in under 300ms, so then the trader placed a sell order at ~25.77c which was also filled in under 300ms. This way the trader finished the trade cycle and process can terminate. That‚Äôs it. Congratulations! You just made your first algorithmic trade and you should be proud of that! In the process of creating that algorithm we touched on multiple topics including GenServer and depending on it‚Äôs state and external data (trade events) to perform different actions - this is a very common workflow that Elixir engineers are following and it‚Äôs great to see it in action. [Note] Please remember to run mix format to keep things nice and tidy. Source code for this chapter can be found at Github "],["introduce-pubsub-as-a-communication-method.html", "Chapter 3 Introduce PubSub as a communication method 3.1 Objectives 3.2 Design 3.3 Implementation", " Chapter 3 Introduce PubSub as a communication method 3.1 Objectives consider reasons why introducing a PubSub communication would be benefitial implement the PubSub communication between the Streamer.Binance and the Naive.Trader(s) 3.2 Design First, let‚Äôs look at the current situation: Current situation We started with the Binance streamer calling the send_event/1 function on the Naive module. The Naive module then calls the trader process using the GenServer‚Äôs cast/2 function(via it‚Äôs registered name). Next step in the process of extending our trading strategy will be to scale it to run multiple Naive.Trader processes in parallel. To be able to do that we will need to remove the option to register the trader process with a name(as only one process can be registered under single name). Multiple traders can‚Äôt have the same process names The second issue with that design was the fact that the Streamer needs to be aware of all processes that are interested in the streamed data and explicitly push that information to them. To fix those issues we will invert the design and introduce a PubSub mechanism: PubSub introduced to invert the dependency and enable multiple parallel traders The streamer will broadcast trade events to the PubSub topic and whatever is interested in that data, can subscribe to the topic and it will receive the broadcasted messages. There‚Äôs no coupling between the Streamer and Naive app any more. We can now introduce multiple traders that will subscribe to the topic and they will receive messages from the PubSub: Multiple traders subscribing to PubSub Going even further down the line we can picture that system could consist of other processes interested in the streamed data. Example of those could be a process that will save all streamed information to the database to be utilized in backtesting later on: Multiple subscribers including traders and other processes 3.3 Implementation We will start by adding a Phoenix.PubSub library to both Streamer and Naive app(as both will be using it, Streamer app as a broadcaster and Naive app as a subscriber) Scrolling down through it‚Äôs readme on GitHub we can see that we need to add :phoenix_pubsub to list of dependencies: # /apps/streamer/mix.exs &amp; /apps/naive/mix.exs defp deps do [ ... {:phoenix_pubsub, &quot;~&gt; 2.0&quot;}, ... ] end Remember to place it so the list will keep alphabetical order. Second step in the readme says that we need to add PubSub as a child of our app. We need to decide where we will put it, Streamer sounds like a good starting point. We will modify the /apps/streamer/lib/streamer/application.ex module by appending the PubSub to it: # /apps/streamer/lib/streamer/application.ex def start(_type, _args) do children = [ { Phoenix.PubSub, name: Streamer.PubSub, adapter_name: Phoenix.PubSub.PG2 } ] ... end We will add the :adapter_name option to instruct PubSub to use pg adapter, which will give us distrubuted process groups. We will now modify the streamer to broadcast a message to PubSub topic instead of using the Naive module‚Äôs function: # /apps/streamer/lib/streamer/binance.ex defp process_event(...) do ... Phoenix.PubSub.broadcast( Streamer.PubSub, &quot;TRADE_EVENTS:#{trade_event.symbol}&quot;, trade_event ) end Inside the trader on init we need to subscribe to the ‚ÄúTRADE_EVENTS‚Äù PubSub channel: # /apps/naive/lib/naive/trader.ex def init(...) do ... Phoenix.PubSub.subscribe( Streamer.PubSub, &quot;TRADE_EVENTS:#{symbol}&quot; ) ... end Next, we need to convert all handle_call callbacks to handle_info inside our Trader module as PubSub doesn‚Äôt use GenServer.cast/2 to send messages over to subscribers. The final change will be to remove the send_event function from the Naive module as it‚Äôs no longer required. Our update is now finished so we can start an iex session to see how it works. First, we will start a streamer process that will broadcast messages to PubSub. Next, we will start trading on the same symbol. On init, trader will subscribe to a PubSub channel and it will make a full trade cycle. $ iex -S mix ... iex(1)&gt; Streamer.start_streaming(&quot;xrpusdt&quot;) {:ok, #PID&lt;0.483.0&gt;} iex(2)&gt; Naive.Trader.start_link(%{symbol: &quot;XRPUSDT&quot;, profit_interval: Decimal.new(&quot;-0.01&quot;)}) 23:46:37.482 [info] Initializing new trader for XRPUSDT {:ok, #PID&lt;0.474.0&gt;} 23:46:55.179 [info] Placing BUY order for XRPUSDT @ 0.29462000, quantity: 100 23:46:55.783 [info] Buy order filled, placing SELL order for XRPUSDT @ 0.29225), quantity: 100.00000000 23:46:56.029 [info] Trade finished, trader will now exit This shows that new trader process successfully subscribed to the PubSub, received the broadcasted messages, placed buy/sell orders and terminated after full trade cycle finished. [Note] Please remember to run mix format to keep things nice and tidy. Source code for this chapter can be found at Github "],["mock-the-binance-api.html", "Chapter 4 Mock the Binance API 4.1 Objectives 4.2 Design 4.3 Create ‚ÄúBinanceMock‚Äù app 4.4 Implement getting exchange info 4.5 Implement placing buy and sell orders 4.6 Implement order retrival 4.7 Implement callback for incoming trade events 4.8 Upgrade trader and config 4.9 Test the implementation", " Chapter 4 Mock the Binance API 4.1 Objectives design the binance mock application create a new app implement getting exchange info implement placing buy and sell orders implement callback for incoming trade events upgrade trader and config test the implementation 4.2 Design First, let‚Äôs start with the current state: Current state of affairs Currently our trader is using the Binance module to place buy/sell orders and get exchange info. The get_exchange_info/0 function doesn‚Äôt require a Binance account as it‚Äôs a publicly available information so we can call the Binance lib directly from our module. The remaining ones(buying/selling) require Binance account and some coins/token inside it‚Äôs wallet. We need to mock those inside our module. We will update the trader to fetch the Binance‚Äôs module name from the config: Binance client dictated by config We will set up a config so it points to the Binance client to be used - either Binance or BinanceMock. Regards the BinanceMock itself it will have the same interface as the Binance module. It will need to store both buy and sell orders and it will allow us to retrieve them. That will cover the REST functions but Binance also streams back trade events for those orders as they get filled, that‚Äôs why BinanceMock will also need to broadcast fake events to the ‚ÄúTRADE_EVENTS:#{symbol}‚Äù PubSub topic so the trader will pick them up: BinanceMock needs to broadcast fake trade events back to the PubSub When exactly should we broadcast those fake trade events? Well, the best thing that we can do is make BinanceMock process to subscribe to the trade events stream and try to broadcast fake trade events whenever the price of orders would be matched: When to send fake trade events Starting from the arrow on the left, our naive strategy will place an order at the current price. In this hypotetical scenario price was raising for a moment after placing the buy order, so BinanceMock will keep on waiting until a trade event will get broadcasted from the PubSub with price below the buy order‚Äôs price. At that moment BinanceMock will generate a fake trade event and broadcast it to the same PubSub topic. Trader will get that event and assume that it came from the Binance and that the buy order got filled so it will place a sell order. Similarly to the buy order, BinanceMock will keep on waiting until a trade event will get broadcasted from the PubSub with the price above the sell order‚Äôs price. At that moment BinanceMock will generate a fake trade event and broadcast it to the same PubSub topic. Enough theory for now, let‚Äôs get our hands dirty with some coding 4.3 Create ‚ÄúBinanceMock‚Äù app We will start by creating a new supervised app called BinanceMock: $ cd apps $ mix new binance_mock --sup The next step will be to update the BinanceMock module to be a GenServer. We will utilize: * the Decimal module for comparing the prices * the Logger module to log As well as we will define internal %State{} struct that will hold: - map called order_books for each traded symbol - list of symbols that mock subscribed to - last generated id - for consistent generating of unique ids for fake trade events order_books map will consist of :\"#{symbol} =&gt; %OrderBook{}. We will define the %OrderBook{} struct as 3 lists buy_side, sell_side and historical: # /apps/binance_mock/lib/binance_mock.ex defmodule BinanceMock do use GenServer alias Decimal, as: D require Logger defmodule State do defstruct order_books: %{}, subscriptions: [], fake_order_id: 1 end defmodule OrderBook do defstruct buy_side: [], sell_side: [], historical: [] end def start_link(_args) do GenServer.start_link(__MODULE__, nil, name: __MODULE__) end def init(_args) do {:ok, %State{}} end end 4.4 Implement getting exchange info As it was mentioned before, to retrieve exchange info we can just call Binance‚Äôs function directly as it‚Äôs a publicly avaiable information: # /apps/binance_mock/lib/binance_mock.ex def get_exchange_info() do Binance.get_exchange_info() end 4.5 Implement placing buy and sell orders For buy and sell limit orders we will write a helper function as the logic is the same for both order sides: # /apps/binance_mock/lib/binance_mock.ex def order_limit_buy(symbol, quantity, price, &quot;GTC&quot;) do order_limit(symbol, quantity, price, &quot;BUY&quot;) end def order_limit_sell(symbol, quantity, price, &quot;GTC&quot;) do order_limit(symbol, quantity, price, &quot;SELL&quot;) end The ‚Äúorder_limit‚Äù helper function will: - ensure that quantity and price are float values as the Binance module accepts both strings and floats - generate a fake order based on symbol, quantity, price, and side - cast a message to the BinanceMock process to add the fake order - return with a tuple with %OrderResponse{} struct to be consistent with the Binance module: # /apps/binance_mock/lib/binance_mock.ex defp order_limit(symbol, quantity, price, side) do quantity = Float.parse(&quot;#{quantity}&quot;) |&gt; elem(0) price = Float.parse(&quot;#{price}&quot;) |&gt; elem(0) %Binance.Order{} = fake_order = generate_fake_order( symbol, quantity, price, side ) GenServer.cast( __MODULE__, {:add_order, fake_order} ) {:ok, convert_order_to_order_response(fake_order)} end We can now move on to the implementation of the handle_cast/2 callback to :add_order to the order book for the symbol from the order. It needs to do two things: - subscribe to the TRADE_EVENTS:#{symbol} topic for the symbol from the order - add the order to the correct order book # /apps/binance_mock/lib/binance_mock.ex def handle_cast( {:add_order, %Binance.Order{symbol: symbol} = order}, %State{ order_books: order_books, subscriptions: subscriptions } = state ) do new_subscriptions = subscribe_to_topic(symbol, subscriptions) updated_order_books = add_order(order, order_books) { :noreply, %{ state | order_books: updated_order_books, subscriptions: new_subscriptions } } end We will start with the implementation of subscribe_to_topic/2 function. We need to make sure that the symbol is uppercase‚Äôd as well as check have we already subscribed to that topic. Otherwise, we can safely use the PubSub module to subscribe to the TRADE_EVENTS:#{symbol} topic for this symbol. We need to remember to append the symbol to the list of subscription and return the updated list: # /apps/binance_mock/lib/binance_mock.ex defp subscribe_to_topic(symbol, subscriptions) do symbol = String.upcase(symbol) stream_name = &quot;TRADE_EVENTS:#{symbol}&quot; case Enum.member?(subscriptions, symbol) do false -&gt; Logger.debug(&quot;BinanceMock subscribing to #{stream_name}&quot;) Phoenix.PubSub.subscribe( Streamer.PubSub, stream_name ) [symbol | subscriptions] _ -&gt; subscriptions end end Next, time for implementation of add_order function. First, we need to get the order book for the symbol of the order. Depends on the side of the order we will update either buy_side or sell_side list remembering that both sides are sorted. We are sorting them so we can easily grab all orders that should be filled whenever trade event arrived, this will become clearer as we will write a handle callback for incoming trade events: # /apps/binance_mock/lib/binance_mock.ex defp add_order( %Binance.Order{symbol: symbol} = order, order_books ) do order_book = Map.get( order_books, :&quot;#{symbol}&quot;, %OrderBook{} ) order_book = if order.side == &quot;SELL&quot; do Map.replace!( order_book, :sell_side, [order | order_book.sell_side] |&gt; Enum.sort(&amp;D.lt?(D.new(&amp;1.price), D.new(&amp;2.price))) ) else Map.replace!( order_book, :buy_side, [order | order_book.buy_side] |&gt; Enum.sort(&amp;D.gt?(D.new(&amp;1.price), D.new(&amp;2.price))) ) end Map.put(order_books, :&quot;#{symbol}&quot;, order_book) end Now we need to follow up and implement the functions that we referred to previously - those are generate_fake_order and convert_order_to_order_response. Starting with the generate_fake_orders, it‚Äôs a function that takes a symbol, quantity, price and side and based on those values returns a Binance.Order struct. To return the struct we will need to generate a unique id for each faked order - this is where fake_order_id will be used(callback implemented later). This way we will be able to run tests multiple times using the BinanceMock and always get the same ids: # /apps/binance_mock/lib/binance_mock.ex defp generate_fake_order(symbol, quantity, price, side) when is_binary(symbol) and is_float(quantity) and is_float(price) and (side == &quot;BUY&quot; or side == &quot;SELL&quot;) do current_timestamp = :os.system_time(:millisecond) order_id = GenServer.call(__MODULE__, :generate_id) client_order_id = :crypto.hash(:md5, &quot;#{order_id}&quot;) |&gt; Base.encode16() Binance.Order.new(%{ symbol: symbol, order_id: order_id, client_order_id: client_order_id, price: Float.to_string(price), orig_qty: Float.to_string(quantity), executed_qty: &quot;0.00000000&quot;, cummulative_quote_qty: &quot;0.00000000&quot;, status: &quot;NEW&quot;, time_in_force: &quot;GTC&quot;, type: &quot;LIMIT&quot;, side: side, stop_price: &quot;0.00000000&quot;, iceberg_qty: &quot;0.00000000&quot;, time: current_timestamp, update_time: current_timestamp, is_working: true }) end We can now focus on converting the Binance.Order to the Binance.OrderResponse struct. As Binance.Order struct contains almost all of the same fields that the Binance.OrderResponse struct, we can use struct function without exclanation mark to ignore all additional fields. The only field that has different name is transact_time field which is called time in the Binance.Order struct - we can fix that separetely: # /apps/binance_mock/lib/binance_mock.ex defp convert_order_to_order_response(%Binance.Order{} = order) do %{ struct( Binance.OrderResponse, order |&gt; Map.to_list() ) | transact_time: order.time } end Last function to finish support for placing buy and sell orders is to add a callback that will iterate the fake order id and return it: # /apps/binance_mock/lib/binance_mock.ex def handle_call( :generate_id, _from, %State{fake_order_id: id} = state ) do {:reply, id + 1, %{state | fake_order_id: id + 1}} end 4.6 Implement order retrival We can now move on to retrieving the orders. First, we need to add an interface function that will call our BinanceMock genserver: # /apps/binance_mock/lib/binance_mock.ex def get_order(symbol, time, order_id) do GenServer.call( __MODULE__, {:get_order, symbol, time, order_id} ) end The callback itself is pretty straightforward. We will need to get order book for passed symbol. As we don‚Äôt know the order‚Äôs side, we will concat all 3 lists(buy_side, sell_side and historical) and try to find an order that will match passed symbol, time and order_id: # /apps/binance_mock/lib/binance_mock.ex def handle_call( {:get_order, symbol, time, order_id}, _from, %State{order_books: order_books} = state ) do order_book = Map.get( order_books, :&quot;#{symbol}&quot;, %OrderBook{} ) result = (order_book.buy_side ++ order_book.sell_side ++ order_book.historical) |&gt; Enum.find( &amp;(&amp;1.symbol == symbol and &amp;1.time == time and &amp;1.order_id == order_id) ) {:reply, {:ok, result}, state} end 4.7 Implement callback for incoming trade events Finally, we need to handle incoming trade events(streamed from the PubSub topic). We need to implement a callback that will: - get the order book for the symbol from the trade event - use the take_while/2 function on the buy orders with prices that are greater than the current price - we can update their status to filled. - use the take_while/2 function again, this time to sell orders with prices less than the current price, we will also update their statuses to filled. - concat both lists of filled orders, convert them to trade events and broadcast them to the PubSub‚Äôs TRADE_EVENTS topic. - remove the filled orders from buy and sell lists and put them into the historical list. Here we can clearly see the benefit of sorting the lists, we can use functions like take_while/2 and drop/2 instead of filter/2 and reject/2(later ones will go through whole lists which could become a bottleneck when multiple open orders would be active): # /apps/binance_mock/lib/binance_mock.ex def handle_info( %Streamer.Binance.TradeEvent{} = trade_event, %{order_books: order_books} = state ) do order_book = Map.get( order_books, :&quot;#{trade_event.symbol}&quot;, %OrderBook{} ) filled_buy_orders = order_book.buy_side |&gt; Enum.take_while(&amp;D.lt?(D.new(trade_event.price), D.new(&amp;1.price))) |&gt; Enum.map(&amp;Map.replace!(&amp;1, :status, &quot;FILLED&quot;)) filled_sell_orders = order_book.sell_side |&gt; Enum.take_while(&amp;D.gt?(D.new(trade_event.price), D.new(&amp;1.price))) |&gt; Enum.map(&amp;Map.replace!(&amp;1, :status, &quot;FILLED&quot;)) (filled_buy_orders ++ filled_sell_orders) |&gt; Enum.map(&amp;convert_order_to_event(&amp;1, trade_event.event_time)) |&gt; Enum.map(&amp;broadcast_trade_event/1) remaining_buy_orders = order_book.buy_side |&gt; Enum.drop(length(filled_buy_orders)) remaining_sell_orders = order_book.sell_side |&gt; Enum.drop(length(filled_sell_orders)) order_books = Map.replace!( order_books, :&quot;#{trade_event.symbol}&quot;, %{ buy_side: remaining_buy_orders, sell_side: remaining_sell_orders, historical: filled_buy_orders ++ filled_sell_orders ++ order_book.historical } ) {:noreply, %{state | order_books: order_books}} end Inside the callback we referred to two new functions that we will implement now(convert_order_to_event and broadcast_trade_event). Starting with the convert_order_to_event function, it will simply return a new Streamer.Binance.TradeEvent struct filled with data. An interesting thing to observe here is that again all values are predicatable and function will return the same values for the same input - this will become beneficial for backtesting over and over again and comparing the behaviour between runs: # /apps/binance_mock/lib/binance_mock.ex defp convert_order_to_event(%Binance.Order{} = order, time) do %Streamer.Binance.TradeEvent{ event_type: order.type, event_time: time - 1, symbol: order.symbol, trade_id: Integer.floor_div(time, 1000), price: order.price, quantity: order.orig_qty, buyer_order_id: order.order_id, seller_order_id: order.order_id, trade_time: time - 1, buyer_market_maker: false } end Broadcasting trade event to PubSub will be the last function that will finish the implementation of BinanceMock for now. It‚Äôs important to upcase the symbol as we want to be sure that we will match the topic name which is case-sensitive: # /apps/binance_mock/lib/binance_mock.ex defp broadcast_trade_event(%Streamer.Binance.TradeEvent{} = trade_event) do symbol = String.upcase(trade_event.symbol) Phoenix.PubSub.broadcast( Streamer.PubSub, &quot;TRADE_EVENTS:#{symbol}&quot;, trade_event ) end That finishes the BinanceMock implementation. Now, we need to add it to the children list of the application so it starts automatically: # /apps/binance_mock/lib/binance_mock/application.ex ... def start(_type, _args) do children = [ {BinanceMock, []} ] ... end end 4.8 Upgrade trader and config We can move on to the Naive.Trader module where we will add an attribute which will point to the Binance client dictated by config: # /apps/naive/lib/naive/trader.ex @binance_client Application.get_env(:naive, :binance_client) We need to replace all direct calls to the Binance module for calls to the @binance_client attribute inside the Naive.Trader: # /apps/naive/lib/naive/trader.ex ... @binance_client.order_limit_buy( ... @binance_client.order_limit_sell ... @binance_client.get_exchange_info() ... As the Naive.Trader is now relying on the config to specify which Binance client should they use, we need to add it to the config: # /config/config.exs config :naive, binance_client: BinanceMock Last modification to our system will be to modify the mix.exs of the binance_mock app to list all deps required for it to work: # /apps/binance_mock/mix.exs ... defp deps do [ {:binance, &quot;~&gt; 0.7.1&quot;}, {:decimal, &quot;~&gt; 2.0&quot;}, {:phoenix_pubsub, &quot;~&gt; 2.0&quot;}, {:streamer, in_umbrella: true} ] end ... We also add :binance_mock to the list of deps of the naive app(as the Naive app will use either Binance or BinanceMock to ‚Äútrade‚Äù): # /apps/naive/mix.exs ... defp deps do [ ... {:binance_mock, in_umbrella: true} ... ] end ... 4.9 Test the implementation We can now see the BinanceMock in action. First, we will start an iex session and double check that BinanceMock process is alive. $ iex -S mix ... iex(1)&gt; Process.whereis(BinanceMock) #PID&lt;0.320.0&gt; # &lt;- confirms that BinanceMock process is alive iex(2)&gt; Streamer.start_streaming(&quot;xrpusdt&quot;) {:ok, #PID&lt;0.332.0&gt;} iex(3)&gt; Naive.Trader.start_link(%{symbol: &quot;XRPUSDT&quot;, profit_interval: Decimal.new(&quot;-0.001&quot;)}) 00:19:39.232 [info] Initializing new trader for XRPUSDT {:ok, #PID&lt;0.318.0&gt;} 00:19:40.826 [info] Placing BUY order for XRPUSDT @ 0.29520000, quantity: 100 00:19:44.569 [info] Buy order filled, placing SELL order for XRPUSDT @ 0.29549), quantity: 100.0 00:20:09.391 [info] Trade finished, trader will now exit As config already points to it so we can continue as previously by starting the streaming and trading on the symbol. The trader is using the BinanceMock and it looks like everything works as it would be dealing with a real exchange. [Note] Please remember to run mix format to keep things nice and tidy. Source code for this chapter can be found at Github "],["enable-parallel-trading-on-multiple-symbols.html", "Chapter 5 Enable parallel trading on multiple symbols 5.1 Objectives 5.2 Introduction - architectural design 5.3 Implement Naive.SymbolSupervisor 5.4 Implement Naive.Leader", " Chapter 5 Enable parallel trading on multiple symbols 5.1 Objectives design supervision tree that will allow trading using multiple traders in parallel per symbol update application supervisor implement Naive.Server implement Naive.SymbolSupervisor 5.2 Introduction - architectural design In the second chapter we implemented a basic trader which goes through the trading cycle. Inside iEx session we were starting the Naive.Trader process using start_link/1 function: IEx session starting the Trader GenServer.start_link/3 creates a link between IEx‚Äôs process and new Naive.Trader process. Whenever trader terminates(either finishes the trading cycle or there was an error), new one won‚Äôt get started as there‚Äôs no supervision at all. We can do much better than that with a a little bit of help from Elixir and OTP. Let‚Äôs introduce a supervisor above our trader process. It will start a new trader process whenever previous one finished/crashed: Supervisor starting the Trader This looks much better but there are few problems with it. So, when the trader will start to place orders it will be in some state(it will hold buy/sell orders) that the supervisor won‚Äôt be aware of. In case of trader crashing, the supervisor will start a new trader without any knowledge of possibly placed orders or any other information from the state(it will be started with a ‚Äúfresh‚Äù state). To fix that we need to keep a copy of the trader‚Äôs state outside of the trader process - that‚Äôs why we will introduce a new server called Naive.Leader that will keep track of traders‚Äô data: Leader -&gt; Supervisor -&gt; Trader The Naive.Leader will become the interface to start new traders. It will call the start_child/1 function of the Supervisor, then consequently DynamicTraderSupervisor will call the start_link/1 function of our Naive.Trader module. We can also see that our Naive.Trader‚Äôs are now started with the temporary restart option. Setting this option will disallow the Supervisor from restarting the traders on its own. The responsibility of restarting traders will now be shifted to the leader. Leader will monitor the traders and restart them to a correct state when any crashes. As trader state will get updated, it will notify the leader about it‚Äôs new state to be stored. This way whenever trader would crash, leader will be able to start new trader process with last known state. This setup will also allow us to start and supervise multiple traders for a single symbol which our naive strategy will require in the future(next chapter). For each symbol that we will be trading on we need a above trio of services(Leader + DynamicTraderSupervisor + Trader), to effectively initialize(and supervise) them we will add an Naive.SymbolSupervisor that will start both Naive.Leader and `Naive.Dynamic: Symbol supervisor added We will need multiple symbol supervisors, one for each symbol that we would like to trade on. As with traders, they will be dynamically started on demand, this should give us a hint that we need another dynamic supervisor that will supervise symbol supervisors and will be direct child of our Naive.Supervisor(Naive.Application module): Full supervision tree You could ask yourself why we don‚Äôt need some additional server to track which symbols are traded at the moment (in the same way as Naive.Leader tracks Naive.Traders). The answer is that we don‚Äôt need to track them as we register all Naive.SymbolSupervisors with a name containing symbol that they trade on. This way we will always be able to refer to them by registered name instead of pids/refs. Here‚Äôs what happens starting from the top of the graph: - the Naive.Application is our top-level application‚Äôs supervisor for the naive app, it was auto-generated as a part of the naive app - it has a single child Naive.DynamicSymbolSupervisor, which has strategy one_for_one and all of it‚Äôs children are Naive.SymbolSupervisors - Naive.SymbolSupervisor process will start 2 further children: the Naive.Leader and DynamicTraderSupervisor, both created on init - the Naive.Leader will ask DynamicTraderSupervisor to start the Naive.Trader child process(es) This can be a little bit confusing at the moment but it will get a lot easier as we will write the code. Let‚Äôs get to it! 5.2.1 Update application supervisor Let‚Äôs start by adding a Naive.DynamicSymbolSupervisor and a server to the children list of the Naive.Application supervisor: # /apps/naive/lib/naive/application.ex def start(_type, _args) do children = [ { DynamicSupervisor, strategy: :one_for_one, name: Naive.DynamicSymbolSupervisor } ] ... end 5.2.2 Add interface method We will now add an interface method to the Naive module that will instruct Naive.DynamicSymbolSupervisor to start Naive.SymbolSupervisor(to be implemented next) as it‚Äôs child: # /apps/naive/lib/naive.ex def start_trading(symbol) do symbol = String.upcase(symbol) {:ok, _pid} = DynamicSupervisor.start_child( Naive.DynamicSymbolSupervisor, {Naive.SymbolSupervisor, symbol} ) end 5.3 Implement Naive.SymbolSupervisor Next, time for the Naive.SymbolSupervisor, first step will be to create a file called symbol_supervisor.ex inside apps/naive/lib/naive directory. There‚Äôs no point of using the DynamicSupervisor, as we know the children that we would like to start automatically on init. This is a full implementation of the supervisor and it‚Äôs a simple as just listing child processes inside the init function: # /apps/naive/lib/naive/symbol_supervisor.ex defmodule Naive.SymbolSupervisor do use Supervisor require Logger def start_link(symbol) do Supervisor.start_link( __MODULE__, symbol, name: :&quot;#{__MODULE__}-#{symbol}&quot; ) end def init(symbol) do Logger.info(&quot;Starting new supervision tree to trade on #{symbol}&quot;) Supervisor.init( [ { DynamicSupervisor, strategy: :one_for_one, name: :&quot;Naive.DynamicTraderSupervisor-#{symbol}&quot; }, {Naive.Leader, symbol} ], strategy: :one_for_all ) end end It‚Äôs advised to keep supervisor processes slim. We registered the Naive.SymbolSupervisor processes with names, which will help us understand the supervision tree inside the observer GUI(it will also allow us to stop those supervisors in the future). As mentioned previously whenever either the Naive.Leader or Naive.DynamicSymbolSupervisor-#{symbol} would crash we would like to kill the other child process as we won‚Äôt be able to recover the state - it‚Äôs just easier to init both again. 5.4 Implement Naive.Leader It‚Äôs time for the Naive.Leader module, again, first step will be to create a file called leader.ex inside apps/naive/lib/naive directory. At this moment it will be a skeleton GenServer implementation just to get the code to compile: # /apps/naive/lib/naive/leader.ex defmodule Naive.Leader do use GenServer def start_link(symbol) do GenServer.start_link( __MODULE__, symbol, name: :&quot;#{__MODULE__}-#{symbol}&quot; ) end def init(symbol) do {:ok, %{symbol: symbol}} end end At this moment we have half of the supervision tree working so we can give it a spin in iex. Using the observer we will be able to see all processes created when start trading function gets called: $ iex -S mix ... iex(1)&gt; :observer.start() Above function will open a new window looking as follows: New observer window To clearly see the supervision tree we will click on ‚ÄúApplications‚Äù tab at the top - the following tree of processes will be shown on the left: Observer applications list If any other process tree is visible, go to the list on the left and select the naive application. The Naive.Supervisor is our Naive.Application module(you can confirm that by checking the name option send to the start_link function inside the module). It start the Naive.DynamicSymbolSupervisor. We can now call the Naive.start_trading/1 function couple time to see how the tree will look like with additional processes(go back to the iex session): ... iex(2)&gt; Naive.start_trading(&quot;adausdt&quot;) 23:14:40.974 [info] Starting new supervision tree to trade on ADAUSDT {:ok, #PID&lt;0.340.0&gt;} iex(3)&gt; Naive.start_trading(&quot;xrpusdt&quot;) 23:15:12.117 [info] Starting new supervision tree to trade on XRPUSDT {:ok, #PID&lt;0.345.0&gt;} We can see that two new branches were created: - SymbolSupervisor-ADAUSDT - SymbolSupervisor-XRPUSDT Each of them contain a Naive.Leader and DynamicTraderSupervisor. 5.4.1 Updating the leader module Let‚Äôs jump back to extending a leader implementation to get those traders running. We will introduce a leader‚Äôs state that will consist of symbol, setting and a list of traders‚Äô data. Trader data will hold pid, ref and state of the trader: # /apps/naive/lib/naive/leader.ex ... alias Naive.Trader require Logger @binance_client Application.get_env(:naive, :binance_client) defmodule State do defstruct symbol: nil, settings: nil, traders: [] end defmodule TraderData do defstruct pid: nil, ref: nil, state: nil end We will use a handle_continue callback which was introduced in Erlang 21 to initialize the leader asynchronously. To do that we will return a tuple starting with a :continue atom from inside the init function: # /apps/naive/lib/naive/leader.ex def init(symbol) do {:ok, %State{ symbol: symbol }, {:continue, :start_traders}} end The Naive.Leader will fetch symbol settings and based on them, it will build the state for traders so they don‚Äôt need to fetch the same settings again. It will also start as many traders there were set under chunks key in setting: # /apps/naive/lib/naive/leader.ex # below init() def handle_continue(:start_traders, %{symbol: symbol} = state) do settings = fetch_symbol_settings(symbol) trader_state = fresh_trader_state(settings) traders = for _i &lt;- 1..settings.chunks, do: start_new_trader(trader_state) {:noreply, %{state | settings: settings, traders: traders}} end Fetching symbol settings will be hardcoded for time being to keep this chapter focused. We will also move the code responsible for fetching tick size from the Naive.Trader to the Naive.Leader and hardcode the rest of the values: # /apps/naive/lib/naive/leader.ex defp fetch_symbol_settings(symbol) do tick_size = fetch_tick_size(symbol) %{ symbol: symbol, chunks: 1, # -0.12% for quick testing profit_interval: Decimal.new(&quot;-0.0012&quot;), tick_size: tick_size } end defp fetch_tick_size(symbol) do @binance_client.get_exchange_info() |&gt; elem(1) |&gt; Map.get(:symbols) |&gt; Enum.find(&amp;(&amp;1[&quot;symbol&quot;] == symbol)) |&gt; Map.get(&quot;filters&quot;) |&gt; Enum.find(&amp;(&amp;1[&quot;filterType&quot;] == &quot;PRICE_FILTER&quot;)) |&gt; Map.get(&quot;tickSize&quot;) |&gt; Decimal.new() end Additionally we need to create a helper method that we used inside the handle_continue/2 callback called fresh_trader_state/1: # /apps/naive/lib/naive/leader.ex # place this one above the `fetch_symbol_settings` function defp fresh_trader_state(settings) do struct(Trader.State, settings) end Starting a new trader isn‚Äôt any different from the code that we already wrote to start a new Naive.SymbolSupervisor. We need to call the DynamicSupervisor.start_child/2 function and start to monitor the process: # /apps/naive/lib/naive/leader.ex defp start_new_trader(%Trader.State{} = state) do {:ok, pid} = DynamicSupervisor.start_child( :&quot;Naive.DynamicTraderSupervisor-#{state.symbol}&quot;, {Naive.Trader, state} ) ref = Process.monitor(pid) %TraderData{pid: pid, ref: ref, state: state} end 5.4.2 Updating the Naive.Trader module Now we can update the Naive.Trader, first, we will set restart to be temporary to avoid restarting it by the Naive.DynamicTraderSupervisor: # /apps/naive/lib/naive/trader.ex defmodule Naive.Trader do use GenServer, restart: :temporary ... Next, we will update start_link/1 and init/1 to take the state instead of building it from args: # /apps/naive/lib/naive/trader.ex def start_link(%State{} = state) do GenServer.start_link(__MODULE__, state) end def init(%State{symbol: symbol} = state) do symbol = String.upcase(symbol) Logger.info(&quot;Initializing new trader for symbol(#{symbol})&quot;) Phoenix.PubSub.subscribe( Streamer.PubSub, &quot;TRADE_EVENTS:#{symbol}&quot; ) {:ok, state} end Next, we need to update two handle_info/2 callbacks that change the state of the Naive.Trader process(when placing buy order and when placing sell order). They will need to notify the Naive.Leader that state is changed before returning it: # /apps/naive/lib/naive/trader.ex ... def handle_info( ... ) do Logger.info(&quot;Placing buy order (#{symbol}@#{price})&quot;) ... new_state = %{state | buy_order: order} Naive.Leader.notify(:trader_state_updated, new_state) {:noreply, new_state} end def handle_info( ... ) do ... Logger.info(&quot;Buy order filled, placing sell order ...&quot;) ... new_state = %{state | sell_order: order} Naive.Leader.notify(:trader_state_updated, new_state) {:noreply, new_state} end ... 5.4.3 Finalizing Naive.Leader implementation Now we need to get back to the Naive.Leader where we will implement the notifing logic. We will start with the notify function that will just call the Naive.Leader process: # /apps/naive/lib/naive/leader.ex # below init def notify(:trader_state_updated, trader_state) do GenServer.call( :&quot;#{__MODULE__}-#{trader_state.symbol}&quot;, {:update_trader_state, trader_state} ) end Now, it‚Äôs time for a callback function that will handle the trader state update. As this is a handle_call/3 callback we have access to trader pid which sent the notification message. We will try to find that trader in the list of traders. If that‚Äôs successful we will update the cached state for that trader locally: # /apps/naive/lib/naive/leader.ex # below handle_continue def handle_call( {:update_trader_state, new_trader_state}, {trader_pid, _}, %{traders: traders} = state ) do case Enum.find_index(traders, &amp;(&amp;1.pid == trader_pid)) do nil -&gt; Logger.warn( &quot;Tried to update the state of trader that leader is not aware of&quot; ) {:reply, :ok, state} index -&gt; old_trader_data = Enum.at(traders, index) new_trader_data = %{old_trader_data | :state =&gt; new_trader_state} {:reply, :ok, %{state | :traders =&gt; List.replace_at(traders, index, new_trader_data)}} end end Another callback functions that we will need to provide are two handle_info/2 functions that will handle the trade finished scenario as well as crashed trader. First, trade finished scenario. As previously, we will try to find the trader data in the traders list. If that‚Äôs successful, we will start a new trader with fresh state. We will also overwrite existing trader data locally(as pid, ref and state changed): # /apps/naive/lib/naive/leader.ex # below state updated handle_call callback def handle_info( {:DOWN, _ref, :process, trader_pid, :normal}, %{traders: traders, symbol: symbol, settings: settings} = state ) do Logger.info(&quot;#{symbol} trader finished trade - restarting&quot;) case Enum.find_index(traders, &amp;(&amp;1.pid == trader_pid)) do nil -&gt; Logger.warn( &quot;Tried to restart finished #{symbol} &quot; &lt;&gt; &quot;trader that leader is not aware of&quot; ) {:noreply, state} index -&gt; new_trader_data = start_new_trader(fresh_trader_state(settings)) new_traders = List.replace_at(traders, index, new_trader_data) {:noreply, %{state | traders: new_traders}} end end Here we will assume that whenever the reason that the Naive.Trader process died is :normal that means that we stopped it after trade cycle finished. Final callback that we need to provide will handle the scenario where the trader crashed. We would like to find the cached state of the crashed trader and start a new one with the same state and then update the local cache as pid and ref will change for that trader: # /apps/naive/lib/naive/leader.ex # below trade finished handle_info callback def handle_info( {:DOWN, _ref, :process, trader_pid, _reason}, %{traders: traders, symbol: symbol} = state ) do Logger.error(&quot;#{symbol} trader died - trying to restart&quot;) case Enum.find_index(traders, &amp;(&amp;1.pid == trader_pid)) do nil -&gt; Logger.warn( &quot;Tried to restart #{symbol} trader &quot; &lt;&gt; &quot;but failed to find its cached state&quot; ) {:noreply, state} index -&gt; trader_data = Enum.at(traders, index) new_trader_data = start_new_trader(trader_data.state) new_traders = List.replace_at(traders, index, new_trader_data) {:noreply, %{state | traders: new_traders}} end end 5.4.4 IEx testing That finishes the implementation part, let‚Äôs jump into iex session to see how it works. We will start the observer first, then we will start trading on any valid symbol. When our trader will start, you should be able to right click and select ‚ÄúKill process‚Äù(leave the reason as kill) and click ‚ÄúOK‚Äù. At that moment you should see that the PID of the trader changed and we can also see log message from the leader. $ iex -S mix ... iex(1)&gt; :observer.start() :ok iex(2)&gt; Naive.start_trading(&quot;xrpusdt&quot;) 00:04:35.041 [info] Starting new supervision tree to trade on XRPUSDT {:ok, #PID&lt;0.455.0&gt;} 00:04:37.697 [info] Initializing new trader for XRPUSDT iex(3)&gt; 00:08:01.476 [error] XRPUSDT trader died - trying to restart 00:08:01.476 [info] Initializing new trader for XRPUSDT [Note] Please remember to run mix format to keep things nice and tidy. Source code for this chapter can be found at Github "],["chapter-6-introduce-a-buy-down-interval-to-make-a-single-trader-more-profitable.html", "Chapter 6 # Chapter 6 - Introduce a buy_down_interval to make a single trader more profitable 6.1 Objectives 6.2 Why we need to buy below the current price? Feature overview 6.3 Naive.Trader implementation 6.4 Naive.Leader implementation", " Chapter 6 # Chapter 6 - Introduce a buy_down_interval to make a single trader more profitable 6.1 Objectives present reasons why to introduce buy_down_interval add buy_down interval to Naive.Trader‚Äôs state and calculate buy price add buy_down interval to Naive.Trader‚Äôs state compiled by the Naive.Leader manually test the implementation inside iex 6.2 Why we need to buy below the current price? Feature overview Current silly buy price The Naive.Trader process(marked in above diagram with blue color) at the arrival of the first trade event, immediately places a buy order at the current price. At the moment when buy order gets filled, it places sell order which later also gets filled. The Trader A exits and a new trader B is started which again immediately places a buy order at the same price as the previous trader just sold. When this gets filled sell order get‚Äôs placed and loop continues on and on. We can see that there‚Äôs a problem here as we just paid a fee twice(once for selling by the Trader A and once for buying by the Trader B) without really gaining anything(the Trader A could just hold the currency and could simply cash in on double profit in this specific situation). The solution is to be more clever about our buy order‚Äôs price. The idea is simple, instead of placing a new buy order at the current price(price from the last TradeEvent), we will introduce a buy_down_interval: Buy price reduced by buy_down_interval So every new Naive.Trader process as it receives first trade event, trader will take it‚Äôs price and will calculate an decreased price by using the buy_down_interval value(for example 0.005 would be 0.5%) and place a buy order at that calculated price. When looking at the chart above we can figure out that buy_down_interval should never be smaller than double the fee(at the moment of writting transaction fee is 0.1%) that you are paying per transaction. 6.3 Naive.Trader implementation Let‚Äôs open the Naive.Trader module‚Äôs file(/apps/naive/lib/naive/trader.ex) and add buy_down_interval to it‚Äôs state: # /apps/naive/lib/naive/trader.ex ... defmodule State do @enforce_keys [ :symbol, :buy_down_interval, # &lt;= add this line :profit_interval, :tick_size ] defstruct [ :symbol, :buy_order, :sell_order, :buy_down_interval, # &lt;= add this line :profit_interval, :tick_size ] end ... Next, we need to update the initial handle_info/2 callback which places the buy order. We need to retrieve the buy_down_interval and the tick_size from the state of the trader to be able to calculate the buy price. We will put the logic to calculate that price in a separate function at the end of the file: # /apps/naive/lib/naive/trader.ex ... def handle_info( %TradeEvent{price: price}, %State{ symbol: symbol, buy_order: nil, buy_down_interval: buy_down_interval, # &lt;= add this line tick_size: tick_size # &lt;= add this line } = state ) do price = calculate_buy_price(price, buy_down_interval, tick_size) # ^ add above call ... To calculate the buy price we will use a very similar method to the one used before to calculate the sell price. First, we will need to cast all variables into the Decimal structs and then, we will simply subtract the buy_down_interval of the price from the price. The number that we will end up with won‚Äôt necessarily be a legal price as every price needs to be divisible by the tick_size which we will assure in the last calculation: # /apps/naive/lib/naive/trader.ex ... defp calculate_buy_price(price, buy_down_interval, tick_size) do current_price = D.new(price) # not necessarily legal price exact_buy_price = D.sub( current_price, D.mult(current_price, buy_down_interval) ) D.to_float( D.mult( D.div_int(exact_buy_price, tick_size), tick_size ) ) end ... 6.4 Naive.Leader implementation Next we need to update the Naive.Leader as it needs to add buy_down_interval to the Naive.Trader‚Äôs state: # /apps/naive/lib/naive/leader.ex defp fetch_symbol_settings(symbol) do ... %{ chunks: 1, buy_down_interval: Decimal.new(&quot;0.0001&quot;), # &lt;= add this line # -0.12% for quick testing profit_interval: Decimal.new(&quot;-0.0012&quot;), tick_size: tick_size } end ... 6.4.1 IEx testing That finishes the buy_down_interval implementation, we will jump into iex session to see how it works, but before that for a moment we will change the logging level to debug to see current prices: # config/config.exs ... config :logger, level: :debug # &lt;= updated for our manual test ... After starting the streaming we should start seeing log messages with current prices. As we updated our implementation we should place our buy order below current price as it‚Äôs visible below: $ iex -S mix ... iex(1)&gt; Streamer.start_streaming(&quot;FLMUSDT&quot;) {:ok, #PID&lt;0.313.0&gt;} iex(2)&gt; Naive.start_trading(&quot;FLMUSDT&quot;) 21:16:14.829 [info] Starting new supervision tree to trade on FLMUSDT ... 21:16:16.755 [info] Initializing new trader for FLMUSDT ... 21:16:20.000 [debug] Trade event received FLMUSDT@0.15180000 21:16:20.009 [info] Placing BUY order for FLMUSDT @ 0.1517, quantity: 100 As we can see our Naive.Trader process placed an buy order below the current price (based on most recent trade event received) [Note] Please remember to revert the change to logger level as otherwise there‚Äôs too much noise in the logs. [Note 2] Please remember to run mix format to keep things nice and tidy. Source code for this chapter can be found at Github "],["introduce-a-trader-budget-and-calculating-the-quantity.html", "Chapter 7 Introduce a trader budget and calculating the quantity 7.1 Objectives 7.2 Fetch step_size 7.3 Append budget and step_size to the Trader‚Äôs state inside the Leader 7.4 Append budget and step_size to the Trader‚Äôs state 7.5 Calculate quantity", " Chapter 7 Introduce a trader budget and calculating the quantity 7.1 Objectives fetch step_size append budget and step_size to the Trader‚Äôs state compiled by the Leader append budget and step_size to the Trader‚Äôs state calculate quantity 7.2 Fetch step_size In 2nd chapter we hardcoded quantity to 100, it‚Äôs time to refactor that. We will need step_size information from the Binance which we are already retrieving together with tick_size in the exchangeInfo call(but not getting it out from the response). So we will rename the fetch_tick_size/1 function to fetch_symbol_filters/1 which will allow us to return multiple filters(tick_size and step_size) from that function. # /apps/naive/lib/naive/leader.ex ... defp fetch_symbol_settings(symbol) do symbol_filters = fetch_symbol_filters(symbol) # &lt;= updated fetch_tick_size Map.merge( %{ chunks: 1, budget: 20, buy_down_interval: Decimal.new(&quot;0.0001&quot;), # -0.12% for quick testing profit_interval: Decimal.new(&quot;-0.0012&quot;) }, symbol_filters ) end defp fetch_symbol_filters(symbol) do # &lt;= updated fetch_tick_size symbol_filters = @binance_client.get_exchange_info() |&gt; elem(1) |&gt; Map.get(:symbols) |&gt; Enum.find(&amp;(&amp;1[&quot;symbol&quot;] == symbol)) |&gt; Map.get(&quot;filters&quot;) tick_size = symbol_filters |&gt; Enum.find(&amp;(&amp;1[&quot;filterType&quot;] == &quot;PRICE_FILTER&quot;)) |&gt; Map.get(&quot;tickSize&quot;) |&gt; Decimal.new() step_size = symbol_filters |&gt; Enum.find(&amp;(&amp;1[&quot;filterType&quot;] == &quot;LOT_SIZE&quot;)) |&gt; Map.get(&quot;stepSize&quot;) |&gt; Decimal.new() %{ tick_size: tick_size, step_size: step_size } end Instead of reassigning the filters one by one into the settings, we will merge them together(#1). Additionally, we will introduce a budget(#2) which will be shared across all traders of the symbol. Also, we don‚Äôt need to assign tick_size here as it‚Äôs part of the settings that are merged. 7.3 Append budget and step_size to the Trader‚Äôs state inside the Leader The budget needs to be added to the %State{}(step_size will be automatically passed on by struct/2) of the trader inside fresh_trader_state/1(where we initialize the state of traders). Before we will assign it we need to divide it by number of chunks as each trader gets only a chunk of the budget: # /apps/naive/lib/naive/leader.ex defp fresh_trader_state(settings) do %{ struct(Trader.State, settings) | budget: D.div(settings.budget, settings.chunks) } end In the code above we are using the Decimal module(aliased as D) to calculate the budget - we need to alias it at the top of Naive.Leader‚Äôs file: # /apps/naive/lib/naive/leader.ex defmodule Naive.Leader do use GenServer alias Decimal, as: D # &lt;= add this line alias Naive.Trader ... 7.4 Append budget and step_size to the Trader‚Äôs state We need to add both budget and step_size to the Naive.Trader‚Äôs state struct: # /apps/naive/lib/naive/trader.ex ... defmodule State do @enforce_keys [ :symbol, :budget, # &lt;= add this line :buy_down_interval, :profit_interval, :tick_size, :step_size # &lt;= add this line and comma above ] defstruct [ :symbol, :budget, # &lt;= add this line :buy_order, :sell_order, :buy_down_interval, :profit_interval, :tick_size, :step_size # &lt;= add this line and comma above ] end ... 7.5 Calculate quantity Jumping back to the handle_info/2 where the Naive.Trader places a buy order, we need to pattern match on the step_size and budget then we will be able to swap hardcoded quantity with result of calling the calculate_quantity/3 function: # /apps/naive/lib/naive/trader.ex ... def handle_info( %TradeEvent{price: price}, %State{ symbol: symbol, budget: budget, # &lt;= add this line buy_order: nil, buy_down_interval: buy_down_interval, tick_size: tick_size, step_size: step_size # &lt;= add this line } = state ) do ... quantity = calculate_quantity(budget, price, step_size) ... To calculate quantity we will just divide the budget by the price with a caveat that it‚Äôs possible (as with calculating the price) that it‚Äôs not a legal quantity value as it needs to be divisiable by step_size: # /apps/naive/lib/naive/trader.ex # add below at the bottom of the file ... defp calculate_quantity(budget, price, step_size) do price = D.from_float(price) # not necessarily legal quantity exact_target_quantity = D.div(budget, price) D.to_float( D.mult( D.div_int(exact_target_quantity, step), step ) ) end 7.5.1 IEx testing That finishes the quantity(and budget) implementation, we will jump into iex session to see how it works. First, start the streaming and trading on the same symbol and moment later you should see variable amount of quantity that more or less uses full allowed budget: $ iex -S mix ... iex(1)&gt; Streamer.start_streaming(&quot;XRPUSDT&quot;) {:ok, #PID&lt;0.313.0&gt;} iex(2)&gt; Naive.start_trading(&quot;XRPUSDT&quot;) 21:16:14.829 [info] Starting new supervision tree to trade on XRPUSDT 21:16:16.755 [info] Initializing new trader for XRPUSDT 21:16:20.009 [info] Placing BUY order for XRPUSDT @ 0.29506, quantity: 67.7 21:16:23.456 [info] Buy order filled, placing SELL order for XRPUSDT @ 0.29529, quantity: 67.7 As we can see our Naive.Trader process is now buying and selling based on passed budget. [Note] Please remember to run mix format to keep things nice and tidy. Source code for this chapter can be found at Github "],["add-support-for-multiple-transactions-per-order.html", "Chapter 8 Add support for multiple transactions per order 8.1 Objectives 8.2 Issue with the current implementation 8.3 Improve buy order filled callback 8.4 Implement buy order ‚Äúfilled‚Äù callback 8.5 Improve sell order callback 8.6 Test the implementation", " Chapter 8 Add support for multiple transactions per order 8.1 Objectives describe issue with current implementation improve buy order filled callback implement buy order ‚Äúfilled‚Äù callback improve sell order callback 8.2 Issue with the current implementation Currently, Naive.Trader process is placing a buy order and it‚Äôs assuming that it will be filled by a single opposite sell order(we are pattern matching on quantity to confirm that): Single transaction fills order Here we can see our buy order for 1000 units(on the left) and other trader‚Äôs sell order(on the right) for 1000 units. This(order fully filled in single transaction) is a case most of the time but it‚Äôs not ALWAYS the case. Sometimes our order will be filled by two or more transactions: Multiple transactions fills order The easiest and the safest way to check has this event filled our order fully is to fetch our order again from Binance at the moment when trade event filling our order arrives. Problem with this approach is that sometimes we will run into a race condition: Race condition timeline - two sell orders From the left, first, we are sending a buy order for quantity 1000 to the Binance. It hangs for a while until it gets filled by 2 transactions that happened very quickly. Quickly enough for us to receive both messages almost in the same moment. When our bot will handle the first one it will fetch our the buy order which is already filled. It will cause the trader to place a sell order but then the there‚Äôs another trade event waiting in the message box. It will be handled by another callback that will again fetch order and place another sell order to be placed and that‚Äôs obviously not correct. What we need to do is to update the status of the buy order after the first fetch(if it‚Äôs filled) so when second trade event arrives we will ignore it(this will require additional callback). The same issue will appear when placing sell order and dealing multiple simultaneous transaction. 8.3 Improve buy order filled callback First, we need to modify the callback which monitors incoming trade events for ones filling it‚Äôs buy order and then places sell order. We need to remove pattern matching assuming that a single trade event will fill our buy order - we need to drop quantity check as well as add # /apps/naive/lib/naive/trader.ex def handle_info( %TradeEvent{ buyer_order_id: order_id # &lt;= quantity got removed from here }, %State{ symbol: symbol, buy_order: %Binance.OrderResponse{ price: buy_price, order_id: order_id, orig_qty: quantity, transact_time: timestamp # &lt;= timestamp added to query order } = buy_order, # &lt;= buy order to update it profit_interval: profit_interval, tick_size: tick_size } = state ) do Now we can fetch our buy order to check is it already filled. We will get the Binance.Order struct instead of the Binance.OrderResponse that we normally deal with. At this moment we will simply update our Binance.OrderResponse struct from state: # /apps/naive/lib/naive/trader.ex # inside the same callback def handle_info( ... ) do {:ok, %Binance.Order{} = current_buy_order} = @binance_client.get_order( symbol, timestamp, order_id ) buy_order = %{buy_order | status: current_buy_order.status} ... The rest of the logic inside this callback will depend on the status of the buy order. If our buy order is ‚Äúfilled‚Äù we would like to follow the existing logic but also update buy_order field inside the state of the trader process. On the other hand if our order is not yet filled the only thing to do is to update buy_order field inside the state of the Trader process. Here‚Äôs and updated body below above changes(few variables got renamed for clarity as we are now fetch the order): # /apps/naive/lib/naive/trader.ex # inside the same callback buy_order = .... {:ok, new_state} = if buy_order.status == &quot;FILLED&quot; do sell_price = calculate_sell_price(buy_price, profit_interval, tick_size) Logger.info( &quot;Buy order filled, placing SELL order for &quot; &lt;&gt; &quot;#{symbol} @ #{sell_price}, quantity: #{quantity}&quot; ) {:ok, %Binance.OrderResponse{} = order} = @binance_client.order_limit_sell(symbol, quantity, sell_price, &quot;GTC&quot;) {:ok, %{state | buy_order: buy_order, sell_order: order}} else Logger.info(&quot;Buy order partially filled&quot;) {:ok, %{state | buy_order: buy_order}} end Naive.Leader.notify(:trader_state_updated, new_state) {:noreply, new_state} end As we are branching our logic and both paths are updating the state, we will return it together with an :ok atom to be able to pattern match it and assign as a new state. 8.4 Implement buy order ‚Äúfilled‚Äù callback Above callback covers the case where we will get multiple transactions filling our buy order. We aren‚Äôt yet convering for the race condition described at the begining of this chapter. When another trade event matching buyer_order_id would arrive, above callback would be used and another sell order would be placed. To avoid that we need to add a new callback ABOVE the one that we just edited that will match buyer_order_id together with ‚Äúfilled‚Äù status and it will simply ignore that trade event as we know that sell event needed to be placed by previous trade event: # /apps/naive/lib/naive/trader.ex # place this callback ABOVE callback from previous section def handle_info( %Streamer.Binance.TradeEvent{ buyer_order_id: order_id }, %State{ buy_order: %Binance.OrderResponse{ order_id: order_id, # &lt;= confirms that it&#39;s event for buy order status: &quot;FILLED&quot; # &lt;= confirms buy order filled }, sell_order: %Binance.OrderResponse{} # &lt;= confirms sell order placed } = state ) do {:noreply, state} end 8.5 Improve sell order callback Let‚Äôs move on to the callback where the trader receives trade event matching sell order‚Äôs id (about line 135 inside the Naive.Trader module). We need to modify the header of our callback in the following ways: - drop the both pattern matches on quantity as we already know that trade event could partially fill our order (#1) - get symbol out of state (#2) - get transact_time out of the sell_order (used to fetch get_order) (#3) - assign sell_order to a variable (#4) # /apps/naive/lib/naive/trader.ex def handle_info( %TradeEvent{ seller_order_id: order_id # `quantity` check removed below (#1) }, %State{ symbol: symbol, (#2) sell_order: %Binance.OrderResponse{ order_id: order_id, transact_time: timestamp # `transact_time` to `get_order` (#3) } = sell_order # to update order (#4) } = state ) do Moving lower to the body of the function, we need to: - fetch current state of our sell order - update status of our sell_order from Trader‚Äôs state - branch out the logic based on status of the sell_order: * log and return the :stop atom to stop the GenServer or * update the state with new sell_order and continue Here‚Äôs the full body of our callback: # /apps/naive/lib/naive/trader.ex # inside the callabck {:ok, %Binance.Order{} = current_sell_order} = @binance_client.get_order( symbol, timestamp, order_id ) sell_order = %{sell_order | status: current_sell_order.status} if sell_order.status == &quot;FILLED&quot; do Logger.info(&quot;Trade finished, trader will now exit&quot;) {:stop, :normal, state} else Logger.info(&quot;Sell order partially filled&quot;) new_state = %{state | sell_order: sell_order} {:noreply, new_state} end 8.6 Test the implementation Testing this feature is a bit tricky as it requires trading on real Binance exachnge(as our BinanceMock always fills order with a single transaction) as well as race condition to happen :) Not that easy but even without race condition we should still test that code works as expected wiht BinanceMock: $ iex -S mix ... iex(1)&gt; Naive.start_trading(&quot;XRPUSDT&quot;) 23:27:35.977 [info] Starting new supervision tree to trade on XRPUSDT {:ok, #PID&lt;0.331.0&gt;} 23:27:39.073 [info] Initializing new trader for XRPUSDT iex(2)&gt; Streamer.start_streaming(&quot;XRPUSDT&quot;) {:ok, #PID&lt;0.345.0&gt;} 23:31:57.044 [info] Initializing new trader for XRPUSDT 23:31:57.888 [info] Placing BUY order for XRPUSDT @ 0.28031, quantity: 71.3 23:32:01.023 [info] Buy order filled, placing SELL order for XRPUSDT @ 0.28053, quantity: 71.30000000 23:33:08.865 [info] Trade finished, trader will now exit 23:33:08.865 [info] XRPUSDT Trader finished - restarting [Note] Please remember to run mix format to keep things nice and tidy. Source code for this chapter can be found at Github "],["run-multiple-traders-in-parallel.html", "Chapter 9 Run multiple traders in parallel 9.1 Objectives 9.2 Describe and design the required functionality 9.3 Implement rebuy inside Naive.Trader 9.4 Implement rebuy in the Naive.Leader 9.5 Improve logs by assigning ids to traders 9.6 Test the implementation", " Chapter 9 Run multiple traders in parallel 9.1 Objectives describe and design the required functionality implement rebuy in the Naive.Trader implement rebuy in the Naive.Leader improve logs by assigning ids to traders 9.2 Describe and design the required functionality At this moment, inside the Naive.Leader we have a silly code that starts all of the traders at the same moment: # /apps/naive/lib/naive/leader.ex ... traders = for _i &lt;- 1..settings.chunks, do: start_new_trader(trader_state) ... All the changes we made in this episode will enable us to fix. Let‚Äôs say that we placed a buy order that got filled and the price fallen before reaching the sell level. We can see here that we missed a nice opportunity to buy more as price drops and make money as it climbs back: Single trader We will implement additional trade event callback inside the Naive.Trader that will keep checking the price after buy order has been filled. Whenever price drops below the buy_order‚Äôs price by rebuy_interval we will notify the Naive.Leader to start the new Naive.Trader process: Multiple traders The Naive.Leader keeps track of how many Naive.Traders are running and needs to honor the number of chunks set up in the settings (one chunk == one trader). To stop the Naive.Traders from continuously notifying about a drop in the price we will also introduce a boolean flag that will track has the Naive.Leader been already notified. 9.3 Implement rebuy inside Naive.Trader We will start by adding the rebuy_interval and the rebuy_notified to trader‚Äôs state: # /apps/naive/lib/naive/trader.ex ... defmodule State do @enforce_keys [ :symbol, :budget, :buy_down_interval, :profit_interval, :rebuy_interval, # &lt;= add this field :rebuy_notified, # &lt;= add this field :tick_size, :step_size ] defstruct [ :symbol, :budget, :buy_order, :sell_order, :buy_down_interval, :profit_interval, :rebuy_interval, # &lt;= add this field :rebuy_notified, # &lt;= add this field :tick_size, :step_size ] end Rebuy logic should be placed almost as last callback just before the one that ignores all events. We will need to retrieve the current price and buy_price and confirm that we didn‚Äôt notify the leader yet(rebuy_notified flag): # /apps/naive/lib/naive/trader.ex ... # sell filled callback here ... def handle_info( %TradeEvent{ price: current_price }, %State{ symbol: symbol, buy_order: %Binance.OrderResponse{ price: buy_price }, rebuy_interval: rebuy_interval, rebuy_notified: false } = state ) do end # catch all callback here We need to calculate is the current price below the rebuy interval. If yes we will notify the leader and update the boolean flag. We will abstract calculation to separate function(for readability) that we will write below: # /apps/naive/lib/naive/trader.ex # body of the above callback if trigger_rebuy?(buy_price, current_price, rebuy_interval) do Logger.info(&quot;Rebuy triggered for #{symbol} trader&quot;) new_state = %{state | rebuy_notified: true} Naive.Leader.notify(:rebuy_triggered, new_state) {:noreply, new_state} else {:noreply, state} end As mentioned before, we will set the rebuy_notified boolean flag to true and notify the leader using the notify function with dedicated atom. At the bottom of the module we need to add trigger_rebuy? helper function: # /apps/naive/lib/naive/trader.ex # bottom of the module defp trigger_rebuy?(buy_price, current_price, rebuy_interval) do current_price = D.new(current_price) buy_price = D.new(buy_price) rebuy_price = D.sub( buy_price, D.mult(buy_price, rebuy_interval) ) D.lt?(current_price, rebuy_price) end 9.4 Implement rebuy in the Naive.Leader Moving on to the Naive.Leader module, we can get update starting of the traders automatically by the leader to starting just one inside handle_continue: # /apps/naive/lib/naive/leader.ex def handle_continue(:start_traders, %{symbol: symbol} = state) do ... traders = [start_new_trader(trader_state)] # &lt;= updated part ... end We will need to add a new clause of notify function that will handle the rebuy scenario: # /apps/naive/lib/naive/leader.ex # add below current `notify` function def notify(:rebuy_triggered, trader_state) do GenServer.call( :&quot;#{__MODULE__}-#{trader_state.symbol}&quot;, {:rebuy_triggered, trader_state} ) end We need to add a new handle_call function that will start new traders only when there are still chunks available(enforce the maximum number of parallel traders) - let‚Äôs start with a header: # /apps/naive/lib/naive/leader.ex # place this one after :update_trader_state handle_call def handle_call( {:rebuy_triggered, new_trader_state}, {trader_pid, _}, %{traders: traders, symbol: symbol, settings: settings} = state ) do end There are few important details to make note of: - we need trader‚Äôs pid to be able to find it details inside the list of traders - we need settings to cofirm number of chunks to be able to limit maximum number of parallel traders Moving on to the body of our callback. As with other ones, we will check can we find trader inside list of traders and based on that we will either start another one(if we didn‚Äôt reach the limit of chunks) or ignore it: # /apps/naive/lib/naive/leader.ex # body of our callback case Enum.find_index(traders, &amp;(&amp;1.pid == trader_pid)) do nil -&gt; Logger.warn(&quot;Rebuy triggered by trader that leader is not aware of&quot;) {:reply, :ok, state} index -&gt; traders = if settings.chunks == length(traders) do Logger.info(&quot;All traders already started for #{symbol}&quot;) traders else Logger.info(&quot;Starting new trader for #{symbol}&quot;) [start_new_trader(fresh_trader_state(settings)) | traders] end old_trader_data = Enum.at(traders, index) new_trader_data = %{old_trader_data | :state =&gt; new_trader_state} new_traders = List.replace_at(traders, index, new_trader_data) {:reply, :ok, %{state | :traders =&gt; new_traders}} end In above code we need to remember to update the state of the trader that triggered the rebuy inside the traders list as well as add state of a new trader to that list. As with other setting we will hardcode the rebuy_interval(inside the fetch_symbol_settings function) and assign them to trader‚Äôs state(inside the fresh_trader_state function): # /apps/naive/lib/naive/leader.ex defp fresh_trader_state(settings) do %{ struct(Trader.State, settings) | budget: D.div(settings.budget, settings.chunks), rebuy_notified: false # &lt;= add this line } end defp fetch_symbol_settings(symbol) do ... Map.merge( %{ ... chunks: 5, # &lt;= update to 5 parallel traders max budget: Decimal.new(&quot;100&quot;), # &lt;= update this line ... profit_interval: Decimal.new(&quot;-0.0012&quot;), rebuy_interval: Decimal.new(&quot;0.001&quot;) # &lt;= add this line }, symbol_filters ) end We also update the chunks and the budget to allow our strategy to start up to 5 parallel traders with a budget of 20 USDT each(100/5) as Binance has minimal order requirement at about $15(when using the BinanceMock this doesn‚Äôt really matter). 9.5 Improve logs by assigning ids to traders The final change will be to add an id to trader‚Äôs state so we can use it inside log messages to give us meaningful data about what‚Äôs going on(otherwise we won‚Äôt be able to tell which message was logged by which trader). First let‚Äôs add the id into the Naive.Leader‚Äôs fresh_trader_state as it will be defined per trader: # /apps/naive/lib/naive/leader.ex defp fresh_trader_state(settings) do %{ struct(Trader.State, settings) | id: :os.system_time(:millisecond), # &lt;= add this line budget: D.div(settings.budget, settings.chunks), rebuy_notified: false } end Now we can move on to the Naive.Trader and add it to the %State{} struct as well as we will modify every callback to include that id inside log messages: # /apps/naive/lib/naive/trader.ex defmodule State do @enforce_keys [ :id, ... ] defstruct [ :id, ... ] end ... def init(%State{id: id, symbol: symbol} = state) do ... Logger.info(&quot;Initializing new trader(#{id}) for #{symbol}&quot;) ... end def handle_info( %TradeEvent{price: price}, %State{ id: id, ... } = state ) do ... Logger.info( &quot;The trader(#{id}) is placing a BUY order &quot; &lt;&gt; &quot;for #{symbol} @ #{price}, quantity: #{quantity}&quot; ) ... end def handle_info( %TradeEvent{ buyer_order_id: order_id }, %State{ id: id, ... } = state ) do ... Logger.info( &quot;The trader(#{id}) is placing a SELL order for &quot; &lt;&gt; &quot;#{symbol} @ #{sell_price}, quantity: #{quantity}.&quot; ) ... Logger.info(&quot;Trader&#39;s(#{id} #{symbol} BUY order got partially filled&quot;) ... end def handle_info( %TradeEvent{ seller_order_id: order_id }, %State{ id: id, ... } = state ) do ... Logger.info(&quot;Trader(#{id}) finished trade cycle for #{symbol}&quot;) ... Logger.info(&quot;Trader&#39;s(#{id} #{symbol} SELL order got partially filled&quot;) ... end def handle_info( %TradeEvent{ price: current_price }, %State{ id: id, ... } = state ) do ... Logger.info(&quot;Rebuy triggered for #{symbol} by the trader(#{id})&quot;) ... end That finishes the implementation part - we should now be able to test the implementation and see dynamically growing number of traders for our strategy based on price movement. 9.6 Test the implementation Let‚Äôs start an iEx the session and open the :observer(inside go to ‚ÄúApplications‚Äù tab and click on naive from the list of the left) so we will be able to see how the number of traders is growing as well as PIDs are changing which means that they are finishing the full trades: $ iex -S mix ... iex(1)&gt; :observer.start() ... iex(2)&gt; Naive.start_trading(&quot;HNTUSDT&quot;) 10:22:05.018 [info] The trader(1616754009963) is placing a BUY order for HNTUSDT @ 8.175, quantity: 2.446 10:22:11.665 [info] Rebuy triggered for HNTUSDT by the trader(1616754009963) 10:22:11.665 [info] Starting new trader for HNTUSDT 10:22:11.665 [info] Initializing new trader(1616754131665) for HNTUSDT 10:22:11.665 [info] The trader(1616754009963) is placing a SELL order for HNTUSDT @ 8.181, quantity: 2.446. 10:22:11.665 [info] The trader(1616754131665) is placing a BUY order for HNTUSDT @ 8.157, quantity: 2.451 10:22:58.339 [info] Trader(1616754009963) finished trade cycle for HNTUSDT 10:22:58.339 [info] HNTUSDT trader finished trade - restarting 10:22:58.339 [info] Initializing new trader(1616754178339) for HNTUSDT 10:22:58.339 [info] The trader(1616754178339) is placing a BUY order for HNTUSDT @ 8.212, quantity: 2.435 10:23:13.232 [info] Rebuy triggered for HNTUSDT by the trader(1616754178339) 10:23:13.232 [info] Starting new trader for HNTUSDT 10:23:13.232 [info] Initializing new trader(1616754193232) for HNTUSDT 10:23:13.232 [info] The trader(1616754178339) is placing a SELL order for HNTUSDT @ 8.218, quantity: 2.435. 10:23:31.120 [info] The trader(1616754193232) is placing a BUY order for HNTUSDT @ 8.194, quantity: 2.44 10:23:31.121 [info] Trader(1616754178339) finished trade cycle for HNTUSDT 10:23:31.122 [info] HNTUSDT trader finished trade - restarting 10:23:31.122 [info] Initializing new trader(1616754211122) for HNTUSDT 10:24:31.891 [info] The trader(1616754211122) is placing a BUY order for HNTUSDT @ 8.198, quantity: 2.439 10:25:24.155 [info] The trader(1616754211122) is placing a SELL order for HNTUSDT @ 8.204, quantity: 2.439. 10:25:24.155 [info] The trader(1616754193232) is placing a SELL order for HNTUSDT @ 8.2, quantity: 2.44. 10:25:24.155 [info] Rebuy triggered for HNTUSDT by the trader(1616754211122) 10:25:24.155 [info] Starting new trader for HNTUSDT 10:25:24.156 [info] Initializing new trader(1616754324155) for HNTUSDT 10:25:24.156 [info] Rebuy triggered for HNTUSDT by the trader(1616754193232) 10:25:24.156 [info] The trader(1616754324155) is placing a BUY order for HNTUSDT @ 8.183, quantity: 2.444 10:25:24.156 [info] Starting new trader for HNTUSDT 10:25:24.156 [info] Initializing new trader(1616754324156) for HNTUSDT 10:25:24.156 [info] The trader(1616754324156) is placing a BUY order for HNTUSDT @ 8.176, quantity: 2.446 10:25:24.156 [info] The trader(1616754324155) is placing a SELL order for HNTUSDT @ 8.189, quantity: 2.444. 10:25:37.527 [info] Trader(1616754324155) finished trade cycle for HNTUSDT 10:25:37.528 [info] HNTUSDT trader finished trade - restarting 10:25:37.528 [info] Initializing new trader(1616754337528) for HNTUSDT 10:25:37.528 [info] The trader(1616754337528) is placing a BUY order for HNTUSDT @ 8.192, quantity: 2.441 10:25:37.530 [info] Trader(1616754211122) finished trade cycle for HNTUSDT 10:25:37.530 [info] Trader(1616754193232) finished trade cycle for HNTUSDT 10:25:37.530 [info] HNTUSDT trader finished trade - restarting 10:25:37.530 [info] Initializing new trader(1616754337530) for HNTUSDT 10:25:37.530 [info] HNTUSDT trader finished trade - restarting 10:25:37.530 [info] Initializing new trader(1616754337530) for HNTUSDT 10:25:40.015 [info] Rebuy triggered for HNTUSDT by the trader(1616754337528) 10:25:40.015 [info] The trader(1616754337530) is placing a BUY order for HNTUSDT @ 8.179, quantity: 2.445 10:25:40.015 [info] All traders already started for HNTUSDT And our observer shows the following: Observer shows 5 parallel traders We can clearly see that our strategy dynamically scaled from 1 to 5 parallel traders and they were going through different trading steps without any problems - I think that‚Äôs really cool to see and it wasn‚Äôt difficult to achieve in Elixir. [Note] Please remember to run mix format to keep things nice and tidy. Source code for this chapter can be found at Github "],["fine-tune-trading-strategy-per-symbol.html", "Chapter 10 Fine-tune trading strategy per symbol 10.1 Objectives 10.2 Describe and design the required functionality 10.3 Add docker to project 10.4 Set up ecto inside the naive app 10.5 Create and migrate the DB 10.6 Seed symbols‚Äô settings 10.7 Update the Naive.Leader to fetch settings", " Chapter 10 Fine-tune trading strategy per symbol 10.1 Objectives describe and design the required functionality add docker to project set up ecto inside the naive app create and migrate the DB seed symbols‚Äô settings update the Naive.Leader to fetch settings 10.2 Describe and design the required functionality At this moment the settings of our naive strategy are hardcoded inside the Naive.Leader: # /apps/naive/lib/naive/leader.ex ... defp fetch_symbol_settings(symbol) do symbol_filters = fetch_symbol_filters(symbol) Map.merge( %{ symbol: symbol, # &lt;= chunks: 5, # &lt;= budget: Decimal.new(&quot;100&quot;), # &lt;= buy_down_interval: Decimal.new(&quot;0.0001&quot;), # &lt;= all of those settings # -0.12% for quick testing # &lt;= profit_interval: Decimal.new(&quot;-0.0012&quot;), # &lt;= rebuy_interval: Decimal.new(&quot;0.001&quot;) # &lt;= }, symbol_filters ) end ... The problem about those is that they are hardcoded and there‚Äôs no flexibility to define them per symbol at the moment. In this chapter, we will move them out from this file into the Postgres database. 10.3 Add docker to project The requirements for this section are docker and docker-compose installed in your system. Inside the main directory of our project create a new file called docker-compose.yml and fill it with below details: # /docker-compose.yml version: &quot;3.2&quot; services: db: image: postgres:latest restart: always environment: POSTGRES_PASSWORD: &quot;postgres&quot; ports: - 5432:5432 volumes: - ../postgres-data:/var/lib/postgresql/data If you are new to docker here‚Äôs the gist of what the above will do: - it will start a single service called ‚Äúdb‚Äù - ‚Äúdb‚Äù service will use the latest version of the postgres (image) inside the docker container (latest version as tagged per https://hub.docker.com/_/postgres/) - we map TCP port 5432 in the container to port 5432 on the Docker host(format container_port:hosts_port) - we set up environmental variable inside the docker container that will be used by the Postgres app as a password for the default (postgres) user - volumes option maps the directory from inside of the container to the host. This way we will keep the state of the database between restarts. We can now start the service using docker-compose: $ docker-compose up -d Creating hedgehog_db_1 ... done To validate that it works we can run: $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 98558827b80b postgres:latest &quot;docker-entrypoint.s√¢‚Ç¨¬¶&quot; 4 seconds ago Up 4 seconds 0.0.0.0:5432-&gt;5432/tcp hedgehog_db_1 10.4 Set up ecto inside the naive app Let‚Äôs start by adding database access to the naive application. The first step is to add the Ecto module together with the Postgrex ecto‚Äôs driver package to the deps function inside the mix.exs file. As we are going to use Enums inside Postgres, we need to add the EctoEnum module as well: # /apps/naive/mix.exs defp deps do [ {:binance, &quot;~&gt; 0.7.1&quot;}, {:binance_mock, in_umbrella: true}, {:decimal, &quot;~&gt; 2.0&quot;}, {:ecto_sql, &quot;~&gt; 3.0&quot;}, # &lt;= New line {:ecto_enum, &quot;~&gt; 1.4&quot;}, # &lt;= New line {:phoenix_pubsub, &quot;~&gt; 2.0&quot;}, {:postgrex, &quot;&gt;= 0.0.0&quot;}, # &lt;= New line {:streamer, in_umbrella: true} ] end Remember about installing those deps using: $ mix deps.get We can now use the ecto generator to add an the ecto repository to the Naive application: $ cd apps/naive $ mix ecto.gen.repo -r Naive.Repo * creating lib/naive * creating lib/naive/repo.ex * updating ../../config/config.exs Don&#39;t forget to add your new repo to your supervision tree (typically in lib/naive/application.ex): {Naive.Repo, []} And to add it to the list of Ecto repositories in your configuration files (so Ecto tasks work as expected): config :naive, ecto_repos: [Naive.Repo] Back to the IDE, the generator updated our config/config.exs file with the default access details to the database, we need to modify them to point to our Postgres docker instance as well as add a list of ecto repositories for our naive app (as per instruction above): # /config/config.exs config :naive, # &lt;= added line ecto_repos: [Naive.Repo], # &lt;= added line binance_client: BinanceMock # &lt;= merged from existing config config :naive, Naive.Repo, database: &quot;naive&quot;, # &lt;= updated username: &quot;postgres&quot;, # &lt;= updated password: &quot;postgres&quot;, # &lt;= updated hostname: &quot;localhost&quot; Here we can use localhost as inside the docker-compose.yml file we defined port forwarding from the container to the host(Postgres is available at localhost:5432). We also merged the existing binance_client setting together with the new ecto_repos setting. The last step to be able to communicate with the database using Ecto will be to add the Naive.Repo module(created by generator) to the children list of the Naive.Application: # /apps/naive/lib/naive/application.ex ... def start(_type, _args) do children = [ {Naive.Repo, []}, # &lt;= added line { DynamicSupervisor, strategy: :one_for_one, name: Naive.DynamicSymbolSupervisor } ] ... 10.5 Create and migrate the DB We can now create a new naive database using the mix tool, after that we will be able to generate a migration file that will create the settings table: $ mix ecto.create -r Naive.Repo The database for Naive.Repo has been created $ cd apps/naive $ mix ecto.gen.migration create_settings * creating priv/repo/migrations * creating priv/repo/migrations/20210202223209_create_settings.exs We can now copy the current hardcoded settings from the Naive.Leader module and use them as a column list of our new settings table. All of the below alterations needs to be done inside the change function of our migration file: # /apps/naive/priv/repo/migrations/20210202223209_create_settings.exs ... def change do create table(:settings) do add(:symbol, :text, null: false) add(:chunks, :integer, null: false) add(:budget, :decimal, null: false) add(:buy_down_interval, :decimal, null: false) add(:profit_interval, :decimal, null: false) add(:rebuy_interval, :decimal, null: false) end end At this moment we just copied the settings and converted them to columns using the add function. We need now to take care of the id column. We need to pass primary_key: false to the create table macro to stop it from creating the default integer-based id column. Instead of that we will define the id column ourselves with :uuid type and pass a flag that will indicate that it‚Äôs the primary key of the settings table: # /apps/naive/priv/repo/migrations/20210202223209_create_settings.exs ... create table(:settings, primary_key: false) do add(:id, :uuid, primary_key: true) ... We will also add create and update timestamps that come as a bundle when using the timestamps() function inside the create table macro: # /apps/naive/priv/repo/migrations/20210202223209_create_settings.exs ... create table(...) do ... timestamps() # &lt;= both create and update timestamps end ... We will add a unique index on the symbol column to avoid any possible duplicates: # /apps/naive/priv/repo/migrations/20210202223209_create_settings.exs ... create table(...) do ... end create(unique_index(:settings, [:symbol])) end ... We will now add the status field which will be an Enum. It will be defined inside a separate file and alias‚Äôed from our migration, this way we will be able to use it from within the migration and the inside the lib code. First, we will apply changes to our migration and then we will move on to creating the Enum module. Here‚Äôs the full implementation of migration for reference: # /apps/naive/priv/repo/migrations/20210202223209_create_settings.exs defmodule Naive.Repo.Migrations.CreateSettings do use Ecto.Migration alias Naive.Schema.TradingStatusEnum def change do TradingStatusEnum.create_type() create table(:settings, primary_key: false) do add(:id, :uuid, primary_key: true) add(:symbol, :text, null: false) add(:chunks, :integer, null: false) add(:budget, :decimal, null: false) add(:buy_down_interval, :decimal, null: false) add(:profit_interval, :decimal, null: false) add(:rebuy_interval, :decimal, null: false) add(:status, TradingStatusEnum.type(), default: &quot;off&quot;, null: false) timestamps() end create(unique_index(:settings, [:symbol])) end end That finishes our work on the migration file. We will now focus on TradingStatusEnum implementation. First, we need to create a schema directory inside the apps/naive/lib/naive directory and file called trading_status_enum.ex and place below logic (defining the enum) in it: # /apps/naive/lib/naive/schema/trading_status_enum.ex import EctoEnum defenum(Naive.Schema.TradingStatusEnum, :trading_status, [:on, :off]) We used the defenum macro from the ecto_enum module to define our enum. It‚Äôs interesting to point out that we didn‚Äôt need to define a new module as defenum macro takes care of that for us. Let‚Äôs run the migration to create the table, unique index, and the enum: $ mix ecto.migrate 00:51:16.757 [info] == Running 20210202223209 Naive.Repo.Migrations.CreateSettings.change/0 forward 00:51:16.759 [info] execute &quot;CREATE TYPE public.trading_status AS ENUM (&#39;on&#39;, &#39;off&#39;)&quot; 00:51:16.760 [info] create table settings 00:51:16.820 [info] create index settings_symbol_index 00:51:16.829 [info] == Migrated 20210202223209 in 0.0s We can now create a schema file for the settings table so inside the /apps/naive/lib/naive/schema create a file called settings.ex. We will start with a skeleton implementation of schema file together with the copied list of columns from the migration and convert to ecto‚Äôs types using it‚Äôs docs: # /apps/naive/lib/naive/schema/settings.ex defmodule Naive.Schema.Settings do use Ecto.Schema alias Naive.Schema.TradingStatusEnum @primary_key {:id, :binary_id, autogenerate: true} schema &quot;settings&quot; do field(:symbol, :string) field(:chunks, :integer) field(:budget, :decimal) field(:buy_down_interval, :decimal) field(:profit_interval, :decimal) field(:rebuy_interval, :decimal) field(:status, TradingStatusEnum) timestamps() end end 10.6 Seed symbols‚Äô settings So we have all the pieces of implementation to be able to create DB, migrate the settings table, and query it using Ecto. To be able to drop the hardcoded settings from the Naive.Leader we will need to fill our database with the ‚Äúdefault‚Äù setting for each symbol. To achieve that we will define default settings inside the config/config.exs file and we will create a seed script that will fetch all pairs from Binance and insert a new config row inside DB for each one. Let‚Äôs start by adding those default values to the config file(we will merge them into the structure defining binance_client and ecto_repos): # config/config.exs config :naive, ecto_repos: [Naive.Repo], binance_client: BinanceMock, trading: %{ defaults: %{ chunks: 5, budget: 1000.0, buy_down_interval: 0.0001, profit_interval: -0.0012, rebuy_interval: 0.001 } } Moving on to the seeding script, we need to create a new file called seed_settings.exs inside the /apps/naive/lib/naive/priv/ directory. Let‚Äôs start by aliasing the required modules and requiring the Logger: # /apps/naive/priv/seed_settings.exs require Logger alias Decimal alias Naive.Repo alias Naive.Schema.Settings Next, we will get the Binance client from the config: # /apps/naive/priv/seed_settings.exs ... binance_client = Application.get_env(:naive, :binance_client) Now, it‚Äôs time to fetch all the symbols(pairs) that Binance supports: # /apps/naive/priv/seed_settings.exs ... Logger.info(&quot;Fetching exchange info from Binance to create trading settings&quot;) {:ok, %{symbols: symbols}} = binance_client.get_exchange_info() Now we need to fetch default trading settings from the config file as well as the current timestamp: # /apps/naive/priv/seed_settings.exs ... %{ chunks: chunks, budget: budget, buy_down_interval: buy_down_interval, profit_interval: profit_interval, rebuy_interval: rebuy_interval } = Application.get_env(:naive, :trading).defaults timestamp = NaiveDateTime.utc_now() |&gt; NaiveDateTime.truncate(:second) We will use the default settings for all rows to be able to insert data into the database. Normally we wouldn‚Äôt need to set inserted_at and updated_at fields as Ecto would generate those values for us when using Repo.insert/2 but we won‚Äôt be able to use this functionality as it takes a single record at the time. We will be using Repo.insert_all/3 which is a bit more low-level function without those nice features like filling timestamps(sadly). Just to be crystal clear - Repo.insert/2 takes at least a couple of seconds(on my machine) for 1000+ symbols currently supported by Binance, on the other hand Repo.insert_all/3, will insert all of them in a couple of hundred milliseconds. As our structs will differ by only the symbol column we can first create a full struct that will serve as a template: # /apps/naive/priv/seed_settings.exs ... base_settings = %{ symbol: &quot;&quot;, chunks: chunks, budget: Decimal.from_float(budget), buy_down_interval: Decimal.from_float(buy_down_interval), profit_interval: Decimal.from_float(profit_interval), rebuy_interval: Decimal.from_float(rebuy_interval), status: &quot;off&quot;, inserted_at: timestamp, updated_at: timestamp } We will now map each of the retrieved symbols and inject them to the base_settings structs and pushing all of those to the Repo.insert_all/3 function: # /apps/naive/priv/seed_settings.exs ... Logger.info(&quot;Inserting default settings for symbols&quot;) maps = symbols |&gt; Enum.map(&amp;(%{base_settings | symbol: &amp;1[&quot;symbol&quot;]})) {count, nil} = Repo.insert_all(Settings, maps) Logger.info(&quot;Inserted settings for #{count} symbols&quot;) 10.7 Update the Naive.Leader to fetch settings The final step will be to update the Naive.Leader to fetch the settings from the database. At the top of the module add the following: # /apps/naive/lib/naive/leader.ex ... alias Naive.Repo alias Naive.Schema.Settings ... Now we need to modify the fetch_symbol_settings/1 to fetch settings from DB instead of the hardcoded map. We will use Repo.get_by!/3 as we are unable to trade without settings. The second trick used here is Map.from_struct/1 that is required here as otherwise result would become the Naive.Schema.Settings struct(this would cause problems further down the line as we are iterating on the returned map and would get the protocol Enumerable not implemented for %Naive.Schema.Settings error): # /apps/naive/lib/naive/leader.ex ... defp fetch_symbol_settings(symbol) do symbol_filters = fetch_symbol_filters(symbol) settings = Repo.get_by!(Settings, symbol: symbol) Map.merge( symbol_filters, settings |&gt; Map.from_struct() ) end ... We can now run the seeding script to fill our database with the default settings: $ cd apps/naive $ mix run priv/seed_settings.exs 18:52:29.341 [info] Fetching exchange info from Binance to create trading settings 18:52:31.571 [info] Inserting default settings for symbols 18:52:31.645 [info] Inserted settings for 1276 symbols We can verify that records were indeed inserted into the database by connecting to it using the psql application: $ psql -Upostgres -hlocalhost Password for user postgres: # &lt;= use &#39;postgres&#39; password here ... postgres=# \\c naive You are now connected to database &quot;naive&quot; as user &quot;postgres&quot;. naive=# \\x Expanded display is on. naive=# SELECT * FROM settings; -[ RECORD 1 ]-----+------------------------------------- id | 159c8f32-d571-47b2-b9d7-38bb42868043 symbol | ETHUSDT chunks | 5 budget | 1000 buy_down_interval | 0.0001 profit_interval | -0.0012 rebuy_interval | 0.001 status | off inserted_at | 2021-02-02 18:52:31 updated_at | 2021-02-02 18:52:31 # press arrows to scroll, otherwise press `q` naive=# SELECT COUNT(*) FROM settings; -[ RECORD 1 ] count | 1276 naive=# \\q # &lt;= to close the `psql` That confirms that there are 1276 settings inside the database that will allow us to continue trading which we can check by running our app inside iex(from the main project‚Äôs directory): $ iex -S mix ... iex(1)&gt; Naive.start_trading(&quot;NEOUSDT&quot;) 19:20:02.936 [info] Starting new supervision tree to trade on NEOUSDT {:ok, #PID&lt;0.378.0&gt;} 19:20:04.584 [info] Initializing new trader(1612293637000) for NEOUSDT The above log messages confirm that the Naive.Leader was able to fetch settings from the database that were later put into the Naive.Trader‚Äôs state and passed to it. [Note] Please remember to run mix format to keep things nice and tidy. Source code for this chapter can be found at Github "],["supervise-and-autostart-streaming.html", "Chapter 11 Supervise and autostart streaming 11.1 Objectives 11.2 Describe and design the required functionality 11.3 Register the Streamer.Binance processes with names 11.4 Set up ecto inside the streamer app 11.5 Create and migrate the db 11.6 Seed default settings 11.7 Implement the supervision tree and start streaming functionality 11.8 Implement the stop functionality 11.9 Implement the autostart streaming functionality 11.10 Test the implementation", " Chapter 11 Supervise and autostart streaming 11.1 Objectives describe and design the required functionality register the Streamer.Binance processes with names set up ecto inside the streamer app create and migrate the db seed default settings implement the supervision tree and start streaming functionality implement the stop functionality implement the autostart streaming functionality test the implementation 11.2 Describe and design the required functionality At this moment there‚Äôs no supervision around the streamer processes, whenever an error would occur inside the Streamer.Binance process, it will die and never come back up. That‚Äôs less than perfect, but we can use supervisors to the rescue. We will create a new Streamer.DynamicStreamerSupervisor module inside our streamer application that will supervise the Streamer.Binance processes. Next, we will consider a list of functionalities that we would like it to provide: - start streaming. This will require a new Streamer.Binance process started under the Streamer.DynamicStreamerSupervisor. We will put logic responsible for starting that process inside the Streamer.DynamicStreamerSupervisor module. - stop streaming. To be able to stop the Streamer.Binance process streaming on a specific symbol, we will need to know that process‚Äô PID based only on symbol string(ie. ‚ÄúETHUSDT‚Äù). To make that possible, we will need to register every Streamer.Binance process with a name that we will be able to ‚Äúreverse-engineer‚Äù based only on symbol string for example: :\"#{__MODULE__}-#{symbol}\" - autostart streaming. At the start of streaming on a symbol, we should persist that action as a symbol‚Äôs streaming setting inside the database. We will need to generate a new Ecto.Repo, configure, create and migrate DB (just like in the last chapter for the naive app) to be able to retrieve that list. We will write a logic that will fetch settings of the symbols, autostart the ones that are enabled and place all that logic inside the Streamer.DynamicStreamerSupervisor module. We will introduce a Task child process that will utilize the logic from the Streamer.DynamicStreamerSupervisor to fetch those enabled symbols and start Streamer.Binance processes on startup - we will describe all of this separately in its section in this chapter. 11.3 Register the Streamer.Binance processes with names To be able to perform start/stop streaming on a symbol we will first need to be able to figure out the PID of the Streamer.Binance process that is streaming that symbol. The first change that we need to apply will be to register Streamer.Binance processes with names by passing the 4th argument to the WebSockex.start_link/4 function: # /apps/streamer/lib/streamer/binance.ex def start_link(symbol) do lowercased_symbol = String.downcase(symbol) # &lt;= separate variable WebSockex.start_link( &quot;#{@stream_endpoint}#{lowercased_symbol}@trade&quot;, # &lt;= lowercase symbol __MODULE__, nil, name: :&quot;#{__MODULE__}-#{symbol}&quot; # &lt;= uppercase symbol ) end Few things worth mention here: - we are getting the uppercase symbol but inside the URL we need to use a lowercase symbol so we will introduce a new separate variable to be used in the URL - we are registering the process using the uppercase symbol so the name will remain consistent with the naive application‚Äôs processes - to register process we are sending keyword list as the 4th argument to custom start_link/4 function of WebSockex module (link to source - again, no need to be afraid of reading the source code in Elixir, that‚Äôs the beauty of it) 11.4 Set up ecto inside the streamer app In the same fashion as in the last chapter, we will need to set up the database inside the streamer app. We will use the same Postgres server(docker container) that we‚Äôve set up inside docker in the last chapter, just a separate database, so there‚Äôs no need to update the docker-compose.yml file. As previously the first step will be to add the ecto modules and Postgres related packages into deps inside the mix.exs file of the streamer app. Additionally, we will add the binance module that we will use to fetch all symbols supported by the exchange(to generate default settings as we‚Äôve done for the naive application. We are unable to use the BinanceMock as this will cause the circular dependency [Binance Mock depends on the streamer app]): # /apps/streamer/mix.exs ... defp deps do [ {:binance, &quot;~&gt; 0.7.1&quot;}, # &lt;= used to retrieve symbols list(exchangeInfo) {:ecto_sql, &quot;~&gt; 3.0&quot;}, # &lt;= added dependency {:ecto_enum, &quot;~&gt; 1.4&quot;}, # &lt;= added dependency {:jason, &quot;~&gt; 1.2&quot;}, {:phoenix_pubsub, &quot;~&gt; 2.0&quot;}, {:postgrex, &quot;&gt;= 0.0.0&quot;}, # &lt;= added dependency {:websockex, &quot;~&gt; 0.4.2&quot;} ] end Run mix deps.get to install new dependencies. We can now use ecto generator to add an ecto repository to the Streamer application: $ cd apps/streamer $ mix ecto.gen.repo -r Streamer.Repo * creating lib/streamer * creating lib/streamer/repo.ex * updating ../../config/config.exs ... Update the config to match access details to Postgres‚Äô docker instance: # /config/config.exs config :streamer, # &lt;= added line ecto_repos: [Streamer.Repo] # &lt;= added line config :streamer, Streamer.Repo, database: &quot;streamer&quot;, # &lt;= database updated username: &quot;postgres&quot;, # &lt;= username updated password: &quot;postgres&quot;, # &lt;= password updated hostname: &quot;localhost&quot; The last step will be to update the children list of the Streamer.Application module: # /apps/streamer/lib/streamer/application.ex ... def start(_type, _args) do children = [ {Streamer.Repo, []}, # &lt;= repo added { Phoenix.PubSub, name: Streamer.PubSub, adapter_name: Phoenix.PubSub.PG2 } ] ... 11.5 Create and migrate the db We can now create a new streamer database using the mix tool, after that we will be able to generate a migration file that will create the settings table: $ mix ecto.create -r Streamer.Repo The database for Streamer.Repo has been created $ cd apps/streamer $ mix ecto.gen.migration create_settings * creating priv/repo/migrations * creating priv/repo/migrations/20210203184805_create_settings.exs We can safely start just with id, symbol and status columns, where the last one will follow the same enum idea from the previous chapter: # /apps/streamer/priv/repo/migrations/20210203184805_create_settings.exs defmodule Streamer.Repo.Migrations.CreateSettings do use Ecto.Migration alias Streamer.Schema.StreamingStatusEnum def change do StreamingStatusEnum.create_type() create table(:settings, primary_key: false) do add(:id, :uuid, primary_key: true) add(:symbol, :text, null: false) add(:status, StreamingStatusEnum.type(), default: &quot;off&quot;, null: false) timestamps() end create(unique_index(:settings, [:symbol])) end end That finishes our work on the migration file, we need to add the StreamingStatusEnum in the same way as in the last chapter (create a schema directory inside the apps/streamer/lib/streamer directory and anew file called streaming_status_enum.ex and place below logic (defining the enum) in it: # /apps/streamer/lib/streamer/schema/streaming_status_enum.ex import EctoEnum defenum(Streamer.Schema.StreamingStatusEnum, :streaming_status, [:on, :off]) Let‚Äôs run the migration to create the table, unique index, and the enum: $ mix ecto.migrate 21:31:56.850 [info] == Running 20210203184805 Streamer.Repo.Migrations.CreateSettings.change/0 forward 21:31:56.850 [info] execute &quot;CREATE TYPE public.streaming_status AS ENUM (&#39;on&#39;, &#39;off&#39;)&quot; 21:31:56.851 [info] create table settings 21:31:56.912 [info] create index settings_symbol_index 21:31:56.932 [info] == Migrated 20210203184805 in 0.0s We can now create a schema file for the settings table. Inside the /apps/streamer/lib/streamer/schema directory create a file called settings.ex: # /apps/streamer/lib/streamer/schema/settings.ex defmodule Streamer.Schema.Settings do use Ecto.Schema alias Streamer.Schema.StreamingStatusEnum @primary_key {:id, :binary_id, autogenerate: true} schema &quot;settings&quot; do field(:symbol, :string) field(:status, StreamingStatusEnum) timestamps() end end We are now ready to query the table but first, we need to insert the default settings into the database. 11.6 Seed default settings As with the settings inside the naive application, we will fetch all symbols from binance and bulk insert them into the database. First let‚Äôs create a new file called seed_settings.exs inside the apps/streamer/priv directory. As this file is nearly the same as the one from the last chapter I will skip the lengthy explanation - this is the script: # /apps/streamer/priv/seed_settings.exs require Logger alias Decimal alias Streamer.Repo alias Streamer.Schema.Settings Logger.info(&quot;Fetching exchange info from Binance to create streaming settings&quot;) {:ok, %{symbols: symbols}} = Binance.get_exchange_info() timestamp = NaiveDateTime.utc_now() |&gt; NaiveDateTime.truncate(:second) base_settings = %{ symbol: &quot;&quot;, status: &quot;off&quot;, inserted_at: timestamp, updated_at: timestamp } Logger.info(&quot;Inserting default settings for symbols&quot;) maps = symbols |&gt; Enum.map(&amp;(%{base_settings | symbol: &amp;1[&quot;symbol&quot;]})) {count, nil} = Repo.insert_all(Settings, maps) Logger.info(&quot;Inserted settings for #{count} symbols&quot;) Don‚Äôt forget to run the seeding script before progressing forward: $ cd apps/streamer $ mix run priv/seed_settings.exs 22:03:46.675 [info] Fetching exchange info from Binance to create streaming settings 22:03:51.386 [info] Inserting default settings for symbols 22:03:51.448 [info] Inserted settings for 1277 symbols 11.7 Implement the supervision tree and start streaming functionality Let‚Äôs start by creating a new file called dynamic_streamer_supervisor.ex inside the /apps/streamer/lib/streamer directory. Let‚Äôs start with a default implementation from the docs (updated with correct module and process names): # /apps/streamer/lib/streamer/dynamic_streamer_supervisor.ex defmodule Streamer.DynamicStreamerSupervisor do use DynamicSupervisor def start_link(init_arg) do DynamicSupervisor.start_link(__MODULE__, init_arg, name: __MODULE__) end def init(_init_arg) do DynamicSupervisor.init(strategy: :one_for_one) end end Next, we will add the start_streaming/1 function at the bottom of the Streamer.DynamicStreamerSupervisor module: # /apps/streamer/lib/streamer/dynamic_streamer_supervisor.ex ... def start_streaming(symbol) when is_binary(symbol) do symbol = String.upcase(symbol) case get_pid(symbol) do nil -&gt; Logger.info(&quot;Starting streaming on #{symbol}&quot;) {:ok, _settings} = update_streaming_status(symbol, &quot;on&quot;) {:ok, _pid} = start_streamer(symbol) pid -&gt; Logger.warn(&quot;Streaming on #{symbol} already started&quot;) {:ok, _settings} = update_streaming_status(symbol, &quot;on&quot;) {:ok, pid} end end To unpack above - we are checking is there a streamer process already running for the passed symbol and based on the result of that check, we either start the new streaming process(and update the symbol‚Äôs settings) or just update the symbol‚Äôs settings. Inside the start_streaming/1 function, we are using 3 helper functions that we need to add at the bottom of the file: # /apps/streamer/lib/streamer/dynamic_streamer_supervisor.ex defp get_pid(symbol) do Process.whereis(:&quot;Elixir.Streamer.Binance-#{symbol}&quot;) end defp update_streaming_status(symbol, status) when is_binary(symbol) and is_binary(status) do Repo.get_by(Settings, symbol: symbol) |&gt; Ecto.Changeset.change(%{status: status}) |&gt; Repo.update() end defp start_streamer(symbol) do DynamicSupervisor.start_child( Streamer.DynamicStreamerSupervisor, {Streamer.Binance, symbol} ) end The above functions are quite self-explanatory, get_pid/1 is a convenience wrapper, update_streaming_status/2 will update the status field for the passed symbol, start_streamer/1 will instruct the Streamer.DynamicStreamerSupervisor to start a new Streamer.Binance process with symbol passed as the first argument. The last step to get the above function to work(and future ones in this module) would be to add an require, an import and a few aliases at the top of the module: # /apps/streamer/lib/streamer/dynamic_streamer_supervisor.ex require Logger import Ecto.Query, only: [from: 2] alias Streamer.Repo alias Streamer.Schema.Settings As we added a new start_streaming/1 logic inside the Streamer.DynamicStreamerSupervisor, we need to replace the start_streaming/1 implementation inside the Streamer module: # /apps/streamer/lib/streamer.ex ... alias Streamer.DynamicStreamerSupervisor defdelegate start_streaming(symbol), to: DynamicStreamerSupervisor As we don‚Äôt need to put any logic inside the Streamer.start_streaming/1 function, we can just delegate the call straight to the Streamer.DynamicStreamerSupervisor module. The last step will be to append the Streamer.DynamicStreamSupervisor to the children list of the Streamer.Application: # /apps/streamer/lib/streamer/application.ex def start(_type, _args) do children = [ ... {Streamer.DynamicStreamerSupervisor, []} ] At this moment our supervision tree already works and all streamer processes are being monitored by the Streamer.DynamicStreamerSupervisor: DynamicStreamerSupervisor with two named streamers 11.8 Implement the stop functionality As we can see, we are now registering the Streamer.Binance processes with names that contain their symbols. We will be able to retrieve PIDs of those registered processes just by simply passing the symbol string(ie. ‚ÄúETHUSDT‚Äù) into get_pid/1, which will allow us to then request termination of those processes by the Streamer.DynamicStreamerSupervisor. Let‚Äôs write a stop_streaming/1 logic inside the Streamer.DynamicStreamerSupervisor module(put it above first private function): # /apps/streamer/lib/streamer/dynamic_streamer_supervisor.ex def stop_streaming(symbol) when is_binary(symbol) do symbol = String.upcase(symbol) case get_pid(symbol) do nil -&gt; Logger.warn(&quot;Streaming on #{symbol} already stopped&quot;) {:ok, _settings} = update_streaming_status(symbol, &quot;off&quot;) pid -&gt; Logger.info(&quot;Stopping streaming on #{symbol}&quot;) :ok = DynamicSupervisor.terminate_child( Streamer.DynamicStreamerSupervisor, pid ) {:ok, _settings} = update_streaming_status(symbol, &quot;off&quot;) end end stop_streaming/1 looks very similar to start_streaming/1, we are checking is there already a Streamer.Binance process registered for that symbol, and we either ask the Streamer.DynamicStreamerSupervisor to terminate it for us (using the DynamicSupervisor.terminate_child/2 function + update the status) or just update the status to be off. We need to update the Streamer module to provide the interface to stop streaming on a symbol: # /apps/streamer/lib/streamer.ex ... defdelegate stop_streaming(symbol), to: DynamicStreamerSupervisor ... 11.9 Implement the autostart streaming functionality Currently, whenever we will shutdown the elixir app, settings persist in the database but streamers are not started on the next init. To fix this, we will add autostart_streaming/0 inside the Streamer.DynamicStreamerSupervisor. Note: It very important to differentiate between module and process. We will put our autostarting logic inside the module but the Streamer.DynamicStreamerSupervisor process won‚Äôt run it. We will introduce a new Task process that will execute all the autostarting logic. That will cover the problem of the Supervisor executing too much business logic (as the Task will execute it), but how will we supervise them together? At init both will start, the Streamer.DynamicStreamerSupervisor first and then Task will ask it to start multiple children and that will work fine. The problem occurs when the Streamer.DynamicStreamerSupervisor would die because of any reason. Currently, it‚Äôs supervised using the one_for_one strategy(and the Task would be as well) which means that it will get started again by the Streamer.Application process but at that moment the ‚Äúautostarting‚Äù Task won‚Äôt be there anymore to start streaming on all enabled symbols. We can clearly see that whenever the Streamer.DynamicStreamerSupervisor will die it needs to be started again together with the ‚Äúautostart‚Äù Task and this won‚Äôt fit our current Streamer.Application‚Äôs strategy. In cases like those, a new level of supervision needs to be introduced that will have a different supervision strategy for those ‚Äúcoupled‚Äù processes. We will rename the process name of the Streamer.Application (which is currently registered as Streamer.Supervisor) to Streamer.Application. Then we will introduce the new Streamer.Supervisor module and register it under the same name. We will attach both Streamer.DynamicStreamerSupervisor and Task to the Streamer.Supervisor and assign it with the rest_for_one strategy which will restart the Task whenever Streamer.DynamicStreamerSupervisor would die: Updated supervision tree with additional superviser and renamed Application process Let‚Äôs start by creating the autostart_streaming/0 functionality inside the Streamer.DynamicStreamerSupervisor: # /apps/streamer/lib/streamer/dynamic_streamer_supervisor.ex # add below after the `init/1` function def autostart_streaming() do fetch_symbols_to_stream() |&gt; Enum.map(&amp;start_streaming/1) end # and this at the end of the module defp fetch_symbols_to_stream() do Repo.all( from(s in Settings, where: s.status == &quot;on&quot;, select: s.symbol ) ) end autostart_streaming/0 function fetches all symbols from the settings table with status == \"on\" then it passes them one by one into the start_streaming/1 function using Enum.map/2. We can now focus on referring to the above autostarting logic inside the new supervisor that we will create now. Let‚Äôs start by creating a new file called supervisor.ex inside the /apps/streamer/lib/streamer/ directory and fill it with default Supervisor implementation: # /apps/streamer/lib/streamer/supervisor.ex defmodule Streamer.Supervisor do # &lt;= updated module name use Supervisor def start_link(init_arg) do Supervisor.start_link(__MODULE__, init_arg, name: __MODULE__) end def init(_init_arg) do children = [ ] Supervisor.init(children, strategy: :one_for_one) end end We can now update the strategy to rest_for_one: # /apps/streamer/lib/streamer/supervisor.ex def init(_init_arg) do ... Supervisor.init(children, strategy: :rest_for_one) # &lt;= strategy updated end The last step inside our new supervisor will be to add 2 children: Streamer.DynamicStreamerSupervisor and Task (that will autostart the symbol streamers): # /apps/streamer/lib/streamer/supervisor.ex def init(_init_arg) do children = [ {Streamer.DynamicStreamerSupervisor, []}, {Task, fn -&gt; Streamer.DynamicStreamerSupervisor.autostart_streaming() end} ] ... end Final update in this chapter will be to replace the Streamer.DynamicStreamerSupervisor as one of the children inside the Streamer.Application module and update the name that application process registers under: # /apps/streamer/lib/streamer/application.ex ... children = [ {Streamer.Repo, []}, { Phoenix.PubSub, name: Streamer.PubSub, adapter_name: Phoenix.PubSub.PG2 }, {Streamer.Supervisor, []} # &lt;= updated supervisor ] opts = [strategy: :one_for_one, name: Streamer.Application] # &lt;= updated name ... 11.10 Test the implementation Let‚Äôs start an iex session and call the start_streaming/1 function twice for two different symbols and then exit using double Ctrl+c: $ iex -S mix ... iex(1)&gt; Streamer.start_streaming(&quot;ethusdt&quot;) 18:51:39.809 [info] Starting streaming on ETHUSDT {:ok, #PID&lt;0.370.0&gt;} iex(2)&gt; Streamer.start_streaming(&quot;neousdt&quot;) 18:51:47.288 [info] Starting streaming on NEOUSDT {:ok, #PID&lt;0.377.0&gt;} Now, open a new iex session and look at the output: $ iex -S mix ... iex(1)&gt; 18:53:48.920 [info] Starting streaming on ETHUSDT 18:53:50.306 [info] Starting streaming on NEOUSDT We can also confirm that streamer processes are there by using :observer.start(): Updated supervision tree with two autostarted streamers and new supervisor Inside the same iex session run the following: iex(5)&gt; Streamer.stop_streaming(&quot;neousdt&quot;) 18:57:37.205 [info] Stopping streaming on NEOUSDT {:ok, %Streamer.Schema.Settings{ ... }} iex(6)&gt; Streamer.stop_streaming(&quot;ethusdt&quot;) 18:57:51.553 [info] Stopping streaming on ETHUSDT {:ok, %Streamer.Schema.Settings{ ... }} Stop the iex session and start new one - streamers shouldn‚Äôt be autostarted any more. [Note] Please remember to run mix format to keep things nice and tidy. Source code for this chapter can be found at Github "],["start-stop-shutdown-and-autostart-trading.html", "Chapter 12 Start, stop, shutdown and autostart trading 12.1 Objectives 12.2 Describe and design the required functionality 12.3 (Re-)Implement the start trading functionality 12.4 Implement the stop trading functionality 12.5 Implement the autostart trading functionality 12.6 Implement the shutdown trading functionality", " Chapter 12 Start, stop, shutdown and autostart trading 12.1 Objectives describe and design the required functionality (re-)implement the start trading functionality implement the stop trading functionality implement the autostart trading functionality implement the shutdown trading functionality test the implementation 12.2 Describe and design the required functionality In the 10th chapter, we‚Äôve introduced the Postgres database inside the naive application together with the settings per symbol. In this chapter, we will progress forward to provide additional trading functionality that will be similar to the functionality implemented in the last chapter for the streaming application: - stop trading - as the Naive.SymbolSupervisor processes are registered with names that can be easily reverse engineered, we should be able to utilize the Process.where_is/1 function to retrieve the PIDs and instruct the Naive.DynamicSymbolSupervisor to terminate those child processes. Again, we need to put that logic somewhere so we will implement the Naive.DynamicSymbolSupervisor as a full module using the DynamicSupervisor behavior. - start_trading - as our Naive.DynamicSymbolSupervisor will now be a module we will be able to remove the start_trading/1 implementation from the Naive module and reimplement it inside the Naive.DynamicSymbolSupervisor module. It will follow the same pattern of checking for PID, starting the Naive.SymbolSupervisor process and flipping the status flag inside the settings table‚Äôs row for that symbol. - shutdown trading - sometimes abruptly stopping trading won‚Äôt be the best solution, it would be better to allow the Naive.Trader processes to finish their ongoing trades. To be able to do that we will need to inform the Naive.Leader process assigned to the symbol that the settings for that symbol have been updated and that should cause the Naive.Leader process to withhold starting new Naive.Trader processes and terminate the whole tree when the last trader will finish. - autostart trading - this will be a very similar implementation to the one from the last chapter. It will require introducing a new supervisor(we will follow the same naming convention: rename Naive.Application‚Äôs registered process name to Naive.Application, create a new supervisor called Naive.Supervisor) and utilize the Task process to execute the autostarting logic. Final supervision tree after adding the autostarting Task and the Naive.Supervisor 12.3 (Re-)Implement the start trading functionality To (re-)implement the start_trading/1 functionality we will need to create a new file called dynamic_symbol_supervisor.ex inside the /apps/naive/lib/naive directory that will use the DynamicSupervisor behaviour. Previously we have been using default DynamicSupervisor implementation(one of the children of the Naive.Application - to be substituted with the below module): # /apps/naive/lib/naive/dynamic_symbol_supervisor.ex defmodule Naive.DynamicSymbolSupervisor do # &lt;= module updated use DynamicSupervisor require Logger # &lt;= Logger added import Ecto.Query, only: [from: 2] # &lt;= added for querying alias Naive.Repo # &lt;= added for querying/updating alias Naive.Schema.Settings # &lt;= added for querying/updating def start_link(init_arg) do DynamicSupervisor.start_link(__MODULE__, init_arg, name: __MODULE__) end def init(_init_arg) do DynamicSupervisor.init(strategy: :one_for_one) end end The above code is a default implementation from the DynamicSupervisor docs with some additional imports, require and aliases as we will use them in this chapter. Our start_trading/1 implementation is almost the same as one for the streamer application from the last chapter: # /apps/naive/lib/naive/dynamic_symbol_supervisor.ex ... def start_trading(symbol) when is_binary(symbol) do symbol = String.upcase(symbol) case get_pid(symbol) do nil -&gt; Logger.info(&quot;Starting trading of #{symbol}&quot;) {:ok, _settings} = update_trading_status(symbol, &quot;on&quot;) {:ok, _pid} = start_symbol_supervisor(symbol) pid -&gt; Logger.warn(&quot;Trading on #{symbol} already started&quot;) {:ok, _settings} = update_trading_status(symbol, &quot;on&quot;) {:ok, pid} end end ... together with additional helper functions: # /apps/naive/lib/naive/dynamic_symbol_supervisor.ex defp get_pid(symbol) do Process.whereis(:&quot;Elixir.Naive.SymbolSupervisor-#{symbol}&quot;) end defp update_trading_status(symbol, status) when is_binary(symbol) and is_binary(status) do Repo.get_by(Settings, symbol: symbol) |&gt; Ecto.Changeset.change(%{status: status}) |&gt; Repo.update() end defp start_symbol_supervisor(symbol) do DynamicSupervisor.start_child( Naive.DynamicSymbolSupervisor, {Naive.SymbolSupervisor, symbol} ) end Both implementation and helper functions are almost the same as the ones inside the naive application. It could be tempting to abstract some of the logic away but remember that we should treat all applications in our umbrella project as standalone services that should not share any code if possible(we broke that rule for the TradeEvent struct from the streamer app but we could easily just make a lib with that struct that would be shared between two applications). I would shy away from sharing any business logic between applications in the umbrella project. There are two additional places where we need to make updates to get our start_trading/1 to work again: - we need to update the children list inside the Naive.Application: # /apps/naive/lib/naive/application.ex ... children = [ {Naive.Repo, []}, {Naive.DynamicSymbolSupervisor, []} # &lt;= replacement of DynamicSupervisor ] we need to replace the start_trading/1 implementation inside the Naive module to defdelegate macro(as we don‚Äôt have any logic to run there): # /apps/naive/lib/naive.ex ... alias Naive.DynamicSymbolSupervisor defdelegate start_trading(symbol), to: DynamicSymbolSupervisor ... At this moment we are again able to use the Naive.start_trading/1 function to start trading on a symbol (behind the scenes it will use logic from the new Naive.DynamicSymbolSupervisor module). 12.4 Implement the stop trading functionality Stop trading will require a change in two places, first inside the Naive.DynamicSymbolSupervisor where we will place the termination logic: # /apps/naive/lib/naive/dynamic_symbol_supervisor.ex ... def stop_trading(symbol) when is_binary(symbol) do symbol = String.upcase(symbol) case get_pid(symbol) do nil -&gt; Logger.warn(&quot;Trading on #{symbol} already stopped&quot;) {:ok, _settings} = update_trading_status(symbol, &quot;off&quot;) pid -&gt; Logger.info(&quot;Stopping trading of #{symbol}&quot;) :ok = DynamicSupervisor.terminate_child( Naive.DynamicSymbolSupervisor, pid ) {:ok, _settings} = update_trading_status(symbol, &quot;off&quot;) end end ... The second change we need to make is to create a forwarding interface using defdelegate inside the Naive module: # /apps/naive/lib/naive.ex ... defdelegate stop_trading(symbol), to: DynamicSymbolSupervisor ... That pretty much finishes the stop_trading/1 functionality. We are now able to start and stop(what was previously not available) trading on a symbol. 12.5 Implement the autostart trading functionality To implement the autostarting we will need to (in a similar fashion as in the last chapter) add a new supervision level that will be dedicated to supervising the Naive.DynamicSymbolSupervisor and the ‚Äúautostarting‚Äù Task. Let‚Äôs create a new file called supervisor.ex inside the /apps/naive/lib/naive directory and (as in the last chapter) we will add the Naive.DynamicSymbolSupervisor and the Task to its children list. We will also update the supervision strategy to :rest_for_one: # /apps/naive/lib/naive/supervisor.ex defmodule Naive.Supervisor do use Supervisor def start_link(init_arg) do Supervisor.start_link(__MODULE__, init_arg, name: __MODULE__) end def init(_init_arg) do children = [ {Naive.DynamicSymbolSupervisor, []}, # &lt;= added {Task, # &lt;= added fn -&gt; # &lt;= added Naive.DynamicSymbolSupervisor.autostart_trading() # &lt;= added end} # &lt;= added ] Supervisor.init(children, strategy: :rest_for_one) # &lt;= strategy updated end end Now we need to swap the Naive.DynamicSymbolSupervisor to Naive.Supervisor in the children list of the Naive.Application, as well as update the registered process‚Äô name of the Naive.Application: # /apps/naive/lib/naive/application.ex ... def start(_type, _args) do children = [ {Naive.Repo, []}, {Naive.Supervisor, []} # &lt;= replacement for DynamicSymbolSupervisor ] opts = [strategy: :one_for_one, name: Naive.Application] # &lt;= name updated Finally, we need to implement autostart_trading/0 inside the Naive.DynamicSymbolSupervisor module as our new Task refers to it: # /apps/naive/lib/naive/dynamic_symbol_supervisor.ex ... # add the below function after the `init/1` function def autostart_trading() do fetch_symbols_to_trade() |&gt; Enum.map(&amp;start_trading/1) end ... # and this helper at the end of the module defp fetch_symbols_to_trade() do Repo.all( from(s in Settings, where: s.status == &quot;on&quot;, select: s.symbol ) ) end ... Those are the same (excluding updated function names) as inside the streamer application. We are fetching enabled symbols and starting new Naive.SymbolSupervisor processes for each one. At this moment we can already see our implementation in action: Trading on two symbols autostarted, new Naive.Supervisor and renamed Naive.Application At this moment we are able to test the current implementation inside the iex: $ iex -S mix ... iex(1)&gt; Naive.start_trading(&quot;ethusdt&quot;) 21:35:30.207 [info] Starting trading of ETHUSDT 21:35:30.261 [info] Starting new supervision tree to trade on ETHUSDT {:ok, #PID&lt;0.372.0&gt;} 21:35:33.344 [info] Initializing new trader(1612647333342) for ETHUSDT iex(3)&gt; Naive.start_trading(&quot;neousdt&quot;) 21:35:54.128 [info] Starting trading of NEOUSDT 21:35:54.169 [info] Starting new supervision tree to trade on NEOUSDT {:ok, #PID&lt;0.383.0&gt;} 21:35:56.007 [info] Initializing new trader(1612647356007) for NEOUSDT 21:38:07.699 [info] Stopping trading of NEOUSDT {:ok, %Naive.Schema.Settings{ ... }} We can now exit the iex and start a new one: $ iex -S mix ... 21:39:16.894 [info] Starting trading of ETHUSDT 21:39:16.938 [info] Starting new supervision tree to trade on ETHUSDT 21:39:18.786 [info] Initializing new trader(1612647558784) for ETHUSDT iex(1)&gt; The above logs confirm that the naive application autostarts the previously enabled symbols(using the start_trading/1 function) as well as stop_trading/1 updates the status inside the database (so the symbol isn‚Äôt autostarted at the next initialization). 12.6 Implement the shutdown trading functionality Last but not least, we will move on to the shutdown_trading/1 functionality. Let‚Äôs start with the simplest part which is delegating the function call to the Naive.DynamicSymbolSupervisor module from the Naive module(interface): # /apps/naive/lib/naive.ex ... defdelegate shutdown_trading(symbol), to: DynamicSymbolSupervisor ... Next, we will create a shutdown_trading/1 function inside the Naive.DynamicSymbolSupervisor module where we will check is there any trading going on for that symbol(the same as for start/stop), and in case of trading happening we will inform the Naive.Leader process handling that symbol that settings have been updated: # /apps/naive/lib/naive/dynamic_symbol_supervisor.ex ... def shutdown_trading(symbol) when is_binary(symbol) do symbol = String.upcase(symbol) case get_pid(symbol) do nil -&gt; Logger.warn(&quot;Trading on #{symbol} already stopped&quot;) {:ok, _settings} = update_trading_status(symbol, &quot;off&quot;) _pid -&gt; Logger.info(&quot;Shutdown of trading on #{symbol} initialized&quot;) {:ok, settings} = update_trading_status(symbol, &quot;shutdown&quot;) Naive.Leader.notify(:settings_updated, settings) {:ok, settings} end end ... The crucial part of the implementation above is the notify(:settings_updated, settings) where we inform the Naive.Leader process that it needs to update trading settings. Currently, the Naive.Leader module does not support updating the settings after startup - let‚Äôs add a new interface function together with a callback function that will handle settings updating: # /apps/naive/lib/naive/leader.ex ... # add the below function as the last clause of the `notify` function def notify(:settings_updated, settings) do GenServer.call( :&quot;#{__MODULE__}-#{settings.symbol}&quot;, {:update_settings, settings} ) end # add the below handler function as the last clause of `handle_call` function def handle_call( {:update_settings, new_settings}, _, state ) do {:reply, :ok, %{state | settings: new_settings}} end Ok, we have a way to update the settings of the Naive.Leader process ‚Äúon the go‚Äù but what effects should the shutdown state have on the Naive.Leader‚Äôs actions? There are two places that require modification: - whenever the Naive.Trader process will finish the trade cycle, Naive.Leader process should not start a new one, as well as check, was that the last trader process and if that was the case it needs to call the Naive.stop_trading/1 function with its symbol to terminate whole supervision tree for that symbol - whenever the Naive.Leader process will receive a rebuy notification, it should just ignore it when the symbol is in the shutdown state. Let‚Äôs look at the updated implementation of the ‚Äúend of trade‚Äù handler: # /apps/naive/lib/naive/leader.ex ... def handle_info( {:DOWN, _ref, :process, trader_pid, :normal}, %{traders: traders, symbol: symbol, settings: settings} = state ) do Logger.info(&quot;#{symbol} trader finished trade - restarting&quot;) case Enum.find_index(traders, &amp;(&amp;1.pid == trader_pid)) do nil -&gt; Logger.warn( &quot;Tried to restart finished #{symbol} &quot; &lt;&gt; &quot;trader that leader is not aware of&quot; ) if settings.status == &quot;shutdown&quot; and traders == [] do # &lt;= additional check Naive.stop_trading(state.symbol) end {:noreply, state} index -&gt; new_traders = if settings.status == &quot;shutdown&quot; do # &lt;= refactored code Logger.warn( &quot;The leader won&#39;t start a new trader on #{symbol} &quot; &lt;&gt; &quot;as symbol is in the &#39;shutdown&#39; state&quot; ) if length(traders) == 1 do Naive.stop_trading(state.symbol) end List.delete_at(traders, index) else new_trader_data = start_new_trader(fresh_trader_state(settings)) List.replace_at(traders, index, new_trader_data) end {:noreply, %{state | traders: new_traders}} end end ... As visible in the above code, whenever the Naive.Trader process will finish the trade cycle, the Naive.Leader process will check can it find a record of that trader in its state (no changes here). We will modify the callback so the leader process will check the settings.status. In the shutdown status it checks wheater it was the last trader in the traders list, to terminate the whole tree at that time(using the Naive.stop_trading/1 function). The second callback that we need to modify is the rebuy triggered: # /apps/naive/lib/naive/leader.ex ... def handle_call( {:rebuy_triggered, new_trader_state}, {trader_pid, _}, %{traders: traders, symbol: symbol, settings: settings} = state ) do case Enum.find_index(traders, &amp;(&amp;1.pid == trader_pid)) do nil -&gt; Logger.warn(&quot;Rebuy triggered by trader that leader is not aware of&quot;) {:reply, :ok, state} index -&gt; traders = if settings.chunks == length(traders) do Logger.info(&quot;All traders already started for #{symbol}&quot;) traders else if settings.status == &quot;shutdown&quot; do # &lt;= refactored code Logger.warn( &quot;The leader won&#39;t start a new trader on #{symbol} &quot; &lt;&gt; &quot;as symbol is in shutdown state(rebuy triggered)&quot; ) traders else Logger.info(&quot;Starting new trader for #{symbol}&quot;) [start_new_trader(fresh_trader_state(settings)) | traders] end end old_trader_data = Enum.at(traders, index) new_trader_data = %{old_trader_data | :state =&gt; new_trader_state} new_traders = List.replace_at(traders, index, new_trader_data) {:reply, :ok, %{state | :traders =&gt; new_traders}} end end ... In the above rebuy_triggered handler function we added branching on the settings.status and we simply ignore the rebuy notification when the symbol is in the shutdown status. The final change will be to create a new migration that will update the TradingStatusEnum to have shutdown option: $ cd apps/naive $ mix ecto.gen.migration update_trading_status * creating priv/repo/migrations/20210205232303_update_trading_status.exs Inside the generated migration file we need to excute a raw SQL command: # /apps/naive/priv/repo/migrations/20210205232303_update_trading_status.exs defmodule Naive.Repo.Migrations.UpdateTradingStatus do use Ecto.Migration @disable_ddl_transaction true def change do Ecto.Migration.execute &quot;ALTER TYPE trading_status ADD VALUE IF NOT EXISTS &#39;shutdown&#39;&quot; end end We need to apply the same change to the Naive.Schema.TradingStatusEnum: # /apps/naive/lib/naive/schema/trading_status_enum.ex import EctoEnum defenum(Naive.Schema.TradingStatusEnum, :trading_status, [:on, :off, :shutdown]) Don‚Äôt forget to run mix ecto.migrate to run the new migration. We can now test the shutdown_trading/1 functionality inside the iex: $ iex -S mix ... iex(1)&gt; Streamer.start_streaming(&quot;ethusdt&quot;) 21:46:26.651 [info] Starting streaming on ETHUSDT {:ok, #PID&lt;0.372.0&gt;} iex(2)&gt; Naive.start_trading(&quot;ethusdt&quot;) 21:46:42.830 [info] Starting trading of ETHUSDT 21:46:42.867 [info] Starting new supervision tree to trade on ETHUSDT {:ok, #PID&lt;0.379.0&gt;} 21:46:44.816 [info] Initializing new trader(1612648004814) for ETHUSDT ... 21:47:52.448 [info] Rebuy triggered for ETHUSDT by the trader(1612648004814) ... 21:49:58.900 [info] Rebuy triggered for ETHUSDT by the trader(1612648089409) ... 21:50:58.927 [info] Rebuy triggered for ETHUSDT by the trader(1612648198900) ... 21:53:27.202 [info] Rebuy triggered for ETHUSDT by the trader(1612648326215) 21:53:27.250 [info] Rebuy triggered for ETHUSDT by the trader(1612648325512) 21:53:27.250 [info] All traders already started for ETHUSDT # at this moment we have 5 `Naive.Trader` processes trading in parallel iex(4)&gt; Naive.shutdown_trading(&quot;ethusdt&quot;) 21:55:01.556 [info] Shutdown of trading on ETHUSDT initialized {:ok, %Naive.Schema.Settings{ ... }} ... 22:06:58.855 [info] Trader(1612648407202) finished trade cycle for ETHUSDT 22:06:58.855 [info] ETHUSDT trader finished trade - restarting 22:06:58.855 [warn] The leader won&#39;t start a new trader on ETHUSDTas symbol is in shutdown state 22:07:50.768 [info] Trader(1612648325512) finished trade cycle for ETHUSDT 22:07:50.768 [info] ETHUSDT trader finished trade - restarting 22:07:50.768 [warn] The leader won&#39;t start a new trader on ETHUSDTas symbol is in shutdown state 22:07:50.857 [info] Trader(1612648326215) finished trade cycle for ETHUSDT 22:07:50.857 [info] ETHUSDT trader finished trade - restarting 22:07:50.857 [warn] The leader won&#39;t start a new trader on ETHUSDTas symbol is in shutdown state 22:07:51.079 [info] Trader(1612648089409) finished trade cycle for ETHUSDT 22:07:51.079 [info] ETHUSDT trader finished trade - restarting 22:07:51.079 [warn] The leader won&#39;t start a new trader on ETHUSDTas symbol is in shutdown state 22:08:05.401 [info] Trader(1612648004814) finished trade cycle for ETHUSDT 22:08:05.401 [info] ETHUSDT trader finished trade - restarting 22:08:05.401 [warn] The leader won&#39;t start a new trader on ETHUSDTas symbol is in shutdown state 22:08:05.401 [info] Stopping trading of ETHUSDT As we can see from the logs above, our naive strategy grown from 1 to 5 Naive.Trader processes running in parallel, then we called the shutdown_trading/1 function. In the shutdown status, the Naive.Leader process ignored rebuy notifications and wasn‚Äôt starting any new Naive.Trader processes as the old ones were finishing. At the moment when the last Naive.Trader process finished the trade cycle, the Naive.Leader called stop_trading/1 on ‚Äúit‚Äôs‚Äù symbol, terminating the whole supervision tree for that symbol. [Note] Please remember to run mix format to keep things nice and tidy. Source code for this chapter can be found at Github "],["abstract-duplicated-supervision-code.html", "Chapter 13 Abstract duplicated supervision code 13.1 Objectives 13.2 Overview of requirements 13.3 Pseudo generalize Core.ServiceSupervisor module 13.4 Utilize pseudo generalized code inside the Naive.DynamicSymbolSupervisor 13.5 Implement a truly generic Core.ServiceSupervisor 13.6 Remove boilerplate using use macro 13.7 Use the Core.ServiceSupervisor module inside the streamer application", " Chapter 13 Abstract duplicated supervision code 13.1 Objectives overview of requirements pseudo generalize Core.ServiceSupervisor module utilize pseudo generalized code inside the Naive.DynamicSymbolSupervisor implement a truly generic Core.ServiceSupervisor use the Core.ServiceSupervisor module inside the streamer application 13.2 Overview of requirements In the last few chapters, we went through adding and modifying the dynamic supervision tree around the naive and streamer applications‚Äô workers. Initially, we just copied the implementation from the streamer application to the naive application (with a few minor tweaks like log messages). That wasn‚Äôt the most sophisticated solution and we will address this copy-paste pattern in this chapter. We will write an ‚Äúextension‚Äù of the DynamicSupervisor that allows to start, stop and autostart workers. Just to keep things simple we will create a new application inside our umbrella project where we will place the logic. This will save us from creating a new repo for time being. Our new Core.ServiceSupervisor module will hold all the logic responsible for starting, stopping, and autostarting worker processes. To limit the boilerplate inside the implementation modules (like Naive.DynamicSymbolSupervisor or Streamer.DynamicStreamerSupervisor) we will utilize the use macro that will dynamically generate low-level wiring for us. 13.3 Pseudo generalize Core.ServiceSupervisor module Let‚Äôs start by creating a new non supervised application called core inside our umbrella project. At this moment our ‚Äúabstraction code‚Äù will sit inside it just to keep things simple as otherwise, we would need to create a new repo and jump between codebases which we will avoid for time being: $ cd apps $ mix new core * creating README.md * creating .formatter.exs * creating .gitignore * creating mix.exs * creating lib * creating lib/core.ex * creating test * creating test/test_helper.exs * creating test/core_test.exs ... We can now create a new directory called core inside the apps/core/lib directory and a new file called service_supervisor.ex inside it where we will put all abstracted starting/stopping/autostarting logic. Let‚Äôs start with an empty module: # /apps/core/lib/core/service_supervisor.ex defmodule Core.ServiceSupervisor do end The first step in our refactoring process will be to move(cut) all of the functions from the Naive.DynamicSymbolSupervisor (excluding the start_link/1, init/1 and shutdown_trading/1) and put them inside the Core.ServiceSupervisor module which should look as follows: # /apps/core/lib/core/service_supervisor.ex defmodule Core.ServiceSupervisor do def autostart_trading() do ... end def start_trading(symbol) when is_binary(symbol) do ... end def stop_trading(symbol) when is_binary(symbol) do ... end defp get_pid(symbol) do ... end defp update_trading_status(symbol, status) when is_binary(symbol) and is_binary(status) do ... end defp start_symbol_supervisor(symbol) do ... end defp fetch_symbols_to_trade() do ... end end All of the above code is trading related - we need to rename functions/logs to be more generic. Starting with autostart_trading/0 we can rename it to autostart_workers/0: # /apps/core/lib/core/service_supervisor.ex ... def autostart_workers() do # &lt;= updated function name fetch_symbols_to_start() # &lt;= updated function name |&gt; Enum.map(&amp;start_worker/1) # &lt;= updated function name end ... As we updated two functions inside the autostart_workers/0 we need to update their implementations. The start_trading/1 will become start_worker/1, internally we will inline the start_symbol_supervisor/1 function(move it‚Äôs contents inside the start_worker/1 function and remove the start_symbol_supervisor/1 function) as it‚Äôs used just once inside this module as well as update_trading_status/2 need to be renamed to update_status/2. The fetch_symbols_to_trade/0 will get updated to fetch_symbols_to_start/0: # /apps/core/lib/core/service_supervisor.ex def start_worker(symbol) when is_binary(symbol) do # &lt;= updated name symbol = String.upcase(symbol) case get_pid(symbol) do nil -&gt; Logger.info(&quot;Starting trading of #{symbol}&quot;) {:ok, _settings} = update_status(symbol, &quot;on&quot;) # &lt;= updated name {:ok, _pid} = DynamicSupervisor.start_child( Naive.DynamicSymbolSupervisor, {Naive.SymbolSupervisor, symbol} ) # ^^^^^^ inlined `start_symbol_supervisor/1` pid -&gt; Logger.warn(&quot;Trading on #{symbol} already started&quot;) {:ok, _settings} = update_status(symbol, &quot;on&quot;) # &lt;= updated name {:ok, pid} end end ... defp fetch_symbols_to_start() do # &lt;= updated name ... end Inside the above code we updated the update_trading_status/2 call to update_status/2 so we need to update the function header to match: # /apps/core/lib/core/service_supervisor.ex defp update_status(symbol, status) # &lt;= updated name when is_binary(symbol) and is_binary(status) do ... end Last function to rename in this module will be the stop_trading/1 to stop_worker/1, we also need to update calls to update_trading_status/2 to update_status/2 as it was renamed: # /apps/core/lib/core/service_supervisor.ex def stop_worker(symbol) when is_binary(symbol) do # &lt;= updated name symbol = String.upcase(symbol) case get_pid(symbol) do nil -&gt; Logger.warn(&quot;Trading on #{symbol} already stopped&quot;) {:ok, _settings} = update_status(symbol, &quot;off&quot;) # &lt;= updated name pid -&gt; Logger.info(&quot;Stopping trading of #{symbol}&quot;) :ok = DynamicSupervisor.terminate_child( Naive.DynamicSymbolSupervisor, pid ) {:ok, _settings} = update_status(symbol, &quot;off&quot;) # &lt;= updated name end end At this moment we have a pseudo-generic implementation of start_worker/1 and stop_worker/1 inside the Core.ServiceSupervisor module. Function names are generic but they still refer to Repo, Settings, and other modules specific to the naive app‚Äôs implementation. We are probably in a worse situation than we have been before starting this refactoring ;) but don‚Äôt fear this was just the first step on the way to abstract away that starting, stopping and autostarting code. 13.4 Utilize pseudo generalized code inside the Naive.DynamicSymbolSupervisor Before we will jump back to the naive‚Äôs application modules we need to add the core application the dependencies of the naive application: # /apps/naive/mix.exs defp deps do [ {:binance, &quot;~&gt; 0.7.1&quot;}, {:binance_mock, in_umbrella: true}, {:core, in_umbrella: true}, # &lt;= core dep added .... Let‚Äôs get back to the Naive.DynamicSymbolSupervisor where we expect functions that we just cut out to exist like start_trading/1 or stop_trading/1. Let‚Äôs reimplement more generic versions of those functions as just simple calls to the Core.ServiceSupervisor module: # /apps/naive/lib/naive/dynamic_symbol_supervisor.ex ... def autostart_workers() do Core.ServiceSupervisor.autostart_workers() end def start_worker(symbol) do Core.ServiceSupervisor.start_worker(symbol) end def stop_worker(symbol) do Core.ServiceSupervisor.stop_worker(symbol) end We also need to update the shutdown_trading/1 function as we removed all the private functions that it relies on: # /apps/naive/lib/naive/dynamic_symbol_supervisor.ex def shutdown_worker(symbol) when is_binary(symbol) do # &lt;= updated name symbol = String.upcase(symbol) case Core.ServiceSupervisor.get_pid(symbol) do # &lt;= module added nil -&gt; Logger.warn(&quot;Trading on #{symbol} already stopped&quot;) {:ok, _settings} = Core.ServiceSupervisor.update_status(symbol, &quot;off&quot;) # &lt;= updated name + module _pid -&gt; Logger.info(&quot;Shutdown of trading on #{symbol} initialized&quot;) {:ok, settings} = Core.ServiceSupervisor.update_status(symbol, &quot;shutdown&quot;) # &lt;= updated name + module Naive.Leader.notify(:settings_updated, settings) {:ok, settings} end end As we were moving all the private helper functions we didn‚Äôt make them public so the Naive.DynamicSymbolSupervisor module can use them - we will fix that now together with temporary aliases/require/imports at the top of the Core.ServiceSupervisor: # /apps/core/lib/core/service_supervisor.ex defmodule Core.ServiceSupervisor do require Logger # &lt;= added require import Ecto.Query, only: [from: 2] # &lt;= added import alias Naive.Repo # &lt;= added alias alias Naive.Schema.Settings # &lt;= added alias ... def get_pid(symbol) do # &lt;= updated from private to public ... end def update_status(symbol, status) # &lt;= updated from private to public when is_binary(symbol) and is_binary(status) do ... end As fetch_symbols_to_start/0 is only used internally by the Core.ServiceSupervisor module itself, we don‚Äôt need to make it public. We can also remove the aliases and import from the Naive.DynamicSymbolSupervisor as it won‚Äôt need them anymore. Next step will be to add ecto to the deps of the core application as it will make db queries now: # /apps/core/mix.exs defp deps do [ {:ecto_sql, &quot;~&gt; 3.0&quot;} ] end As we modified the interface of the Naive.DynamicSymbolSupervisor (for example renamed start_trading/1 to start_worker/1 and others) we need to modify the Naive.Supervisor‚Äôs children list - more specifically the Task process: # /apps/naive/lib/naive/supervisor.ex ... {Task, fn -&gt; Naive.DynamicSymbolSupervisor.autostart_workers() # &lt;= func name updated end} ... Last step will be to update interface of the naive application: # /apps/naive/lib/naive.ex alias Naive.DynamicSymbolSupervisor defdelegate start_trading(symbol), to: DynamicSymbolSupervisor, as: :start_worker defdelegate stop_trading(symbol), to: DynamicSymbolSupervisor, as: :stop_worker defdelegate shutdown_trading(symbol), to: DynamicSymbolSupervisor, as: :shutdown_worker Believe it or not, but at this moment(ignoring all of the warnings because we created a circular dependency between the core and the naive applications - which we will fix in the next steps) our application runs just fine! We are able to start and stop trading, autostarting works as well. 13.5 Implement a truly generic Core.ServiceSupervisor Ok. Why did we even do this? What we are aiming for is a separation between the interface of our Naive.DynamicSymbolSupervisor module (like start_worker/1, autostart_workers/0 and stop_worker/1) and the implementation which is now placed inside the Core.ServiceSupervisor module. That‚Äôs all nice and to-some-extent understandable but Core.ServiceSupervisor module is not a generic module. We can‚Äôt use it inside the streaming application to supervise the Streamer.Binance processes. So, what‚Äôs the point? Well, we can make it even more generic! 13.5.1 First path starting with the fetch_symbols_to_start/0 function Moving on to full generalization of the Core.ServiceSupervisor module. We will start with the helper functions first as they are the ones doing the work and they need to be truly generalized first: # /apps/core/lib/core/service_supervisor.ex def fetch_symbols_to_start() do Repo.all( from(s in Settings, where: s.status == &quot;on&quot;, select: s.symbol ) ) end The fetch_symbols_to_start/0 function uses Repo and Settings that are aliased at the top of the Core.ServiceSupervisor module. This just won‚Äôt work with any other applications as Streamer will require its own Repo and Settings modules etc. To fix that we will pass both repo and schema as arguments to the fetch_symbols_to_start/0 function which will become fetch_symbols_to_start/2: # /apps/core/lib/core/service_supervisor.ex def fetch_symbols_to_start(repo, schema) do # &lt;= args added repo.all( # &lt;= lowercase `repo` is an argument not aliased module from(s in schema, # &lt;= settings schema module passed as arg where: s.status == &quot;on&quot;, select: s.symbol ) ) end This will have a knock-on effect on any functions that are using fetch_symbols_to_start/0 - now they need to use fetch_symbols_to_start/2 and pass appropriate Repo and Schema modules. So, the fetch_symbols_to_start/0 is referenced by the autostart_workers/0 - we will need to modify it to pass the repo and schema to the fetch_symbols_to_start/2 and as it‚Äôs inside the Core.ServiceSupervisor module it needs to get them passed as arguments: # /apps/core/lib/core/service_supervisor.ex def autostart_workers(repo, schema) do # &lt;= args added fetch_symbols_to_start(repo, schema) # &lt;= args passed |&gt; Enum.map(&amp;start_worker/1) end Going even further down the line, autostart_workers/0 is referenced by the autostart_workers/0 inside the Naive.DynamicSymbolSupervisor module. As this module is (naive) application-specific, it is a place where repo and schema are known from the context - for the naive application repo is the Naive.Repo module and schema is the Naive.Schema.Settings module: # /apps/naive/lib/naive/dynamic_symbol_supervisor.ex ... def autostart_workers() do Core.ServiceSupervisor.autostart_workers( Naive.Repo, # &lt;= new value passed Naive.Schema.Settings # &lt;= new value passed ) end This finishes the first of multiple paths that we need to follow to fully refactor the Core.ServiceSupervisor module. 13.5.2 Second path starting with the update_status/2 Let‚Äôs don‚Äôt waste time and start from the other helper function inside the Core.ServiceSupervisor module. This time we will make the update_status/2 function fully generic: # /apps/core/lib/core/service_supervisor.ex def update_status(symbol, status, repo, schema) # &lt;= args added when is_binary(symbol) and is_binary(status) do repo.get_by(schema, symbol: symbol) # &lt;= using dynamic repo and shcema modules |&gt; Ecto.Changeset.change(%{status: status}) |&gt; repo.update() # &lt;= using dynamic repo module end As previously we added repo and schema as arguments and modified the body of the function to utilize them instead of hardcoded modules (aliased at the top of the Core.ServiceSupervisor module). In the same fashion as previously, we need to check ‚Äúwho‚Äù is using the update_status/2 and update those calls to update_status/4. The function is used inside the start_worker/1 and the stop_worker/1 inside the Core.ServiceSupervisor module so as previously we need to bubble them up(pass via arguments to both start_worker/1 and stop_worker/1 functions): # /apps/core/lib/core/service_supervisor.ex def start_worker(symbol, repo, schema) when is_binary(symbol) do # &lt;= new args ... {:ok, _settings} = update_status(symbol, &quot;on&quot;, repo, schema) # &lt;= args passed ... {:ok, _settings} = update_status(symbol, &quot;on&quot;, repo, schema) # &lt;= args passed ... end def stop_worker(symbol, repo, schema) when is_binary(symbol) do # &lt;= new args ... {:ok, _settings} = update_status(symbol, &quot;off&quot;, repo, schema) # &lt;= args passed ... {:ok, _settings} = update_status(symbol, &quot;off&quot;, repo, schema) # &lt;= args passed ... end As we modified both start_worker/1 and stop_worker/1 by adding two additional arguments we need to update all references to them and here is where things branch out a bit. We will start with start_worker/1 function (which is now start_worker/3) - it‚Äôs used by the autostart_workers/2 inside Core.ServiceSupervisor module. The autostart_workers/2 function already has repo and schema so we can just pass them to the start_worker/3: # /apps/core/lib/core/service_supervisor.ex def autostart_workers(repo, schema) do fetch_symbols_to_start(repo, schema) |&gt; Enum.map(&amp;start_worker(&amp;1, repo, schema)) # &lt;= args passed end Both the start_worker/3 and the stop_worker/3 function are used by the functions inside the Naive.DynamicSymbolSupervisor module. We need to pass the Repo and Schema in the same fashion as previously with autostart_workers/2 function: # /apps/naive/lib/naive/dynamic_symbol_supervisor.ex def start_worker(symbol) do Core.ServiceSupervisor.start_worker( symbol, Naive.Repo, # &lt;= new arg passed Naive.Schema.Settings # &lt;= new arg passed ) end def stop_worker(symbol) do Core.ServiceSupervisor.stop_worker( symbol, Naive.Repo, # &lt;= new arg passed Naive.Schema.Settings # &lt;= new arg passed ) end At this moment there‚Äôs no code inside the Core.ServiceSupervisor module referencing the aliased Repo nor Schema modules so we can safely remove both aliases - definitely, we are moving in the right direction! Btw. Our project still works at this stage, we can start/stop trading and it autostarts trading. 13.5.3 Third path starting with the get_pid/1 function Starting again from the most nested helper function - this time the get_pid/1: # /apps/core/lib/core/service_supervisor.ex def get_pid(symbol) do Process.whereis(:&quot;Elixir.Naive.SymbolSupervisor-#{symbol}&quot;) end We can see that it has a hardcoded Naive.SymbolSupervisor worker module - we need to make this part dynamic by using the worker_module argument: # /apps/core/lib/core/service_supervisor.ex def get_pid(worker_module, symbol) do # &lt;= arg added Process.whereis(:&quot;#{worker_module}-#{symbol}&quot;) # &lt;= arg used end Moving up to functions that are referencing the get_pid/1 function, those will be the start_worker/3 and the stop_worker/3 function. As those are the two last functions to be updated, we will look into them more closely to finish our refactoring in this 3rd run. At this moment both need to add worker_module as both are calling the get_pid/2 function. Looking at both function we can see two other hardcoded details: - inside log message there are words ‚Äútrading‚Äù - we can replace them so we will utilize the worker_module and symbol arguments - there are two references to the Naive.DynamicSymbolSupervisor which we will replace with the module argument - there is one more reference to the Naive.SymbolSupervisor module which we will replace with the worker_module argument Let‚Äôs look at updated functions: # /apps/core/lib/core/service_supervisor.ex # module and worker_module args added vvvv def start_worker(symbol, repo, schema, module, worker_module) when is_binary(symbol) do symbol = String.upcase(symbol) case get_pid(worker_module, symbol) do # &lt;= worker_module passed nil -&gt; Logger.info(&quot;Starting #{worker_module} worker for #{symbol}&quot;) # &lt;= dynamic text {:ok, _settings} = update_status(symbol, &quot;on&quot;, repo, schema) {:ok, _pid} = DynamicSupervisor.start_child(module, {worker_module, symbol}) # &lt;= args used pid -&gt; Logger.warn(&quot;#{worker_module} worker for #{symbol} already started&quot;) # &lt;= dynamic text {:ok, _settings} = update_status(symbol, &quot;on&quot;, repo, schema) {:ok, pid} end end # module and worker_module added as args vvvv def stop_worker(symbol, repo, schema, module, worker_module) when is_binary(symbol) do symbol = String.upcase(symbol) case get_pid(worker_module, symbol) do # &lt;= worker_module passed nil -&gt; Logger.warn(&quot;#{worker_module} worker for #{symbol} already stopped&quot;) # &lt;= dynamic text {:ok, _settings} = update_status(symbol, &quot;off&quot;, repo, schema) pid -&gt; Logger.info(&quot;Stopping #{worker_module} worker for #{symbol}&quot;) # &lt;= dynamic text :ok = DynamicSupervisor.terminate_child(module, pid) # &lt;= arg used {:ok, _settings} = update_status(symbol, &quot;off&quot;, repo, schema) end end Inside both the start_worker/5 and the stop_worker/5 functions we modified: - get_pid/1 to pass the worker_module - Logger‚Äôs messages to use the worker_module and symbol - DynamicSupervisor‚Äôs functions to use the module and the worker_module Again, as we modified start_worker/5 we need to make the last change inside the Core.ServiceSupervisor module - autostart_workers/2 uses the start_worker/5 function: # /apps/core/lib/core/service_supervisor.ex def autostart_workers(repo, schema, module, worker_module) do # &lt;= args added fetch_symbols_to_start(repo, schema) |&gt; Enum.map(&amp;start_worker(&amp;1, repo, schema, module, worker_module)) # &lt;= args added end Just for referrence - final function headers look as following: # /apps/core/lib/core/service_supervisor.ex defmodule Core.ServiceSupervisor do def autostart_workers(repo, schema, module, worker_module) do ... end def start_worker(symbol, repo, schema, module, worker_module) when is_binary(symbol) do ... end def stop_worker(symbol, repo, schema, module, worker_module) when is_binary(symbol) do ... end def get_pid(worker_module, symbol) do ... end def update_status(symbol, status, repo, schema) when is_binary(symbol) and is_binary(status) do ... end def fetch_symbols_to_start(repo, schema) do ... end end That finishes the 3rd round of updates inside the Core.ServiceSupervisor module, now we need to update the Naive.DynamicSymbolSupervisor module to use updated functions(and pass required arguments): # /apps/naive/lib/naive/dynamic_symbol_supervisor.ex ... def autostart_workers() do Core.ServiceSupervisor.autostart_workers( Naive.Repo, Naive.Schema.Settings, __MODULE__, # &lt;= added arg Naive.SymbolSupervisor # &lt;= added arg ) end def start_worker(symbol) do Core.ServiceSupervisor.start_worker( symbol, Naive.Repo, Naive.Schema.Settings, __MODULE__, # &lt;= added arg Naive.SymbolSupervisor # &lt;= added arg ) end def stop_worker(symbol) do Core.ServiceSupervisor.stop_worker( symbol, Naive.Repo, Naive.Schema.Settings, __MODULE__, # &lt;= added arg Naive.SymbolSupervisor # &lt;= added arg ) end def shutdown_worker(symbol) when is_binary(symbol) do symbol = String.upcase(symbol) case Core.ServiceSupervisor.get_pid(Naive.SymbolSupervisor, symbol) do # &lt;= arg added nil -&gt; Logger.warn(&quot;#{Naive.SymbolSupervisor} worker for #{symbol} already stopped&quot;) # &lt;= updated {:ok, _settings} = Core.ServiceSupervisor.update_status(symbol, &quot;off&quot;, Naive.Repo, Naive.Schema.Settings) # &lt;= args added _pid -&gt; Logger.info(&quot;Initializing shutdown of #{Naive.SymbolSupervisor} worker for #{symbol}&quot;) # &lt;= updated {:ok, settings} = Core.ServiceSupervisor.update_status( symbol, &quot;shutdown&quot;, Naive.Repo, Naive.Schema.Settings ) # ^^^ additional args passed Naive.Leader.notify(:settings_updated, settings) {:ok, settings} end end We needed to update referrences inside the shutdown_trading/1 function as well, as it calls get_pid/2 and update_status/4 functions. We are now done with refactoring the Core.ServiceSupervisor module, it‚Äôs completely generic and can be used inside both streamer and the naive applications. At this moment to use the Core.ServiceSupervisor module we need to write interface functions in our supervisors and pass multiple arguments in each one - again, would need to use of copies those functions inside streamer and naive application. In the next section we will look into how could we leverage Elixir macros to remove that boilerplate. 13.6 Remove boilerplate using use macro Elixir provides a way to use other modules. Idea is that inside the Naive.DynamicSymbolSupervisor module we are useing the DynamicSupervisor module currently but we could use Core.ServiceSupervisor: # /apps/naive/lib/naive/dynamic_symbol_supervisor.ex defmodule Naive.DynamicSymbolSupervisor do use Core.ServiceSupervisor To be able to use the Core.ServiceSupervisor module it needs to provide the __using__/1 macro. As the simplest content of that macro, we can use here would be just to use the DynamicSupervisor inside: # /apps/core/lib/core/service_supervisor.ex defmacro __using__(opts) do IO.inspect(opts) quote location: :keep do use DynamicSupervisor end end How does this work? As an oversimplification, you can think about it as Elixir will look through the contents of quote‚Äôs body(everything between quote ... do and end) in search for the unquote function which can inject dynamic content. All of this will become much clearer as we will go through the first example but the important part is that after executing any potential unquotes inside the quote‚Äôs body, Elixir will grab that code as it would be just part of code and place it inside the Naive.DynamicSymbolSupervisor module at compile time(we will also see the result of IO.inspect/1 at compilation). At this moment(after swapping to use Core.ServiceSupervisor) our code still works and it‚Äôs exactly as we would simply have use DynamicSupervisor inside the Naive.DynamicSymbolSupervisor module - as at compilation, it will be swapped to it either way(as per contents of the __using__/1 macro). As the autostart_workers/0 function is a part of the boilerplate, we will move it from the Naive.DynamicSymbolSupervisor module to the Core.ServiceSupervisor module inside the __using__/1 macro. Ok, but it has all of those other naive application-specific arguments - where will we get those? That‚Äôs what that opts argument is for inside the __using__/1 macro. When we call use Core.ServiceSupervisor we can pass an additional keyword list which will contain all naive application-specific details: # /apps/naive/lib/naive/dynamic_symbol_supervisor.ex defmodule Naive.DynamicSymbolSupervisor do use Core.ServiceSupervisor, repo: Naive.Repo, schema: Naive.Schema.Settings, module: __MODULE__, worker_module: Naive.SymbolSupervisor We can now update the __using__/1 macro to assign all of those details to variables(instead of using IO.inspect/1): # /apps/core/lib/core/service_supervisor.ex defmacro __using__(opts) do {:ok, repo} = Keyword.fetch(opts, :repo) {:ok, schema} = Keyword.fetch(opts, :schema) {:ok, module} = Keyword.fetch(opts, :module) {:ok, worker_module} = Keyword.fetch(opts, :worker_module) ... At this moment we can use those dynamic values to generate code that will be specific to the implementation module for example autostart_workers/0 that we moved from the Naive.DynamicSymbolSupervisor module and will need to have different values passed to it(like Streamer.Binance as worker_module) for the streamer application. We can see that it requires inserting those dynamic values inside the autstart_workers/0 but how to dynamically inject arguments - unquote to the rescue. When we will update the autostart_workers/0 function from: # sample moved code from the `Naive.DynamicSymbolSupervisor` module def autostart_workers() do Core.ServiceSupervisor.autostart_workers( Naive.Repo, Naive.Schema.Settings, __MODULE__, Naive.SymbolSupervisor ) end to: # updated code that will become part of the `__using__/1` macro def autostart_workers() do Core.ServiceSupervisor.autostart_workers( unquote(repo), unquote(schema), unquote(module), unquote(worker_module) ) end in the end generated code that will be ‚Äúpasted‚Äù to the Naive.DynamicSymbolSupervisor module at compile time will be: # compiled code attached to the `Naive.DynamicSymbolSupervisor` module def autostart_workers() do Core.ServiceSupervisor.autostart_workers( Naive.Repo, Naive.Schema.Settings, Naive.DynamicSymbolSupervisor, Naive.SymbolSupervisor ) end This way we can dynamically create functions for any application(for the streamer application, it will generate function call with the Streamer.Repo, Streamer.Schema.Settings args etc). We can apply that to all of the passed variables inside the autostart_workers/0 function - just for reference full macro will look as follows: # /apps/core/lib/core/service_supervisor.ex defmacro __using__(opts) do {:ok, repo} = Keyword.fetch(opts, :repo) {:ok, schema} = Keyword.fetch(opts, :schema) {:ok, module} = Keyword.fetch(opts, :module) {:ok, worker_module} = Keyword.fetch(opts, :worker_module) quote location: :keep do use DynamicSupervisor def autostart_workers() do Core.ServiceSupervisor.autostart_workers( unquote(repo), unquote(schema), unquote(module), unquote(worker_module) ) end end end You can think about the above macro that it will substitute the unquote(..) parts with passed values and then it will grab the whole contents between quote ... do and end and it will paste it to the Naive.DynamicSymbolSupervisor module at compile-time - we can visualize generated/‚Äúpasted‚Äù code as: # code generated by the `__using__/1` macro that # will be &quot;pasted&quot; to the `Naive.DynamicSymbolSupervisor` module use DynamicSupervisor def autostart_workers() do Core.ServiceSupervisor.autostart_workers( Naive.Repo, Naive.Schema.Settings, Naive.DynamicSymbolSupervisor, Naive.SymbolSupervisor ) end This is exactly the code that we had before inside the Naive.DynamicSymbolSupervisor module but now it‚Äôs stored away inside the Core.ServiceSupervisor‚Äôs __using__/1 macro and it doesn‚Äôt need to be implemented/copied across twice into two apps anymore. We can now follow the same principle and move start_worker/1 and stop_worker/1 from the Naive.DynamicSymbolSupervisor module into __using__/1 macro inside the Core.ServiceSupervisor module: # /apps/core/lib/core/service_supervisor.ex # inside the __using__/1 macro ... def start_worker(symbol) do Core.ServiceSupervisor.start_worker( symbol, # &lt;= this needs to stay as variable unquote(repo), unquote(schema), unquote(module), unquote(worker_module) ) end def stop_worker(symbol) do Core.ServiceSupervisor.stop_worker( symbol, # &lt;= this needs to stay as variable unquote(repo), unquote(schema), unquote(module), unquote(worker_module) ) end Here we have an example of an execution time variable called symbol that we should not unquote as it will be different per function call (source code should have symbol variable there not for example \"NEOUSDT\"). At this moment the Naive.DynamicSymbolSupervisor consists of only start_link/1, init/1 and shutdown_worker/1, it‚Äôs under 50 lines of code and works exactly as before refactoring. All of the boilerplate was moved to the Core.ServiceSupervisor module. We left the shutdown_worker/1 function as it‚Äôs specific to the naive application, but inside it, we utilize both the get_pid/2 and the update_status/4 functions where we are passing the naive application-specific variables(like Naive.Repo). To make things even nicer we can create convenience wrappers for those two functions inside the __using__/1 macro: # /apps/core/lib/core/service_supervisor.ex # add below at the end of `quote` block inside `__using__/1` defp get_pid(symbol) do Core.ServiceSupervisor.get_pid( unquote(worker_module), symbol ) end defp update_status(symbol, status) do Core.ServiceSupervisor.update_status( symbol, status, unquote(repo), unquote(schema) ) end As those will get compiled and ‚Äúpasted‚Äù into the Naive.DynamicSymbolSupervisor module we can utilize them inside the shutdown_worker/1 function as they would be much simpler naive application-specific local functions: # /apps/naive/lib/naive/dynamic_symbol_supervisor.ex def shutdown_worker(symbol) when is_binary(symbol) do symbol = String.upcase(symbol) case get_pid(symbol) do # &lt;= macro provided function nil -&gt; Logger.warn(&quot;#{Naive.SymbolSupervisor} worker for #{symbol} already stopped&quot;) {:ok, _settings} = update_status(symbol, &quot;off&quot;) # &lt;= macro provided function _pid -&gt; Logger.info(&quot;Initializing shutdown of #{Naive.SymbolSupervisor} worker for #{symbol}&quot;) {:ok, settings} = update_status(symbol, &quot;shutdown&quot;) # &lt;= macro provided function Naive.Leader.notify(:settings_updated, settings) {:ok, settings} end end And now, a very last change - I promise ;) Both the start_link/1 and the init/1 functions are still referencing the DynamicSupervisor module which could be a little bit confusing - let‚Äôs swap those calls to use the Core.ServiceSupervisor module (both to not confuse people and be consistent with the use macro): # /apps/naive/lib/naive/dynamic_symbol_supervisor.ex def start_link(init_arg) do Core.ServiceSupervisor.start_link(__MODULE__, init_arg, name: __MODULE__) end def init(_init_arg) do Core.ServiceSupervisor.init(strategy: :one_for_one) end As we don‚Äôt want/need to do anything different inside the Core.ServiceSupervisor module than the DynamicSupervisor is doing we can just delegate both of those inside the Core.ServiceSupervisor module: # /apps/core/lib/core/service_supervisor.ex defdelegate start_link(module, args, opts), to: DynamicSupervisor defdelegate init(opts), to: DynamicSupervisor That finishes our refactoring of both the Naive.DynamicSymbolSupervisor and the Core.ServiceSupervisor modules. We can test to confirm that everything works as expected: $ iex -S mix iex(1)&gt; Naive.start_trading(&quot;NEOUSDT&quot;) 21:42:37.741 [info] Starting Elixir.Naive.SymbolSupervisor worker for NEOUSDT 21:42:37.768 [info] Starting new supervision tree to trade on NEOUSDT {:ok, #PID&lt;0.464.0&gt;} 21:42:39.455 [info] Initializing new trader(1614462159452) for NEOUSDT iex(2)&gt; Naive.stop_trading(&quot;NEOUSDT&quot;) 21:43:08.362 [info] Stopping Elixir.Naive.SymbolSupervisor worker for NEOUSDT {:ok, %Naive.Schema.Settings{ ... }} iex(3)&gt; Naive.start_trading(&quot;HNTUSDT&quot;) 21:44:08.689 [info] Starting Elixir.Naive.SymbolSupervisor worker for HNTUSDT 21:44:08.723 [info] Starting new supervision tree to trade on HNTUSDT {:ok, #PID&lt;0.475.0&gt;} 21:44:11.182 [info] Initializing new trader(1614462251182) for HNTUSDT BREAK: (a)bort (A)bort with dump (c)ontinue (p)roc info (i)nfo (l)oaded (v)ersion (k)ill (D)b-tables (d)istribution $ iex -S mix 21:47:22.119 [info] Starting Elixir.Naive.SymbolSupervisor worker for HNTUSDT 21:47:22.161 [info] Starting new supervision tree to trade on HNTUSDT 21:47:24.213 [info] Initializing new trader(1614462444212) for HNTUSDT iex(1)&gt; Naive.shutdown_trading(&quot;HNTUSDT&quot;) 21:48:42.003 [info] Initializing shutdown of Elixir.Naive.SymbolSupervisor worker for HNTUSDT {:ok, %Naive.Schema.Settings{ ... }} The above test confirms that we can start, stop, and shut down trading on a symbol as well as autostarting of trading works. 13.7 Use the Core.ServiceSupervisor module inside the streamer application As we are happy with the implementation of the Core.ServiceSupervisor module we can upgrade the streamer application to use it. We need to start with adding the core application to the list of dependencies of the streamer application: # /apps/streamer/mix.exs defp deps do [ {:binance, &quot;~&gt; 0.7.1&quot;}, {:core, in_umbrella: true}, # &lt;= core added to deps ... We can now move on to the Streamer.DynamicStreamerSupervisor where we will remove everything (really everything including imports, aliases and even require) beside the start_link/1 and the init/1. As with the Naive.DynamicSymbolSupervisor we will use the Core.ServiceSupervisor and pass all required options - full implementation of the Streamer.DynamicStreamerSupervisor module should look as follows: # /apps/streamer/lib/streamer/dynamic_streamer_supervisor.ex defmodule Streamer.DynamicStreamerSupervisor do use Core.ServiceSupervisor, repo: Streamer.Repo, schema: Streamer.Schema.Settings, module: __MODULE__, worker_module: Streamer.Binance def start_link(init_arg) do Core.ServiceSupervisor.start_link(__MODULE__, init_arg, name: __MODULE__) end def init(_init_arg) do Core.ServiceSupervisor.init(strategy: :one_for_one) end end Not much to add here - we are useing the Core.ServiceSupervisor module and passing options to it so it can macro generates streamer application-specific wrappers(like start_worker/1 or stop_worker/1 with required repo, schema etc) around generic logic from the Core.ServiceSupervisor module. Using the Core.Servicesupervisor module will have an impact on the interface of the Streamer.DynamicStreamerSupervisor as it will now provide functions like start_worker/1 instead of start_streaming/1 etc. As with the naive application, we need to update the Task function inside the Streamer.Supervisor module: # /apps/streamer/lib/streamer/supervisor.ex ... {Task, fn -&gt; Streamer.DynamicStreamerSupervisor.autostart_workers() end} ... As well as main Streamer module needs to forward calls instead of delegating: # /apps/streamer/lib/streamer.ex alias Streamer.DynamicStreamerSupervisor defdelegate start_streaming(symbol), to: DynamicStreamerSupervisor, as: :start_worker defdelegate stop_streaming(symbol), to: DynamicStreamerSupervisor, as: :stop_worker We can run a quick test to confirm that indeed everything works as expected: $ iex -S mix iex(1)&gt; Streamer.start_streaming(&quot;NEOUSDT&quot;) 22:10:38.813 [info] Starting Elixir.Streamer.Binance worker for NEOUSDT {:ok, #PID&lt;0.465.0&gt;} iex(2)&gt; Streamer.stop_streaming(&quot;NEOUSDT&quot;) 22:10:48.212 [info] Stopping Elixir.Streamer.Binance worker for NEOUSDT {:ok, %Streamer.Schema.Settings{ __meta__: #Ecto.Schema.Metadata&lt;:loaded, &quot;settings&quot;&gt;, id: &quot;db8c9429-2356-4243-a08f-0d0e89b74986&quot;, inserted_at: ~N[2021-02-25 22:15:16], status: &quot;off&quot;, symbol: &quot;NEOUSDT&quot;, updated_at: ~N[2021-02-27 22:10:48] }} iex(3)&gt; Streamer.start_streaming(&quot;LTCUSDT&quot;) 22:26:03.361 [info] Starting Elixir.Streamer.Binance worker for LTCUSDT {:ok, #PID&lt;0.490.0&gt;} BREAK: (a)bort (A)bort with dump (c)ontinue (p)roc info (i)nfo (l)oaded (v)ersion (k)ill (D)b-tables (d)istribution ^C $ iex -S mix ... 22:26:30.775 [info] Starting Elixir.Streamer.Binance worker for LTCUSDT This finishes the implementation for both the streamer and the naive application. We are generating dynamic functions(metaprogramming) using Elixir macros which is a cool exercise to go through and feels like superpowers ;) [Note] Please remember to run mix format to keep things nice and tidy. Source code for this chapter can be found at Github "],["store-trade-events-and-orders-inside-the-database.html", "Chapter 14 Store trade events and orders inside the database 14.1 Objectives 14.2 Overview of requirements 14.3 Create a new data_warehouse application in the umbrella 14.4 Connect to the database using Ecto 14.5 Store trade events‚Äô data 14.6 Store orders‚Äô data 14.7 Implement supervision", " Chapter 14 Store trade events and orders inside the database 14.1 Objectives overview of requirements create a new data_warehouse application in the umbrella connect to the database using Ecto store trade events‚Äô data store orders‚Äô data implement supervision 14.2 Overview of requirements In the next chapter, we will move on to testing our strategy against historical data(aka backtesting - I will explain that process in the next chapter). What we need to have in place before we will be able to do that is both trade events and orders stored in the database. Starting with the trade events. The streamer application could store trade events from Binance inside its database but how would that work if we would like to introduce another source of non-streamed trade events(ie. flat files, HTTP polling). It would be better if the Streamer.Binance process would keep on streaming those trade events as it is and we would create a new application that would subscribe to the existing TRADE_EVENTS:#{symbol} topic and store them to the database. A similar idea applies to the orders‚Äô data. At this moment the naive application uses the Binance module to place orders. We could store them inside the naive application‚Äôs database but how would that work if we would like to introduce another trading strategy. Holding data in separate databases for each strategy would cause further complications in future reporting, auditing, etc. To store trade events‚Äô and orders‚Äô data we will create a new application called data_warehouse inside our umbrella project. It will subscribe to a TRADE_EVENTS:#{symbol} stream as well as ORDERS:#{symbol} stream, convert broadcasted data to its own representations(structs), and store it inside the database. Trade events are already broadcasted to the PubSub topic, orders on the other hand aren‚Äôt. We will need to modify the Naive.Trader module to broadcast the new and updated orders to the ORDERS:#{symbol} topic. After implementing the basic worker that will store the incoming data(trade events and orders) inside the database, we will look into adding a supervision tree utilizing Elixir Registry. It will allow us to skip registering every worker with a unique atom and will offer an easy lookup to fetch pids instead. 14.3 Create a new data_warehouse application in the umbrella Let‚Äôs start by creating a new application called data_warehouse inside our umbrella: $ cd apps $ mix new data_warehouse --sup * creating README.md * creating .formatter.exs * creating .gitignore * creating mix.exs * creating lib * creating lib/data_warehouse.ex * creating lib/data_warehouse/application.ex * creating test * creating test/test_helper.exs * creating test/data_warehouse_test.exs ... 14.4 Connect to the database using Ecto We can now follow similar steps as previously and add required dependencies (like the ecto) to its deps by modifying its mix.exs file: # /apps/data_warehouse/mix.exs ... defp deps do [ {:ecto_sql, &quot;~&gt; 3.0&quot;}, {:ecto_enum, &quot;~&gt; 1.4&quot;}, {:phoenix_pubsub, &quot;~&gt; 2.0&quot;}, {:postgrex, &quot;&gt;= 0.0.0&quot;}, {:streamer, in_umbrella: true} ] end ... Additionally, we added the phoenix_pubsub module to be able to subscribe to the PubSub topic and the streamer application to be able to use its Streamer.Binance.TradeEvent struct. We can now jump back to the terminal to install added dependencies and generate a new Ecto.Repo module: $ mix deps.get ... $ cd apps/data_warehouse $ mix ecto.gen.repo -r DataWarehouse.Repo * creating lib/data_warehouse * creating lib/data_warehouse/repo.ex * updating ../../config/config.exs Before we will be able to create migrations that will create our tables we need to update the generated configuration inside the config/config.exs file: # /config/config.exs ... config :data_warehouse, # &lt;= added line ecto_repos: [DataWarehouse.Repo] # &lt;= added line config :data_warehouse, DataWarehouse.Repo, database: &quot;data_warehouse&quot;, # &lt;= updated line username: &quot;postgres&quot;, # &lt;= updated line password: &quot;postgres&quot;, # &lt;= updated line hostname: &quot;localhost&quot; ... and add the DataWarehouse.Repo module to the children list of the DataWarehouse.Application‚Äôs process: # /apps/data_warehouse/lib/data_warehouse/application.ex ... children = [ {DataWarehouse.Repo, []} ] ... The last step will be to create a database by running mix ecto.create -r DataWarehouse.Repo command. This ends up the setup of the Ecto - we can now move on to the implementation of storing the orders and the trade events. 14.5 Store trade events‚Äô data The first step to store trade events inside the database will be to create a table that will hold our data. We will start by creating the migration: $ cd apps/data_warehouse $ mix ecto.gen.migration create_trade_events * creating priv/repo/migrations * creating priv/repo/migrations/20210222224514_create_trade_events.exs The Streamer.Binance.TradeEvent struct will serve as a list of columns for our new trade_events table. Here‚Äôs the full implementation of our migration: # /apps/data_warehouse/priv/repo/migrations/20210222224514_create_trade_events.exs defmodule DataWarehouse.Repo.Migrations.CreateTradeEvents do use Ecto.Migration def change do create table(:trade_events, primary_key: false) do add(:id, :uuid, primary_key: true) add(:event_type, :text) add(:event_time, :bigint) add(:symbol, :text) add(:trade_id, :integer) add(:price, :text) add(:quantity, :text) add(:buyer_order_id, :bigint) add(:seller_order_id, :bigint) add(:trade_time, :bigint) add(:buyer_market_maker, :bool) timestamps() end end end We added the additional id field to easily identify each trade event and our timestamps for monitoring. Let‚Äôs run the migration so it will create a new trade_events table for us: $ mix ecto.migrate The next step will be to create a new directory called schema inside the apps/data_warehouse/lib/data_warehouse directory. Inside it, we need to create a new schema file called trade_event.ex. We can copy across the same columns from the migration straight to schema: # /apps/data_warehouse/lib/data_warehouse/schema/trade_event.ex defmodule DataWarehouse.Schema.TradeEvent do use Ecto.Schema @primary_key {:id, :binary_id, autogenerate: true} schema &quot;trade_events&quot; do field(:event_type, :string) field(:event_time, :integer) field(:symbol, :string) field(:trade_id, :integer) field(:price, :string) field(:quantity, :string) field(:buyer_order_id, :integer) field(:seller_order_id, :integer) field(:trade_time, :integer) field(:buyer_market_maker, :boolean) timestamps() end end At this moment we should be able to execute crud(create, read[select], update, delete) operations over the table using the above struct. Currently, we can already store the trade events‚Äô data inside the database so we can move on to collecting it. Trade events are getting broadcasted by the Streamer.Binance process here: # /apps/streamer/lib/streamer/binance.ex ... Phoenix.PubSub.broadcast( Streamer.PubSub, &quot;TRADE_EVENTS:#{trade_event.symbol}&quot;, trade_event ) ... We will implement a subscriber process that will be given a PubSub topic and it will store incoming data inside the database. Let‚Äôs start by creating a new folder called subscriber inside the apps/data_warehouse/lib/data_warehouse directory together with a new file called worker.ex inside it: # /apps/data_warehouse/lib/data_warehouse/subscriber/worker.ex defmodule DataWarehouse.Subscriber.Worker do use GenServer require Logger defmodule State do @enforce_keys [:topic] defstruct [:topic] end def start_link(topic) do GenServer.start_link( __MODULE__, topic, name: :&quot;#{__MODULE__}-#{topic}&quot; ) end def init(topic) do {:ok, %State{ topic: topic }} end end At this moment it‚Äôs just a box standard implementation of the GenServer with a state struct containing a single key(:topic). We need to update the init/1 function to subscribe to the PubSub topic: # /apps/data_warehouse/lib/data_warehouse/subscriber/worker.ex def init(topic) do Logger.info(&quot;DataWarehouse worker is subscribing to #{topic}&quot;) Phoenix.PubSub.subscribe( Streamer.PubSub, topic ) ... Next, we need to add a handler for received messages: # /apps/data_warehouse/lib/data_warehouse/subscriber/worker.ex def handle_info(%Streamer.Binance.TradeEvent{} = trade_event, state) do opts = trade_event |&gt; Map.from_struct() struct!(DataWarehouse.Schema.TradeEvent, opts) |&gt; DataWarehouse.Repo.insert() {:noreply, state} end As we did in the case of the Naive.Trader, all incoming messages trigger a handle_info/2 callback with contents of the message and the current state of the subscriber worker. We just convert that incoming trade event to a map and then that map to the TradeEvent struct that gets inserted into the database. This finishes storing of trade events implementation which we can test by in the interactive shell by running: $ iex -S mix ... iex(1)&gt; Streamer.start_streaming(&quot;XRPUSDT&quot;) 00:48:30.147 [info] Starting Elixir.Streamer.Binance worker for XRPUSDT {:ok, #PID&lt;0.395.0&gt;} iex(2)&gt; DataWarehouse.Subscriber.Worker.start_link(&quot;TRADE_EVENTS:XRPUSDT&quot;) 00:49:48.204 [info] DataWarehouse worker is subscribing to TRADE_EVENTS:XRPUSDT {:ok, #PID&lt;0.405.0&gt;} After a couple of minutes we can check the database using psql: $ psql -Upostgres -h127.0.0.1 Password for user postgres: ... postgres=# \\c data_warehouse; You are now connected to database &quot;data_warehouse&quot; as user &quot;postgres&quot;. data_warehouse=# \\x Expanded display is on. data_warehouse=# SELECT * FROM trade_events; -[ RECORD 1 ]------+------------------------------------- id | f6eae686-946a-4e34-9c33-c7034c2cad5d event_type | trade event_time | 1614041388236 symbol | XRPUSDT trade_id | 152765072 price | 0.56554000 quantity | 1199.10000000 buyer_order_id | 1762454848 seller_order_id | 1762454775 trade_time | 1614041388235 buyer_market_maker | f inserted_at | 2021-02-23 00:49:48 ... As we can see in the above output, trade events are now getting stored inside the database. 14.6 Store orders‚Äô data In the same fashion as with trade events‚Äô data above, to store orders data we will create an orders table inside a new migration: $ cd apps/data_warehouse $ mix ecto.gen.migration create_orders * creating priv/repo/migrations/20210222224522_create_orders.exs The list of columns for this table will be a copy of Binance.Order struct returned from the Binance exchange: # /apps/data_warehouse/priv/repo/migrations/20210222224522_create_orders.exs defmodule DataWarehouse.Repo.Migrations.CreateOrders do use Ecto.Migration def change do create table(:orders, primary_key: false) do add(:order_id, :bigint, primary_key: true) add(:client_order_id, :text) add(:symbol, :text) add(:price, :text) add(:original_quantity, :text) add(:executed_quantity, :text) add(:cummulative_quote_quantity, :text) add(:status, :text) add(:time_in_force, :text) add(:type, :text) add(:side, :text) add(:stop_price, :text) add(:iceberg_quantity, :text) add(:time, :bigint) add(:update_time, :bigint) timestamps() end end end We updated all of the shortened names like orig_qty to full names like original_quantity. Let‚Äôs run the migration so it will create a new orders table for us: $ mix ecto.migrate We can copy the above fields list to create a schema module. First, let‚Äôs create a new file called order.ex inside the apps/data_warehouse/lib/data_warehouse/schema directory: # /apps/data_warehouse/lib/data_warehouse/schema/order.ex defmodule DataWarehouse.Schema.Order do use Ecto.Schema @primary_key {:order_id, :integer, autogenerate: false} schema &quot;orders&quot; do field(:client_order_id, :string) field(:symbol, :string) field(:price, :string) field(:original_quantity, :string) field(:executed_quantity, :string) field(:cummulative_quote_quantity, :string) field(:status, :string) field(:time_in_force, :string) field(:type, :string) field(:side, :string) field(:stop_price, :string) field(:iceberg_quantity, :string) field(:time, :integer) field(:update_time, :integer) timestamps() end end We can now add a handler to our DataWarehouse.Subscriber.Worker that will convert the Binance.Order struct to DataWarehouse.Schema.Order and store data inside the database: # /apps/data_warehouse/lib/data_warehouse/subscriber/worker.ex def handle_info(%Binance.Order{} = order, state) do data = order |&gt; Map.from_struct() struct(DataWarehouse.Schema.Order, data) |&gt; Map.merge(%{ original_quantity: order.orig_qty, executed_quantity: order.executed_qty, cummulative_quote_quantity: order.cummulative_quote_qty, iceberg_quantity: order.iceberg_qty }) |&gt; DataWarehouse.Repo.insert( on_conflict: :replace_all, conflict_target: :order_id ) {:noreply, state} end ... In the above code, we are copying the matching fields using the struct/2 function but all other fields that aren‚Äôt 1 to 1 between two structs won‚Äôt be copied, so we need to merge them in the second step(using the Map.merge/2 function). We are also using the on_conflict: :replace_all option to make the insert/2 function act as it would be upsert/2(to avoid writing separate logic for inserting and updating the orders). Having all of this in place we will now be able to store broadcasted orders‚Äô data in the database but there‚Äôs nothing actually broadcasting them. We need to modify the Naive.Trader module to broadcast the Binance.Order whenever it places buy/sell orders or fetches them again: # /apps/naive/lib/naive/trader.ex ... # inside placing initial buy order callback {:ok, %Binance.OrderResponse{} = order} = @binance_client.order_limit_buy(symbol, quantity, price, &quot;GTC&quot;) :ok = broadcast_order(order) ... # inside buy order (partially) filled callback {:ok, %Binance.Order{} = current_buy_order} = @binance_client.get_order( symbol, timestamp, order_id ) :ok = broadcast_order(current_buy_order) ... # inside the same callback in case of buy order filled {:ok, %Binance.OrderResponse{} = order} = @binance_client.order_limit_sell(symbol, quantity, sell_price, &quot;GTC&quot;) :ok = broadcast_order(order) ... # inside sell order (partially) filled callback {:ok, %Binance.Order{} = current_sell_order} = @binance_client.get_order( symbol, timestamp, order_id ) :ok = broadcast_order(current_sell_order) ... Above 4 places send both the Binance.OrderResponse and the Binance.Order structs - our broadcast_order/1 function needs to be able to handle them both. Add the following at the bottom of the Naive.Trader module: # /apps/naive/lib/naive/trader.ex defp broadcast_order(%Binance.OrderResponse{} = response) do response |&gt; convert_to_order() |&gt; broadcast_order() end defp broadcast_order(%Binance.Order{} = order) do Phoenix.PubSub.broadcast( Streamer.PubSub, &quot;ORDERS:#{order.symbol}&quot;, order ) end defp convert_to_order(%Binance.OrderResponse{} = response) do data = response |&gt; Map.from_struct() struct(Binance.Order, data) |&gt; Map.merge(%{ cummulative_quote_qty: &quot;0.00000000&quot;, stop_price: &quot;0.00000000&quot;, iceberg_qty: &quot;0.00000000&quot;, is_working: true }) end As DataWarehouse.Subscriber.Worker process expects only the Binance.Order structs to be broadcasted, we first check is it the Binance.OrderResponse struct and convert the passed value to the Binance.Order struct (if that‚Äôs the case) and only then broadcast it to the PubSub topic. The converting logic as previously uses the struct/2 function but it also merges in default values that are missing from the much smaller Binance.OrderResponse struct(with comparison to the Binance.Order). At this moment we will be able to store orders inside the database and we can check that by running: $ iex -S mix ... iex(1)&gt; DataWarehouse.Subscriber.Worker.start_link(&quot;ORDERS:NEOUSDT&quot;) 22:37:43.043 [info] DataWarehouse worker is subscribing to ORDERS:XRPUSDT {:ok, #PID&lt;0.400.0&gt;} iex(2)&gt; Naive.start_trading(&quot;NEOUSDT&quot;) 22:38:39.741 [info] Starting Elixir.Naive.SymbolSupervisor worker for NEOUSDT 22:38:39.832 [info] Starting new supervision tree to trade on NEOUSDT {:ok, #PID&lt;0.402.0&gt;} 22:38:41.654 [info] Initializing new trader(1614119921653) for NEOUSDT iex(3)&gt; Streamer.start_streaming(&quot;NEOUSDT&quot;) 22:39:23.786 [info] Starting Elixir.Streamer.Binance worker for NEOUSDT {:ok, #PID&lt;0.412.0&gt;} 22:39:27.187 [info] The trader(1614119921653) is placing a BUY order for NEOUSDT @ 37.549, quantity: 5.326 22:39:27.449 [info] The trader(1614119921653) is placing a SELL order for NEOUSDT @ 37.578, quantity: 5.326. At this moment inside the DataWarehouse‚Äôs database we should see orders: $ psql -Upostgres -h127.0.0.1 Password for user postgres: ... postgres=# \\c data_warehouse; You are now connected to database &quot;data_warehouse&quot; as user &quot;postgres&quot;. data_warehouse=# \\x Expanded display is on. data_warehouse=# SELECT * FROM orders; -[ RECORD 1 ]--------------+--------------------------------- order_id | 1 client_order_id | C81E728D9D4C2F636F067F89CC14862C symbol | NEOUSDT price | 38.16 original_quantity | 5.241 executed_quantity | 0.00000000 cummulative_quote_quantity | 0.00000000 status | FILLED time_in_force | GTC type | LIMIT side | BUY stop_price | 0.00000000 iceberg_quantity | 0.00000000 time | 1614120906320 update_time | 1614120906320 inserted_at | 2021-02-23 22:55:10 updated_at | 2021-02-23 22:55:10 -[ RECORD 2 ]--------------+--------------------------------- order_id | 2 client_order_id | ECCBC87E4B5CE2FE28308FD9F2A7BAF3 symbol | NEOUSDT price | 38.19 original_quantity | 5.241 executed_quantity | 0.00000000 cummulative_quote_quantity | 0.00000000 status | NEW time_in_force | GTC type | LIMIT side | SELL stop_price | 0.00000000 iceberg_quantity | 0.00000000 time | update_time | inserted_at | 2021-02-23 22:55:10 updated_at | 2021-02-23 22:55:10 The first record above got inserted and updated as its state is ‚ÄúFILLED‚Äù, the second one wasn‚Äôt updated yet as it‚Äôs still in ‚ÄúNEW‚Äù state - that confirms that the upsert trick works. That finishes the implementation of storing orders inside the database. 14.7 Implement supervision Currently, we have a DataWarehouse.Subscriber.Worker process that will take care of storing data into the database, but sadly if anything will go wrong inside our worker and it will crash there‚Äôs no supervision in place to restart it. The supervision tree for the data_warehouse application will be similar to ones from the naive and streamer apps but different enough to not use the Core.ServiceSupervisor abstraction. For example, it doesn‚Äôt use the symbol column, it works based on the topic column. This would require changes to the Core.ServiceSupervisor‚Äôs functions like update_status/4 or fetch_symbols_to_start/2, we could update them to accept column name but that would need to be passed through other functions. We can see that this is probably not the best approach and the further we will get the more complex it will become. The second issue would be that we are registering all processes with names and that can be problematic as the list of processes will start to grow(as we can imagine in the case of the data_warehouse application). The better approach would be to mix the DynamicSupervisor together with Registry. The DynamicSupervisor will supervise the Subscriber.Workers and instead of keeping track of them by registering them using atoms we will start them :via Elixir Registry. We will add all functionality that we implemented for naive and streamer applications. We will provide the functions to start and stop storing data on passed PubSub topics as well as store those topics inside the database so storing will be autostarted. 14.7.1 Create subscriber_settings table To provide autostarting function we need to create a new migration that will create the subscriber_settings table: $ cd apps/data_warehouse $ mix ecto.gen.migration create_subscriber_settings * creating priv/repo/migrations/20210227230123_create_subscriber_settings.exs At this moment we can copy the code to create the settings table(enum and index as well) from the streamer application and tweak it to fit the data_warehouse application. So the first important change (beside updating namespaces from Streamer to DataWarehouse) will be to make a note that we have a setting per topic - not per symbol as for the naive and streamer applications: # /apps/data_warehouse/priv/repo/migrations/20210227230123_create_subscriber_settings.exs defmodule DataWarehouse.Repo.Migrations.CreateSubscriberSettings do use Ecto.Migration alias DataWarehouse.Schema.SubscriberStatusEnum def change do SubscriberStatusEnum.create_type() create table(:subscriber_settings, primary_key: false) do add(:id, :uuid, primary_key: true) add(:topic, :text, null: false) add(:status, SubscriberStatusEnum.type(), default: &quot;off&quot;, null: false) timestamps() end create(unique_index(:subscriber_settings, [:topic])) end end Both schema and enum will be almost identical to the ones from the streamer application - we can simply copy those files and apply basic tweaks like updating the namespace: $ cp apps/streamer/lib/streamer/schema/settings.ex apps/data_warehouse/lib/data_warehouse/schema/subscriber_settings.ex $ cp apps/streamer/lib/streamer/schema/streaming_status_enum.ex apps/data_warehouse/lib/data_warehouse/schema/subscriber_status_enum.ex Remember about updating the symbol column to topic as well as table name inside the DataWarehouse.Schema.SubscriberSettings: # /apps/data_warehouse/lib/data_warehouse/schema/subscriber_settings.ex defmodule DataWarehouse.Schema.SubscriberSettings do use Ecto.Schema alias DataWarehouse.Schema.SubscriberStatusEnum @primary_key {:id, :binary_id, autogenerate: true} schema &quot;subscriber_settings&quot; do field(:topic, :string) field(:status, SubscriberStatusEnum) timestamps() end end Inside apps/data_warehouse/lib/data_warehouse/schema/subscriber_status_enum.ex we need to swap references of Streamer to DataWarehouse and references of StreamingStatusEnum to SubscriberStatusEnum: # /apps/data_warehouse/lib/data_warehouse/schema/subscriber_status_enum.ex import EctoEnum defenum(DataWarehouse.Schema.SubscriberStatusEnum, :subscriber_status, [:on, :off]) Don‚Äôt forget to run the migration: $ mix ecto.migrate At this moment we have all pieces in place to execute queries on our new table. In this place, we can think about the seeding script. For the data_warehouse specifically, we won‚Äôt need to provide that script as we don‚Äôt know in advance what topic names we will use. Instead of seeding settings in advance, our code will ‚Äúupsert‚Äù(using insert function) settings when start_storing/1 or stop_storing/1 are called. 14.7.2 Redesign supervision using Registry We can now focus on drafting a supervision tree for the data_warehouse application. At this moment we have only the DataWarehouse.Subscriber.Worker and the DataWarehouse.Application modules. As it was with the case of naive and streamer applications, we will need an additional level of supervision to cater for ‚Äúautostarting‚Äù Task as well as, in the case of the data_warehouse application the Registry. The full supervision tree will look as follows: Supervision tree with Registry Everything looks very similar to the supervision tree that we created in the streamer and the naive applications but there‚Äôs an additional Registry that is supervised by the SubscriberSupervisior process. The idea is that inside the Worker module‚Äôs start_link/1 we will register worker processes using :via tuple. Internally, GenServer will utilize Registry‚Äôs functions like register_name/2 to add process to the registry under the topic string. This way we will be able to retrieve pids assigned to topics using those topic strings instead of registering each worker process with an atom name. Just as previously the DynamicSupervisor will be in charge of the supervising the Worker processes and it won‚Äôt be even aware that we are using the Registry to keep track of topic =&gt; pid association. 14.7.3 Create the DataWarehouse.Subscriber.DynamicSupervisor module Let‚Äôs start by creating a new file called dynamic_supervisor.ex inside the apps/data_warehouse/lib/data_warehouse/subscriber directory and put default dynamic supervisor implementation inside: # /apps/data_warehouse/lib/data_warehouse/subscriber/dynamic_supervisor.ex defmodule DataWarehouse.Subscriber.DynamicSupervisor do use DynamicSupervisor def start_link(_arg) do DynamicSupervisor.start_link(__MODULE__, [], name: __MODULE__) end def init(_arg) do DynamicSupervisor.init(strategy: :one_for_one) end end As we will put all our logic related to autostarting, starting and stopping inside this module we can already add aliases, import and require: # /apps/data_warehouse/lib/data_warehouse/subscriber/dynamic_supervisor.ex require Logger alias DataWarehouse.Repo alias DataWarehouse.Schema.SubscriberSettings alias DataWarehouse.Subscriber.Worker import Ecto.Query, only: [from: 2] @registry :subscriber_workers Additionally, we added the @registry module attribute that we will use to retrieve pid for the specific topic. We can move on to implementing autostart_workers/0 which will look very similar to the ones that we implemented in the streamer and the naive applications: # /apps/data_warehouse/lib/data_warehouse/subscriber/dynamic_supervisor.ex ... def autostart_workers() do Repo.all( from(s in SubscriberSettings, where: s.status == &quot;on&quot;, select: s.topic ) ) |&gt; Enum.map(&amp;start_child/1) end defp start_child(args) do DynamicSupervisor.start_child( __MODULE__, {Worker, args} ) end We can see that we are querying the database for list of topics(not symbols) and we are calling start_child/2 for each results. The start_worker/1 is where the Registry will shine as we won‚Äôt need to check is there already a process running for that topic - we can leave that check to the Registry. If there‚Äôs process already running for that topic it will just return a tuple starting with :error atom: # /apps/data_warehouse/lib/data_warehouse/subscriber/dynamic_supervisor.ex ... def start_worker(topic) do Logger.info(&quot;Starting storing data from #{topic} topic&quot;) update_status(topic, &quot;on&quot;) start_child(topic) end ... defp update_status(topic, status) when is_binary(topic) and is_binary(status) do %SubscriberSettings{ topic: topic, status: status } |&gt; Repo.insert( on_conflict: :replace_all, conflict_target: :topic ) end As we are not seeding the database with the default settings we will use the insert/2 function with options(as previously) to make it work as it would be an ‚Äúupsert‚Äù function. Last function in this module will be stop_worker/1 which uses private stop_child/1 function. The stop_child/1 function shows how to retrieve pid of the process assigned to the passed topic: # /apps/data_warehouse/lib/data_warehouse/subscriber/dynamic_supervisor.ex ... def stop_worker(topic) do Logger.info(&quot;Stopping storing data from #{topic} topic&quot;) update_status(topic, &quot;off&quot;) stop_child(topic) end ... defp stop_child(args) do case Registry.lookup(@registry, args) do [{pid, _}] -&gt; DynamicSupervisor.terminate_child(__MODULE__, pid) _ -&gt; Logger.warn(&quot;Unable to locate process assigned to #{inspect(args)}&quot;) end end That is a full implementation of the DataWarehouse.Subscriber.DynamicSupervisor module and it‚Äôs almost as slim as one from the last chapter where we leveraged macros to achieve that lightness. Using the Registry is the preferred way to manage a list of identifiable processes. We won‚Äôt run into an issue of overusing the atoms(as they are not garbage collected, we could hit that limit sooner or later). 14.7.4 Register Worker processes using :via The above DynamicSupervisor module assumes that Workers are registered inside the Registry - to make this happen we will need to update the start_link/1 function of the DataWarehouse.Subscriber.Worker module: # /apps/data_warehouse/lib/data_warehouse/subscriber/worker.ex ... def start_link(topic) do GenServer.start_link( __MODULE__, topic, name: via_tuple(topic) ) end ... defp via_tuple(topic) do {:via, Registry, {:subscriber_workers, topic}} end ... Passing the :name option to the GenServer‚Äôs start_link/3 function we instruct it to utilize the Registry module to register processes under topic names. 14.7.5 Create a new supervision level for Registry, Task and the DynamicSupervisor We have the lowest level modules - the Worker and the DynamicSupervisor implemented - time to add a new Supervisor that will start the Registry, the DynamicSupervisor and the autostart storing Task. First create a new file called subscriber_supervisor.ex inside the apps/data_warehouse/lib/data_warehouse directory: # /apps/data_warehouse/lib/data_warehouse/subscriber_supervisor.ex defmodule DataWarehouse.SubscriberSupervisor do use Supervisor alias DataWarehouse.Subscriber.DynamicSupervisor @registry :subscriber_workers def start_link(_args) do Supervisor.start_link(__MODULE__, [], name: __MODULE__) end def init(_args) do children = [ {Registry, [keys: :unique, name: @registry]}, {DynamicSupervisor, []}, {Task, fn -&gt; DynamicSupervisor.autostart_workers() end} ] Supervisor.init(children, strategy: :rest_for_one) end end Important part here will be to match the Registry name to the one defined inside the DynamicSupervisor and the Worker modules. 14.7.6 Link the SubscriberSupervisor to the Application We need to update the DataWarehouse.Application module to start our new DataWarehouse.SubscriberSupervisor process as well as register itself under name matching to its module(just for consistency with other applications): # /apps/data_warehouse/lib/data_warehouse/application.ex ... def start(_type, _args) do children = [ {DataWarehouse.Repo, []}, {DataWarehouse.SubscriberSupervisor, []} # &lt;= new module added ] # See https://hexdocs.pm/elixir/Supervisor.html # for other strategies and supported options opts = [strategy: :one_for_one, name: __MODULE__] # &lt;= name updated Supervisor.start_link(children, opts) end ... 14.7.7 Add interface The final step will be to add an interface to the DataWarehouse application to start and stop storing: # /apps/data_warehouse/lib/data_warehouse.ex alias DataWarehouse.Subscriber.DynamicSupervisor def start_storing(stream, symbol) do to_topic(stream, symbol) |&gt; DynamicSupervisor.start_worker() end def stop_storing(stream, symbol) do to_topic(stream, symbol) |&gt; DynamicSupervisor.stop_worker() end defp to_topic(stream, symbol) do [stream, symbol] |&gt; Enum.map(&amp;String.upcase/1) |&gt; Enum.join(&quot;:&quot;) end Inside the above functions, we are just doing a couple of sanity checks on the case of the passed arguments assuming that both topics and stream are uppercase. 14.7.8 Test The interface above was the last step in our implementation, we can now test that all works as expected: $ iex -S mix ... iex(1)&gt; DataWarehouse.start_storing(&quot;TRADE_EVENTS&quot;, &quot;NEOUSDT&quot;) 19:34:00.740 [info] Starting storing data from TRADE_EVENTS:NEOUSDT topic 19:34:00.847 [info] DataWarehouse worker is subscribing to TRADE_EVENTS:NEOUSDT {:ok, #PID&lt;0.429.0&gt;} iex(2)&gt; DataWarehouse.start_storing(&quot;TRADE_EVENTS&quot;, &quot;NEOUSDT&quot;) 19:34:04.753 [info] Starting storing data from TRADE_EVENTS:NEOUSDT topic {:error, {:already_started, #PID&lt;0.459.0&gt;}} iex(3)&gt; DataWarehouse.start_storing(&quot;ORDERS&quot;, &quot;NEOUSDT&quot;) 19:34:09.386 [info] Starting storing data from ORDERS:NEOUSDT topic 19:34:09.403 [info] DataWarehouse worker is subscribing to ORDERS:NEOUSDT {:ok, #PID&lt;0.431.0&gt;} BREAK: (a)bort (A)bort with dump (c)ontinue (p)roc info (i)nfo (l)oaded (v)ersion (k)ill (D)b-tables (d)istribution ^C% $ iex -S mix ... 19:35:30.058 [info] DataWarehouse worker is subscribing to TRADE_EVENTS:NEOUSDT 19:35:30.062 [info] DataWarehouse worker is subscribing to ORDERS:NEOUSDT # autostart works ^^^ iex(1)&gt; Naive.start_trading(&quot;NEOUSDT&quot;) 19:36:45.316 [info] Starting Elixir.Naive.SymbolSupervisor worker for NEOUSDT 19:36:45.417 [info] Starting new supervision tree to trade on NEOUSDT {:ok, #PID&lt;0.419.0&gt;} iex(3)&gt; 19:36:47.484 [info] Initializing new trader(1615221407466) for NEOUSDT iex(2)&gt; Streamer.start_streaming(&quot;NEOUSDT&quot;) 16:37:39.660 [info] Starting Elixir.Streamer.Binance worker for NEOUSDT {:ok, #PID&lt;0.428.0&gt;} ... iex(3)&gt; DataWarehouse.stop_storing(&quot;trade_events&quot;, &quot;NEOUSDT&quot;) 19:39:26.398 [info] Stopping storing data from trade_events:NEOUSDT topic :ok iex(4)&gt; DataWarehouse.stop_storing(&quot;trade_events&quot;, &quot;NEOUSDT&quot;) 19:39:28.151 [info] Stopping storing data from trade_events:NEOUSDT topic 19:39:28.160 [warn] Unable to locate process assigned to &quot;trade_events:NEOUSDT&quot; :ok iex(5)&gt; [{pid, nil}] = Registry.lookup(:subscriber_workers, &quot;ORDERS:NEOUSDT&quot;) [{#PID&lt;0.417.0&gt;, nil}] iex(6)&gt; Process.exit(pid, :crash) true 16:43:40.812 [info] DataWarehouse worker is subscribing to ORDERS:NEOUSDT As we can see even this simple implementation handles starting, autostarting and stopping. It also gracefully handles starting workers when one is already running as well as stopping when there none running. As a challenge, you could update the naive and the streamer application to use Registry and remove Core.ServiceSupervisor module as it was superseded by the above solution. [Note] Please remember to run mix format to keep things nice and tidy. Source code for this chapter can be found at Github "],["backtest-trading-strategy.html", "Chapter 15 Backtest trading strategy 15.1 Objectives 15.2 Overview of requirements 15.3 Implement the storing task 15.4 Test the backtesting", " Chapter 15 Backtest trading strategy 15.1 Objectives overview of requirements implement the storing task test the backtesting 15.2 Overview of requirements In the last chapter, we started storing trade events and orders in the database which will be crucial for backtesting, which we will focus on in this chapter. Backtesting is a procedure of running historical data through the system and observing how our strategy would perform as if we would run it ‚Äúin the past‚Äù. Backtesting works on assumption that the market will behave in a similar fashion in the future as it was in the past. At this moment we are receiving the trade events from the Binance through WebSocket. The Streamer.Binance process is handling those messages by parsing them from JSON string to map, then converting them to structs and broadcasting them to the TRADE_EVENTS:#{symbol} PubSub topic. The Naive.Trader subscribes to the TRADE_EVENTS:#{symbol} topic and takes decisions based on incoming data. As it places buy and sell orders it broadcasts them to the ORDERS:#{symbol} PubSub topic. The DataWarehouse.Subscriber.Worker processes subscribe to both trade events and orders topics and store incoming data inside the database - we can visualize that flow like that: Normal broadcast/subscribe flow To backtest we can substitute the Streamer.Binance process with a Task that will stream trade events‚Äô data from the database and broadcasts it to the TRADE_EVENTS:#{symbol} PubSub topic(the same topic as the Streamer.Binance process). From the perspective of the Naive.Trader it does not make any difference who is broadcasting those trade events. This should be a clear indication of the value of publish/subscribe model that we implemented from the beginning. It allows us to swap producer and consumers freely to backtest our trading strategies: Task feeds the stream of data from the database straight to the PubSub topic 15.3 Implement the storing task We will start by creating a new file called publisher.ex inside the apps/data_warehouse/lib/data_warehouse directory. We will start by implementing the basic Task behavior: # /apps/data_warehouse/lib/data_warehouse/publisher.ex defmodule DataWarehouse.Publisher do use Task def start_link(arg) do Task.start_link(__MODULE__, :run, [arg]) end def run(arg) do # ... end end To be able to query the database we will import Ecto and require Logger for logging: # /apps/data_warehouse/lib/data_warehouse/publisher.ex ... import Ecto.Query, only: [from: 2] require Logger ... We can now modify the run/1 function to expect specific type, symbol, from, to and interval: # /apps/data_warehouse/lib/data_warehouse/publisher.ex ... def run(%{ type: :trade_events, symbol: symbol, from: from, to: to, interval: interval }) do ... Inside the body of the run/1 function, the first we will convert from and to Unix timestamps by using private helper functions: # /apps/data_warehouse/lib/data_warehouse/publisher.ex ... def run(%{ ... }) do from_ts = &quot;#{from}T00:00:00.000Z&quot; |&gt; convert_to_ms() to_ts = &quot;#{to}T23:59:59.000Z&quot; |&gt; convert_to_ms() end ... defp convert_to_ms(iso8601DateString) do iso8601DateString |&gt; NaiveDateTime.from_iso8601!() |&gt; DateTime.from_naive!(&quot;Etc/UTC&quot;) |&gt; DateTime.to_unix() |&gt; Kernel.*(1000) end Next, we will select data from the database but because of possibly hundreds of thousands of rows being selected and because we are broadcasting them to the PubSub every x ms it could take a substantial amount of time to broadcast all of them. Instead of selecting data and storing all of it in the memory, we will use Repo.stream/1 function to keep broadcasting it on the go. Additionally, we will add index to the data to be able to log info messages every 10k messages. The last thing that we need to define will be the timeout value - the default value is 5 seconds and we will change it to :infinity: # /apps/data_warehouse/lib/data_warehouse/publisher.ex def run(%{ ... }) do ... DataWarehouse.Repo.transaction( fn -&gt; from(te in DataWarehouse.Schema.TradeEvent, where: te.symbol == ^symbol and te.trade_time &gt;= ^from_ts and te.trade_time &lt; ^to_ts, order_by: te.trade_time ) |&gt; DataWarehouse.Repo.stream() |&gt; Enum.with_index() |&gt; Enum.map(fn {row, index} -&gt; :timer.sleep(interval) if rem(index, 10_000) == 0 do Logger.info(&quot;Publisher broadcasted #{index} events&quot;) end publishTradeEvent(row) end) end, timeout: :infinity ) Logger.info(&quot;Publisher finished streaming trade events&quot;) end Finally, the above code uses the publishTradeEvent/1 helper function which converts DataWarehouse‚Äôs TradeEvent to the Stremer‚Äôs TradeEvent to broadcast the same structs as the streamer application: # /apps/data_warehouse/lib/data_warehouse/publisher.ex ... defp publishTradeEvent(%DataWarehouse.Schema.TradeEvent{} = trade_event) do new_trade_event = struct( Streamer.Binance.TradeEvent, trade_event |&gt; Map.to_list() ) Phoenix.PubSub.broadcast( Streamer.PubSub, &quot;TRADE_EVENTS:#{trade_event.symbol}&quot;, new_trade_event ) end This finishes our implementation - we should be able to stream trade events from the database to the PubSub using the above Task which we will do below. 15.4 Test the backtesting For consistency and ease of testing/use, I prepared an compressed single data of trade events for XRPUSDT(2019-06-03). We can download that file from GitHub using wget: $ cd /tmp $ wget https://github.com/Cinderella-Man/binance-trade-events/raw/master/XRPUSDT/XRPUSDT-2019-06-03.csv.gz We can now uncompress the archive and load those trade events into our database: $ gunzip XRPUSDT-2019-06-03.csv.gz $ PGPASSWORD=postgres psql -Upostgres -h localhost -ddata_warehouse -c &quot;\\COPY trade_events FROM &#39;/tmp/XRPUSDT-2019-06-03.csv&#39; WITH (FORMAT csv, delimiter &#39;;&#39;);&quot; COPY 206115 The number after the word COPY in the response indicates the number of rows that got copied into the database. We can now give it a try and run full backtesting but first let‚Äôs clean the orders table: $ psql -Upostgres -h127.0.0.1 Password for user postgres: ... postgres=# \\c data_warehouse You are now connected to database &quot;data_warehouse&quot; as user &quot;postgres&quot;. data_warehouse=# DELETE FROM orders; DELETE ... We can now start a new iex session where we will start trading(the naive application) as well as storing orders(the data_warehouse application) and instead of starting the Streamer.Binance worker we will start the DataWarehouse.Publisher task with arguments matching the imported day and symbol: $ iex -S mix ... iex(1)&gt; DataWarehouse.start_storing(&quot;ORDERS&quot;, &quot;XRPUSDT&quot;) 19:17:59.596 [info] Starting storing data from ORDERS:XRPUSDT topic 19:17:59.632 [info] DataWarehouse worker is subscribing to ORDERS:XRPUSDT {:ok, #PID&lt;0.417.0&gt;} iex(2)&gt; Naive.start_trading(&quot;XRPUSDT&quot;) 19:18:16.293 [info] Starting Elixir.Naive.SymbolSupervisor worker for XRPUSDT 19:18:16.332 [info] Starting new supervision tree to trade on XRPUSDT {:ok, #PID&lt;0.419.0&gt;} 19:18:18.327 [info] Initializing new trader(1615288698325) for XRPUSDT iex(3)&gt; DataWarehouse.Publisher.start_link(%{type: :trade_events, symbol: &quot;XRPUSDT&quot;, from: &quot;2019-06-02&quot;, to: &quot;2019-06-04&quot;, interval: 5}) {:ok, #PID&lt;0.428.0&gt;} 19:19:07.532 [info] Publisher broadcasted 0 events 19:19:07.534 [info] The trader(1615288698325) is placing a BUY order for XRPUSDT @ 0.44391, quantity: 450.5 19:19:07.749 [info] The trader(1615288698325) is placing a SELL order for XRPUSDT @ 0.44426, quantity: 450.5. ... 19:20:07.568 [info] Publisher broadcasted 10000 events ... 19:21:07.571 [info] Publisher broadcasted 20000 events 19:22:07.576 [info] Publisher broadcasted 30000 events ... 19:39:07.875 [info] Publisher broadcasted 200000 events 19:39:44.576 [info] Publisher finished streaming trade events From the above log, we can see that it took about 20 minutes to run 206k records through the system(a lot of that time[17+ minutes] was indeed the 5ms sleep). After the streaming finished we can check out the orders table inside the database to figure out how many trades we made and what income have they generated. $ psql -Upostgres -h127.0.0.1 Password for user postgres: ... postgres=# \\c data_warehouse You are now connected to database &quot;data_warehouse&quot; as user &quot;postgres&quot;. data_warehouse=# SELECT COUNT(*) FROM orders; count ------- 224 (1 row) By looking at the orders we can figure out some performance metrics but that‚Äôs less than perfect to get answers to simple questions like ‚Äúwhat‚Äôs the performance of my strategy?‚Äù. We will address that and other concerns in future chapters. [Note] Please remember to run mix format to keep things nice and tidy. Source code for this chapter can be found at Github "]]
